{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "\n",
    "import pytest\n",
    "from aiokafka import AIOKafkaProducer\n",
    "from fastcore.basics import patch\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from fastkafka.helpers import (\n",
    "    get_collapsible_admonition,\n",
    "    source2markdown,\n",
    ")\n",
    "from fastkafka.testing import (\n",
    "    mock_AIOKafkaProducer_send,\n",
    "    run_script_and_cancel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "# | hide\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "# | hide\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastKafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Effortless Kafka integration for your web services</b>\n",
    "\n",
    "---\n",
    "\n",
    "![PyPI](https://img.shields.io/pypi/v/fastkafka)\n",
    "![PyPI - Downloads](https://img.shields.io/pypi/dm/fastkafka)\n",
    "![PyPI - Python Version](https://img.shields.io/pypi/pyversions/fastkafka)\n",
    "\n",
    "![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/airtai/fastkafka/test.yaml)\n",
    "![CodeQL](https://github.com/airtai/fastkafka//actions/workflows/codeql.yml/badge.svg)\n",
    "![Dependency Review](https://github.com/airtai/fastkafka//actions/workflows/dependency-review.yml/badge.svg)\n",
    "\n",
    "![GitHub](https://img.shields.io/github/license/airtai/fastkafka)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FastKafka](fastkafka.airt.ai) is a powerful and easy-to-use Python library for building asynchronous services that interact with Kafka topics. Built on top of [Pydantic](https://docs.pydantic.dev/), [AIOKafka](https://github.com/aio-libs/aiokafka) and [AsyncAPI](https://www.asyncapi.com/), FastKafka simplifies the process of writing producers and consumers for Kafka topics, handling all the parsing, networking, task scheduling and data generation automatically. With FastKafka, you can quickly prototype and develop high-performance Kafka-based services with minimal code, making it an ideal choice for developers looking to streamline their workflow and accelerate their projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "FastKafka works on macOS, Linux, and most Unix-style operating systems. You can install it with `pip` as usual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install fastkafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing server code\n",
    "\n",
    "Here is an example python script using FastKafka that takes data from a Kafka topic, makes a prediction using a predictive model, and outputs the prediction to another Kafka topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "\n",
    "FastKafka uses [Pydantic](https://docs.pydantic.dev/) to parse input JSON-encoded data into Python objects, making it easy to work with structured data in your Kafka-based applications. Pydantic's [`BaseModel`](https://docs.pydantic.dev/usage/models/) class allows you to define messages using a declarative syntax, making it easy to specify the fields and types of your messages.\n",
    "\n",
    "This example defines two message classes for use in a FastKafka application:\n",
    "\n",
    "- The `InputData` class is used to represent input data for a predictive model. It has three fields: `user_id`, `feature_1`, and `feature_2`. The `user_id` field is of type [`NonNegativeInt`](https://docs.pydantic.dev/usage/types/#constrained-types), which is a subclass of int that only allows non-negative integers. The `feature_1` and `feature_2` fields are both lists of floating-point numbers and integers, respectively.\n",
    "\n",
    "- The `Prediction` class is used to represent the output of the predictive model. It has two fields: `user_id` and `score`. The `score` field is a floating-point number and it represents the prediction made by the model, such as the probability of churn in the next 28 days.\n",
    "\n",
    "These message classes will be used to parse and validate incoming data in Kafka consumers and producers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "from typing import List\n",
       "from pydantic import BaseModel, Field, NonNegativeInt\n",
       "\n",
       "class InputData(BaseModel):\n",
       "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
       "    feature_1: List[float] = Field(\n",
       "        ...,\n",
       "        example=[1.2, 2.3, 4.5, 6.7, 0.1],\n",
       "        description=\"input feature 1\",\n",
       "    )\n",
       "    feature_2: List[int] = Field(\n",
       "        ...,\n",
       "        example=[2, 4, 3, 1, 0],\n",
       "        description=\"input feature 2\",\n",
       "    )\n",
       "\n",
       "class Prediction(BaseModel):\n",
       "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
       "    score: float = Field(\n",
       "        ...,\n",
       "        example=0.4321,\n",
       "        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
       "        ge=0.0,\n",
       "        le=1.0,\n",
       "    )\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "message_defs_str = \"\"\"from typing import List\n",
    "from pydantic import BaseModel, Field, NonNegativeInt\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
    "    feature_1: List[float] = Field(\n",
    "        ...,\n",
    "        example=[1.2, 2.3, 4.5, 6.7, 0.1],\n",
    "        description=\"input feature 1\",\n",
    "    )\n",
    "    feature_2: List[int] = Field(\n",
    "        ...,\n",
    "        example=[2, 4, 3, 1, 0],\n",
    "        description=\"input feature 2\",\n",
    "    )\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
    "    score: float = Field(\n",
    "        ...,\n",
    "        example=0.4321,\n",
    "        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )\n",
    "\"\"\"\n",
    "await run_script_and_cancel(message_defs_str, cancel_after=2)\n",
    "source2markdown(message_defs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These message classes will be used to parse and validate incoming data in a Kafka consumer and to produce a JSON-encoded message in a producer. Using Pydantic's BaseModel in combination with FastKafka makes it easy to work with structured data in your Kafka-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "This example shows how to initialize a FastKafka application.\n",
    "\n",
    "It starts by defining  a dictionary called `kafka_brokers`, which contains two entries: `\"localhost\"` and `\"production\"`, specifying local development and production Kafka brokers. Each entry specifies the URL, port, and other details of a Kafka broker. This dictionary is used for generating the documentation only and it is not being checked by the actual server.\n",
    "\n",
    "Next, an object of the `FastAPI` class is created. It role is to serve the documentation and to start and shutdown `FastKafka`.\n",
    "\n",
    "Finally, an object of the `FastKafka` class is initialized with the minimum set of arguments:\n",
    "\n",
    "- `app`: an `FastAPI` application used for serving the documentation and starting/shutting down the service\n",
    "\n",
    "- `kafka_brokers`: a dictionary used for generation of documentation\n",
    "\n",
    "- `bootstrap_servers`: a ``host[:port]`` string or list of ``host[:port]`` strings that a consumer or a producer should contact to bootstrap initial cluster metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "\n",
       "from os import environ\n",
       "\n",
       "from fastkafka.application import FastKafka\n",
       "\n",
       "kafka_brokers = {\n",
       "    \"localhost\": {\n",
       "        \"url\": \"localhost\",\n",
       "        \"description\": \"local development kafka broker\",\n",
       "        \"port\": 9092,\n",
       "    },\n",
       "    \"production\": {\n",
       "        \"url\": \"kafka.airt.ai\",\n",
       "        \"description\": \"production kafka broker\",\n",
       "        \"port\": 9092,\n",
       "        \"protocol\": \"kafka-secure\",\n",
       "        \"security\": {\"type\": \"plain\"},\n",
       "    },\n",
       "}\n",
       "\n",
       "bootstrap_servers = f\"{environ['KAFKA_HOSTNAME']}:{environ['KAFKA_PORT']}\"\n",
       "\n",
       "kafka_app = FastKafka(\n",
       "    kafka_brokers=kafka_brokers,\n",
       "    bootstrap_servers=bootstrap_servers,\n",
       ")\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "app_creation_str = \"\"\"\n",
    "from os import environ\n",
    "\n",
    "from fastkafka.application import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "bootstrap_servers = f\"{environ['KAFKA_HOSTNAME']}:{environ['KAFKA_PORT']}\"\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    kafka_brokers=kafka_brokers,\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "await run_script_and_cancel(app_creation_str, cancel_after=2)\n",
    "source2markdown(app_creation_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function decorators\n",
    "\n",
    "FastKafka provides convenient function decorators `@kafka_app.consumes` and `@kafka_app.produces` to allow you to delegate the actual process of\n",
    "\n",
    "- consuming and producing data to Kafka, and\n",
    "\n",
    "- decoding and encoding JSON encode messages\n",
    "\n",
    "from user defined functions to the framework. The FastKafka framework delegates these jobs to AIOKafka and Pydantic libraries.\n",
    "\n",
    "These decorators make it easy to specify the processing logic for your Kafka consumers and producers, allowing you to focus on the core business logic of your application without worrying about the underlying Kafka integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following example shows how to use the `@kafka_app.consumes` and `@kafka_app.produces` decorators in a FastKafka application:\n",
    "\n",
    "- The `@kafka_app.consumes` decorator is applied to the `on_input_data` function, which specifies that this function should be called whenever a message is received on the \"input_data\" Kafka topic. The `on_input_data` function takes a single argument which is expected to be an instance of the `InputData` message class. Specifying the type of the single argument is instructing the Pydantic to use `InputData.parse_raw()` on the consumed message before passing it to the user defined function `on_input_data`.\n",
    "\n",
    "- The `@produces` decorator is applied to the `to_predictions` function, which specifies that this function should produce a message to the \"predictions\" Kafka topic whenever it is called. The `to_predictions` function takes two arguments: `user_id` and `score`. It creates a new `Prediction` message with these values and then returns it. The framework will call the `Prediction.json().encode(\"utf-8\")` function on the returned value and produce it to the specified topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "\n",
       "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
       "async def on_input_data(msg: InputData):\n",
       "    global model\n",
       "    score = await model.predict(feature_1=msg.feature_1, feature_2=msg.feature_2)\n",
       "    await to_predictions(user_id=msg.user_id, score=score)\n",
       "\n",
       "\n",
       "@kafka_app.produces(topic=\"predictions\")\n",
       "async def to_predictions(user_id: int, score: float) -> Prediction:\n",
       "    prediction = Prediction(user_id=user_id, score=score)\n",
       "    return prediction\n",
       "\n",
       "\n",
       "# this is a mock up for testing, should be replaced with the real model\n",
       "class Model:\n",
       "    async def predict(self, feature_1: List[int], feature_2: List[float]) -> float:\n",
       "        return 0.87\n",
       "\n",
       "\n",
       "model = Model()\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "decorators_str = \"\"\"\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
    "async def on_input_data(msg: InputData):\n",
    "    global model\n",
    "    score = await model.predict(feature_1=msg.feature_1, feature_2=msg.feature_2)\n",
    "    await to_predictions(user_id=msg.user_id, score=score)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "async def to_predictions(user_id: int, score: float) -> Prediction:\n",
    "    prediction = Prediction(user_id=user_id, score=score)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# this is a mock up for testing, should be replaced with the real model\n",
    "class Model:\n",
    "    async def predict(self, feature_1: List[int], feature_2: List[float]) -> float:\n",
    "        return 0.87\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\"\"\"\n",
    "\n",
    "# await run_script_and_cancel(decorators_str, cancel_after=2)\n",
    "source2markdown(decorators_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the service\n",
    "\n",
    "The service can be started using builtin faskafka run CLI command\n",
    "\n",
    "We will concatenate the code snippets from above and save them in a file `\"server.py\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "??? Example \n",
       "\n",
       "    This example contains the content of the file \"server.py\":\n",
       "\n",
       "    ```python\n",
       "\n",
       "    from typing import List\n",
       "    from pydantic import BaseModel, Field, NonNegativeInt\n",
       "\n",
       "    class InputData(BaseModel):\n",
       "        user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
       "        feature_1: List[float] = Field(\n",
       "            ...,\n",
       "            example=[1.2, 2.3, 4.5, 6.7, 0.1],\n",
       "            description=\"input feature 1\",\n",
       "        )\n",
       "        feature_2: List[int] = Field(\n",
       "            ...,\n",
       "            example=[2, 4, 3, 1, 0],\n",
       "            description=\"input feature 2\",\n",
       "        )\n",
       "\n",
       "    class Prediction(BaseModel):\n",
       "        user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
       "        score: float = Field(\n",
       "            ...,\n",
       "            example=0.4321,\n",
       "            description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
       "            ge=0.0,\n",
       "            le=1.0,\n",
       "        )\n",
       "\n",
       "\n",
       "\n",
       "    from os import environ\n",
       "\n",
       "    from fastkafka.application import FastKafka\n",
       "\n",
       "    kafka_brokers = {\n",
       "        \"localhost\": {\n",
       "            \"url\": \"localhost\",\n",
       "            \"description\": \"local development kafka broker\",\n",
       "            \"port\": 9092,\n",
       "        },\n",
       "        \"production\": {\n",
       "            \"url\": \"kafka.airt.ai\",\n",
       "            \"description\": \"production kafka broker\",\n",
       "            \"port\": 9092,\n",
       "            \"protocol\": \"kafka-secure\",\n",
       "            \"security\": {\"type\": \"plain\"},\n",
       "        },\n",
       "    }\n",
       "\n",
       "    bootstrap_servers = f\"{environ['KAFKA_HOSTNAME']}:{environ['KAFKA_PORT']}\"\n",
       "\n",
       "    kafka_app = FastKafka(\n",
       "        kafka_brokers=kafka_brokers,\n",
       "        bootstrap_servers=bootstrap_servers,\n",
       "    )\n",
       "\n",
       "\n",
       "\n",
       "    @kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
       "    async def on_input_data(msg: InputData):\n",
       "        global model\n",
       "        score = await model.predict(feature_1=msg.feature_1, feature_2=msg.feature_2)\n",
       "        await to_predictions(user_id=msg.user_id, score=score)\n",
       "\n",
       "\n",
       "    @kafka_app.produces(topic=\"predictions\")\n",
       "    async def to_predictions(user_id: int, score: float) -> Prediction:\n",
       "        prediction = Prediction(user_id=user_id, score=score)\n",
       "        return prediction\n",
       "\n",
       "\n",
       "    # this is a mock up for testing, should be replaced with the real model\n",
       "    class Model:\n",
       "        async def predict(self, feature_1: List[int], feature_2: List[float]) -> float:\n",
       "            return 0.87\n",
       "\n",
       "\n",
       "    model = Model()\n",
       "\n",
       "\n",
       "    ```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "script = f\"\"\"\n",
    "{message_defs_str}\n",
    "\n",
    "{app_creation_str}\n",
    "\n",
    "{decorators_str}\n",
    "\"\"\"\n",
    "\n",
    "get_collapsible_admonition(script, name=\"server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we start the FaskKafka servie by running the following command in the folder where the server.py file is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```shell\n",
       "fastkafka run --num-workers=1 server:kafka_app\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "cmd = \"fastkafka run --num-workers=1 server:kafka_app\"\n",
    "\n",
    "Markdown(f\"```shell\\n{cmd}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command, you should see an output like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52931]: [INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[52931]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[52931]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092', 'auto_offset_reset': 'latest', 'max_poll_records': 100, 'group_id': 'my_group'}\n",
      "[52931]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[52931]: [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[52931]: [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[52931]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group my_group\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group my_group\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group my_group\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: Joined group 'my_group' (generation 26) with member_id aiokafka-0.8.0-d3172244-9a47-4f94-8037-e0ac77cbd136\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: Successfully synced group my_group with generation 26\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='input_data', partition=0)} for group my_group\n",
      "Starting process cleanup, this may take a few seconds...\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 52931...\n",
      "[52931]: [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[52931]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[52931]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 52931 terminated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(\n",
    "    script=script,\n",
    "    script_file=\"server.py\",\n",
    "    cmd=cmd,\n",
    "    cancel_after=5,\n",
    ")\n",
    "\n",
    "assert exit_code == 0, output.decode(\"utf-8\")\n",
    "print(output.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the service is started, several log messages are printed to the console, including information about the application startup, AsyncAPI specification generation, and consumer loop status.\n",
    "\n",
    "During the lifetime of the service, incoming requests will be processed by the FastKafka application and appropriate actions will be taken based on the defined Kafka consumers and producers. For example, if a message is received on the \"input_data\" Kafka topic, the `on_input_data` function will be called to process the message, and if the `to_predictions` function is called, it will produce a message to the \"predictions\" Kafka topic. The service will continue to run until it is shut down, at which point the application shutdown process will be initiated and the service will stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate and serve the documentation **locally**, you can use the built in kafka function that will do all the work for you. In the folder where the server.py file is located, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "cmd = \"fastkafka docs serve server:kafka_app\"\n",
    "\n",
    "Markdown(f\"```shell\\n{cmd}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command you should see the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_code, output = await run_script_and_cancel(\n",
    "    script=script,\n",
    "    script_file=\"server.py\",\n",
    "    cmd=cmd,\n",
    "    cancel_after=45,\n",
    ")\n",
    "assert exit_code == 0, exit_code\n",
    "print(output.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "pprint(kafka_brokers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated documentation is as follows:\n",
    "\n",
    "![Kafka_servers](images/screenshot-kafka-servers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can see the documentation generated from the `@consumes` decorator when used on the function `on_input_data` with a single parameter of type `InputData`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class InputData(BaseModel):\n",
    "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
    "    feature_1: List[float] = Field(\n",
    "        ...,\n",
    "        example=[1.2, 2.3, 4.5, 6.7, 0.1],\n",
    "        description=\"input feature 1\",\n",
    "    )\n",
    "    feature_2: List[int] = Field(\n",
    "        ...,\n",
    "        example=[2, 4, 3, 1, 0],\n",
    "        description=\"input feature 2\",\n",
    "    )\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
    "async def on_input_data(msg: InputData):\n",
    "    global model\n",
    "    score = await model.predict(feature_1=msg.feature_1, feature_2=msg.feature_2)\n",
    "    await to_predictions(user_id=msg.user_id, score=score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting documentation is generated as follows:\n",
    "\n",
    "![Kafka_consumer](images/screenshot-kafka-consumer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "import anyio\n",
    "import asyncer\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from fastkafka.helpers import (\n",
    "    consumes_messages,\n",
    "    create_missing_topics,\n",
    "    create_admin_client,\n",
    "    produce_messages,\n",
    "    wait_for_get_url,\n",
    ")\n",
    "\n",
    "from fastkafka.kafka_broker import create_kafka_test_env"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# | notest\n",
    "\n",
    "bootstrap_servers = f\"{environ['KAFKA_HOSTNAME']}:{environ['KAFKA_PORT']}\"\n",
    "\n",
    "\n",
    "create_missing_topics(\n",
    "    [\"input_data\", \"predictions\"],\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    num_partitions=48,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field, NonNegativeInt\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
    "    feature_1: List[float] = Field(\n",
    "        ...,\n",
    "        example=[1.2, 2.3, 4.5, 6.7, 0.1],\n",
    "        description=\"input feature 1\",\n",
    "    )\n",
    "    feature_2: List[int] = Field(\n",
    "        ...,\n",
    "        example=[2, 4, 3, 1, 0],\n",
    "        description=\"input feature 2\",\n",
    "    )\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n",
    "    score: float = Field(\n",
    "        ...,\n",
    "        example=0.4321,\n",
    "        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "from os import environ\n",
    "\n",
    "from fastkafka.application import FastKafka\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"localhost\": {\n",
    "        \"url\": \"localhost\",\n",
    "        \"description\": \"local development kafka broker\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"url\": \"kafka.airt.ai\",\n",
    "        \"description\": \"production kafka broker\",\n",
    "        \"port\": 9092,\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"plain\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "bootstrap_servers = f\"localhost:9999\"\n",
    "\n",
    "kafka_app = FastKafka(\n",
    "    kafka_brokers=kafka_brokers,\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "@kafka_app.consumes(topic=\"input_data\", auto_offset_reset=\"latest\", group_id=\"my_group\")\n",
    "async def on_input_data(msg: InputData):\n",
    "    global model\n",
    "    score = await model.predict(feature_1=msg.feature_1, feature_2=msg.feature_2)\n",
    "    await to_predictions(user_id=msg.user_id, score=score)\n",
    "\n",
    "\n",
    "@kafka_app.produces(topic=\"predictions\")\n",
    "async def to_predictions(user_id: int, score: float) -> Prediction:\n",
    "    prediction = Prediction(user_id=user_id, score=score)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# this is a mock up for testing, should be replaced with the real model\n",
    "class Model:\n",
    "    async def predict(self, feature_1: List[int], feature_2: List[float]) -> float:\n",
    "        return 0.87\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9049bc2321b44710abddf1ffd545d6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generating messages:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})\n",
      "[ERROR] aiokafka.cluster: Topic predictions not found in cluster metadata\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {'predictions': 1} to {'predictions': 0}. \n",
      "[WARNING] aiokafka.cluster: Topic predictions is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic input_data is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic predictions is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic input_data is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic predictions is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic input_data is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic predictions is not available during auto-create initialization\n",
      "[WARNING] aiokafka.cluster: Topic input_data is not available during auto-create initialization\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b3694df6c9434581827895be237062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'predictions':   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'input_data': 1}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c6ba31a4cb4c2dac00fd6d2c8e22c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'input_data':   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261cc5c670c14669a4dee176c62fcf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'input_data':   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 59039...\n",
      "[ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('192.168.176.6', 9999)\n",
      "[ERROR] aiokafka: Unable to update metadata from [0]\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 59039 terminated.\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 58674...\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 58674 terminated.\n",
      "[59464]: [INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'localhost:9999'}'\n",
      "[59464]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[59464]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'localhost:9999', 'auto_offset_reset': 'latest', 'max_poll_records': 100, 'group_id': 'my_group'}\n",
      "[59464]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[59464]: [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[59464]: [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[59464]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59462]: [INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'localhost:9999'}'\n",
      "[59462]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[59462]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'localhost:9999', 'auto_offset_reset': 'latest', 'max_poll_records': 100, 'group_id': 'my_group'}\n",
      "[59462]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[59462]: [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[59462]: [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[59462]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[59462]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59458]: [INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'localhost:9999'}'\n",
      "[59458]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[59458]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'localhost:9999', 'auto_offset_reset': 'latest', 'max_poll_records': 100, 'group_id': 'my_group'}\n",
      "[59458]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[59458]: [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[59458]: [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[59458]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[59458]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [INFO] fastkafka.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'localhost:9999'}'\n",
      "[59462]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[59460]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'localhost:9999', 'auto_offset_reset': 'latest', 'max_poll_records': 100, 'group_id': 'my_group'}\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[59460]: [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n",
      "[59460]: [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n",
      "[59460]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[59460]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59458]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59462]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59458]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59462]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59458]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59462]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59458]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59462]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59464]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59460]: [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group my_group\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group my_group\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group my_group\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group my_group\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group my_group\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group my_group\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group my_group\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group my_group\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group my_group\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Joined group 'my_group' (generation 1) with member_id aiokafka-0.8.0-1720b7f0-2ace-4291-a3c1-54cd94321032\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group my_group\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group my_group\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group my_group\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group my_group\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Joined group 'my_group' (generation 2) with member_id aiokafka-0.8.0-1720b7f0-2ace-4291-a3c1-54cd94321032\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: Joined group 'my_group' (generation 2) with member_id aiokafka-0.8.0-efc4e176-93fa-4985-a2b5-7f50d6ab6884\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: Joined group 'my_group' (generation 2) with member_id aiokafka-0.8.0-f1b2c12f-6fde-49be-bf53-0d9289259190\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: Joined group 'my_group' (generation 2) with member_id aiokafka-0.8.0-2977bfea-5098-477f-bdc0-27e55b1f1ad6\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: Successfully synced group my_group with generation 2\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions set() for group my_group\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Successfully synced group my_group with generation 2\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='input_data', partition=0)} for group my_group\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: Successfully synced group my_group with generation 2\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions set() for group my_group\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: Successfully synced group my_group with generation 2\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions set() for group my_group\n",
      "Starting process cleanup, this may take a few seconds...\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 59458...\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 59460...\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 59462...\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Terminating the process 59464...\n",
      "[59460]: [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[59460]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[59460]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[59462]: [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[59462]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[59462]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[59458]: [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[59458]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[59458]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[59464]: [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[59464]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[59464]: [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 59460 terminated.\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 59462 terminated.\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 59464 terminated.\n",
      "[INFO] fastkafka.server: terminate_asyncio_process(): Process 59458 terminated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_workers = 4\n",
    "\n",
    "msgs = [\n",
    "    dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "    for i in trange(5_000, desc=\"generating messages\")\n",
    "]\n",
    "\n",
    "async with create_kafka_test_env(zookeeper_port=9998, kafka_port=9999) as bootstrap_servers:\n",
    "    \n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        output = tg.soonify(run_script_and_cancel)(\n",
    "            script=script,\n",
    "            script_file=f\"server.py\",\n",
    "            cmd=f\"fastkafka run --num-workers={num_workers} server:kafka_app\",\n",
    "            cancel_after=30,\n",
    "        )\n",
    "\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"input_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        \n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"predictions\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "        \n",
    "        \n",
    "        await anyio.sleep(10)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"input_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "    \n",
    "print(output.value[1].decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
