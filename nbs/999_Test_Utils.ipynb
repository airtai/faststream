{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc959176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import contextlib\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "import multiprocessing\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "\n",
    "# [B404:blacklist] Consider possible security implications associated with the subprocess module.\n",
    "import requests\n",
    "import shutil\n",
    "import signal\n",
    "import subprocess  # nosec\n",
    "import textwrap\n",
    "import time\n",
    "import typer\n",
    "import unittest\n",
    "import unittest.mock\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "import glob\n",
    "from unittest.mock import AsyncMock, MagicMock\n",
    "\n",
    "import asyncer\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.foundation import patch\n",
    "from pydantic import BaseModel, Field\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import posix_ipc\n",
    "import nest_asyncio\n",
    "\n",
    "# from fastkafka.server import _import_from_string\n",
    "from fastkafka._components.helpers import combine_params, use_parameters_of\n",
    "from fastkafka._components.logger import get_logger, supress_timestamps\n",
    "from fastkafka.helpers import (\n",
    "    consumes_messages,\n",
    "    create_admin_client,\n",
    "    create_missing_topics,\n",
    "    in_notebook,\n",
    "    tqdm,\n",
    "    trange,\n",
    "    produce_messages,\n",
    ")\n",
    "from fastkafka._components._subprocess import terminate_asyncio_process\n",
    "from fastkafka.application import FastKafka, filter_using_signature\n",
    "from fastkafka._components.helpers import _import_from_string\n",
    "from fastkafka.helpers import in_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ce24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "\n",
    "import anyio\n",
    "import pytest\n",
    "from nbdev_mkdocs.docstring import run_examples_from_docstring\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from fastkafka.helpers import (\n",
    "    consumes_messages,\n",
    "    produce_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3eee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6902fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "kafka_server_url = (\n",
    "    os.environ[\"KAFKA_HOSTNAME\"] if \"KAFKA_HOSTNAME\" in os.environ else \"localhost\"\n",
    ")\n",
    "kafka_server_port = os.environ[\"KAFKA_PORT\"] if \"KAFKA_PORT\" in os.environ else \"9092\"\n",
    "\n",
    "aiokafka_config = {\n",
    "    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nb_safe_seed(s: str) -> Callable[[int], int]:\n",
    "    \"\"\"Gets a unique seed function for a notebook\n",
    "\n",
    "    Params:\n",
    "        s: name of the notebook used to initialize the seed function\n",
    "\n",
    "    Returns:\n",
    "        A unique seed function\n",
    "    \"\"\"\n",
    "    init_seed = int(hashlib.sha256(s.encode(\"utf-8\")).hexdigest(), 16) % (10**8)\n",
    "\n",
    "    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -> int:\n",
    "        return init_seed + x\n",
    "\n",
    "    return _get_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = nb_safe_seed(\"999_test_utils\")\n",
    "\n",
    "assert seed() == seed(0)\n",
    "assert seed() + 1 == seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def true_after(seconds: float) -> Callable[[], bool]:\n",
    "    \"\"\"Function returning True after a given number of seconds\"\"\"\n",
    "    t = datetime.now()\n",
    "\n",
    "    def _true_after(seconds: float = seconds, t: datetime = t) -> bool:\n",
    "        return (datetime.now() - t) > timedelta(seconds=seconds)\n",
    "\n",
    "    return _true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac939ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = true_after(1.1)\n",
    "assert not f()\n",
    "time.sleep(1)\n",
    "assert not f()\n",
    "time.sleep(0.1)\n",
    "assert f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "@delegates(create_missing_topics)  # type: ignore\n",
    "def create_testing_topic(\n",
    "    *,\n",
    "    topic_prefix: str = \"test_topic_\",\n",
    "    seed: Optional[int] = None,\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"Create testing topic\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from os import environ\n",
    "        from fastkafka.testing import create_testing_topic, create_admin_client\n",
    "\n",
    "        kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "        aiokafka_config = {\"bootstrap_servers\": f\"{kafka_server_url}:9092\"}\n",
    "\n",
    "        with create_testing_topic(\n",
    "            topic_prefix=\"my_topic_for_create_testing_topic_\",\n",
    "            seed=746855,\n",
    "            num_partitions=1,\n",
    "            **aiokafka_config\n",
    "        ) as topic:\n",
    "            # Check if topic is created and exists in topic list\n",
    "            kafka_admin = create_admin_client(**aiokafka_config)\n",
    "            existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "            assert topic in existing_topics\n",
    "\n",
    "        # Check if topic is deleted after exiting context\n",
    "        existing_topics = kafka_admin.list_topics().topics.keys()\n",
    "        assert topic not in existing_topics\n",
    "        ```\n",
    "\n",
    "    Args:\n",
    "        topic_prefix: topic name prefix which will be augumented with a randomly generated sufix\n",
    "        seed: seed used to generate radnom sufix\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "\n",
    "    Returns:\n",
    "        Generator returning the generated name of the created topic\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # create random topic name\n",
    "    random.seed(seed)\n",
    "    # [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n",
    "    suffix = str(random.randint(0, 10**10))  # nosec\n",
    "\n",
    "    topic = topic_prefix + suffix.zfill(3)\n",
    "\n",
    "    # delete topic if it already exists\n",
    "    admin = create_admin_client(**kwargs)  # type: ignore\n",
    "    existing_topics = admin.list_topics().topics.keys()\n",
    "    if topic in existing_topics:\n",
    "        logger.warning(f\"topic {topic} exists, deleting it...\")\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        results = {k: f.result() for k, f in fs.items()}\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "    try:\n",
    "        # create topic if needed\n",
    "        create_missing_topics([topic], **kwargs)\n",
    "        while topic not in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)\n",
    "        yield topic\n",
    "\n",
    "    finally:\n",
    "        pass\n",
    "        # cleanup if needed again\n",
    "        fs = admin.delete_topics(topics=[topic])\n",
    "        while topic in admin.list_topics().topics.keys():\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(create_testing_topic, create_missing_topics).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n",
      "[ERROR] griffe.agents.nodes: Failed to parse annotation from 'Name' node: 'NoneType' object has no attribute 'resolve'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace;font-size:.68rem\">Example:\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────── </span>code<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────</span>\n",
       "\n",
       "    from os import environ\n",
       "    from fastkafka.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"KAFKA_HOSTNAME\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    aiokafka_config = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"bootstrap_servers\"</span>: f\"<span style=\"font-weight: bold\">{</span>kafka_server_url<span style=\"font-weight: bold\">}</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9092</span>\"<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "    with <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_testing_topic</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">topic_prefix</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"my_topic_for_create_testing_topic_\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">746855</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">num_partitions</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        **aiokafka_config\n",
       "    <span style=\"font-weight: bold\">)</span> as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">create_admin_client</span><span style=\"font-weight: bold\">(</span>**aiokafka_config<span style=\"font-weight: bold\">)</span>\n",
       "        existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">kafka_admin.list_topics</span><span style=\"font-weight: bold\">()</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.topics.keys</span><span style=\"font-weight: bold\">()</span>\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">─────────────────────────────── </span>stdout supressed<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───────────────────────────────</span>\n",
       "N/A\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────── </span>stderr<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ────────────────────────────────────</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example:\n",
       "\u001b[92m───────────────────────────────────── \u001b[0mcode\u001b[92m ─────────────────────────────────────\u001b[0m\n",
       "\n",
       "    from os import environ\n",
       "    from fastkafka.testing import create_testing_topic, create_admin_client\n",
       "\n",
       "    kafka_server_url = environ\u001b[1m[\u001b[0m\u001b[32m\"KAFKA_HOSTNAME\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    aiokafka_config = \u001b[1m{\u001b[0m\u001b[32m\"bootstrap_servers\"\u001b[0m: f\"\u001b[1m{\u001b[0mkafka_server_url\u001b[1m}\u001b[0m:\u001b[1;36m9092\u001b[0m\"\u001b[1m}\u001b[0m\n",
       "\n",
       "    with \u001b[1;35mcreate_testing_topic\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mtopic_prefix\u001b[0m=\u001b[32m\"my_topic_for_create_testing_topic_\"\u001b[0m,\n",
       "        \u001b[33mseed\u001b[0m=\u001b[1;36m746855\u001b[0m,\n",
       "        \u001b[33mnum_partitions\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "        **aiokafka_config\n",
       "    \u001b[1m)\u001b[0m as topic:\n",
       "        # Check if topic is created and exists in topic list\n",
       "        kafka_admin = \u001b[1;35mcreate_admin_client\u001b[0m\u001b[1m(\u001b[0m**aiokafka_config\u001b[1m)\u001b[0m\n",
       "        existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        assert topic in existing_topics\n",
       "\n",
       "    # Check if topic is deleted after exiting context\n",
       "    existing_topics = \u001b[1;35mkafka_admin.list_topics\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.topics.keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    assert topic not in existing_topics\n",
       "\n",
       "\u001b[92m─────────────────────────────── \u001b[0mstdout supressed\u001b[92m ───────────────────────────────\u001b[0m\n",
       "N/A\n",
       "\u001b[92m──────────────────────────────────── \u001b[0mstderr\u001b[92m ────────────────────────────────────\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_examples_from_docstring(create_testing_topic(width=120), supress_stdout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "@delegates(produce_messages)  # type: ignore\n",
    "@delegates(create_testing_topic, keep=True)  # type: ignore\n",
    "async def create_and_fill_testing_topic(**kwargs: Dict[str, str]) -> AsyncIterator[str]:\n",
    "    \"\"\"Create testing topic with a random sufix in the same and fill it will messages\n",
    "\n",
    "    Args:\n",
    "        topic_names: a list of topic names\n",
    "        num_partitions: Number of partitions to create\n",
    "        replication_factor: Replication factor of partitions, or -1 if replica_assignment is used.\n",
    "        replica_assignment: List of lists with the replication assignment for each new partition.\n",
    "        new_topic_config: topic level config parameters as defined here: https://kafka.apache.org/documentation.html#topicconfigs\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        topic: Topic name\n",
    "        msgs: a list of messages to produce\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "    \"\"\"\n",
    "\n",
    "    with create_testing_topic(\n",
    "        **use_parameters_of(create_testing_topic, **kwargs)\n",
    "    ) as topic:\n",
    "        await produce_messages(\n",
    "            topic=topic, **use_parameters_of(produce_messages, **kwargs)\n",
    "        )\n",
    "\n",
    "        yield topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0930893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(create_and_fill_testing_topic, create_missing_topics), produce_messages).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00149fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka.helpers: create_missing_topics(['my_topic_test_create_and_fill_testing_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_test_create_and_fill_testing_topic_9167024629,num_partitions=3)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad1e30c2e764fc9b6ca392237009632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "producing to 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?it…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_test_create_and_fill_testing_topic_9167024629'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_test_create_and_fill_testing_topic_9167024629': 3}. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5449ecce8f484c8290f986052b03a23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consuming from 'my_topic_test_create_and_fill_testing_topic_9167024629':   0%|          | 0/120000 [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "msgs_count = 120_000\n",
    "\n",
    "\n",
    "class Hello(BaseModel):\n",
    "    msg: str\n",
    "\n",
    "\n",
    "msgs_count = 120_000\n",
    "msgs = (\n",
    "    [b\"Hello world bytes\" for _ in range(msgs_count // 3)]\n",
    "    + [f\"Hello world as string for the {i+1}. time!\" for i in range(msgs_count // 3)]\n",
    "    + [\n",
    "        Hello(msg=\"Hello workd as Pydantic object for the {i+1}. time!\")\n",
    "        for i in range(msgs_count // 3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async with create_and_fill_testing_topic(\n",
    "    topic_prefix=\"my_topic_test_create_and_fill_testing_topic_\",\n",
    "    msgs=msgs,\n",
    "    seed=1,\n",
    "    **aiokafka_config,\n",
    ") as topic:\n",
    "    await consumes_messages(\n",
    "        topic=topic,\n",
    "        msgs_count=msgs_count,\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        #         group_id=\"test_group\",\n",
    "        **aiokafka_config,\n",
    "    )\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mock_AIOKafkaProducer_send() -> Generator[unittest.mock.Mock, None, None]:\n",
    "    \"\"\"Mocks **send** method of **AIOKafkaProducer**\"\"\"\n",
    "    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n",
    "\n",
    "        async def _f():\n",
    "            pass\n",
    "\n",
    "        mock.return_value = asyncio.create_task(_f())\n",
    "\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def change_dir(d: str) -> Generator[None, None, None]:\n",
    "    curdir = os.getcwd()\n",
    "    os.chdir(d)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tests\n",
    "with TemporaryDirectory() as d:\n",
    "    original_wd = os.getcwd()\n",
    "    assert original_wd != d\n",
    "    with change_dir(d):\n",
    "        assert os.getcwd() == d\n",
    "    assert os.getcwd() == original_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85420005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def run_script_and_cancel(\n",
    "    script: str,\n",
    "    *,\n",
    "    script_file: Optional[str] = None,\n",
    "    cmd: Optional[str] = None,\n",
    "    cancel_after: int = 10,\n",
    "    app_name: str = \"app\",\n",
    "    kafka_app_name: str = \"kafka_app\",\n",
    "    generate_docs: bool = False,\n",
    ") -> Tuple[int, bytes]:\n",
    "    \"\"\"Run script and cancel after predefined time\n",
    "\n",
    "    Args:\n",
    "        script: a python source code to be executed in a separate subprocess\n",
    "        script_file: name of the script where script source will be saved\n",
    "        cmd: command to execute. If None, it will be set to 'python3 -m {Path(script_file).stem}'\n",
    "        cancel_after: number of seconds before sending SIGTERM signal\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing exit code and combined stdout and stderr as a binary string\n",
    "    \"\"\"\n",
    "    if script_file is None:\n",
    "        script_file = \"script.py\"\n",
    "\n",
    "    if cmd is None:\n",
    "        cmd = f\"python3 -m {Path(script_file).stem}\"\n",
    "\n",
    "    with TemporaryDirectory() as d:\n",
    "        consumer_script = Path(d) / script_file\n",
    "\n",
    "        with open(consumer_script, \"w\") as file:\n",
    "            file.write(script)\n",
    "\n",
    "        if generate_docs:\n",
    "            logger.info(\n",
    "                f\"Generating docs for: {Path(script_file).stem}:{kafka_app_name}\"\n",
    "            )\n",
    "            try:\n",
    "                kafka_app: FastKafka = _import_from_string(\n",
    "                    f\"{Path(script_file).stem}:{kafka_app_name}\"\n",
    "                )\n",
    "                await asyncer.asyncify(kafka_app.create_docs)()\n",
    "            except Exception as e:\n",
    "                logger.warning(\n",
    "                    f\"Generating docs failed for: {Path(script_file).stem}:{kafka_app_name}, ignoring it for now.\"\n",
    "                )\n",
    "\n",
    "        proc = subprocess.Popen(  # nosec: [B603:subprocess_without_shell_equals_true] subprocess call - check for execution of untrusted input.\n",
    "            shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=d\n",
    "        )\n",
    "        await asyncio.sleep(cancel_after)\n",
    "        proc.terminate()\n",
    "        output, _ = proc.communicate()\n",
    "\n",
    "        return (proc.returncode, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09054da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 0\n",
    "script = \"\"\"\n",
    "from time import sleep\n",
    "print(\"hello\")\n",
    "sleep({t})\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script.format(t=0), cancel_after=2)\n",
    "assert exit_code == 0, exit_code\n",
    "assert output.decode(\"utf-8\") == \"hello\\n\", output.decode(\"utf-8\")\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script.format(t=5), cancel_after=2)\n",
    "assert exit_code < 0, exit_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8484fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 1\n",
    "script = \"exit(1)\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 1\n",
    "assert output.decode(\"utf-8\") == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aaf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exit code 0 and output to stdout and stderr\n",
    "script = \"\"\"\n",
    "import sys\n",
    "sys.stderr.write(\"hello from stderr\\\\n\")\n",
    "sys.stderr.flush()\n",
    "print(\"hello, exiting with exit code 0\")\n",
    "exit(0)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 0, exit_code\n",
    "assert (\n",
    "    output.decode(\"utf-8\") == \"hello from stderr\\nhello, exiting with exit code 0\\n\"\n",
    "), output.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Check random exit code and output\n",
    "script = \"\"\"\n",
    "print(\"hello\\\\nexiting with exit code 143\")\n",
    "exit(143)\n",
    "\"\"\"\n",
    "\n",
    "exit_code, output = await run_script_and_cancel(script, cancel_after=1)\n",
    "\n",
    "assert exit_code == 143\n",
    "assert output.decode(\"utf-8\") == \"hello\\nexiting with exit code 143\\n\"\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fded319",
   "metadata": {},
   "source": [
    "### Local Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552eb74",
   "metadata": {},
   "source": [
    "#### Kafka and zookeeper config helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_zookeeper_config_string(\n",
    "    data_dir: Union[str, Path], # the directory where the snapshot is stored.\n",
    "    zookeeper_port: int = 2181, # the port at which the clients will connect\n",
    ") -> str:\n",
    "    \"\"\"Generates a zookeeeper configuration string that can be exported to file \n",
    "    and used to start a zookeeper instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "        zookeeper_port: Port for clients (Kafka brokes) to connect \n",
    "    Returns:\n",
    "        Zookeeper configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    zookeeper_config = f\"\"\"dataDir={data_dir}/zookeeper\n",
    "clientPort={zookeeper_port}\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "    return zookeeper_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_zookeeper_config_string(data_dir=\"..\") == \"\"\"dataDir=../zookeeper\n",
    "clientPort=2181\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "assert get_zookeeper_config_string(data_dir=\"..\", zookeeper_port = 100) == \"\"\"dataDir=../zookeeper\n",
    "clientPort=100\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_kafka_config_string(\n",
    "    data_dir: Union[str, Path],\n",
    "    zookeeper_port: int = 2181,\n",
    "    listener_port: int = 9092\n",
    ") -> str:\n",
    "    \"\"\"Generates a kafka broker configuration string that can be exported to file \n",
    "    and used to start a kafka broker instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the kafka broker instance will save data\n",
    "        zookeeper_port: Port on which the zookeeper instance is running\n",
    "        listener_port: Port on which the clients (producers and consumers) can connect\n",
    "    Returns:\n",
    "        Kafka broker configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    kafka_config = f\"\"\"broker.id=0\n",
    "\n",
    "############################# Socket Server Settings #############################\n",
    "\n",
    "# The address the socket server listens on. If not configured, the host name will be equal to the value of\n",
    "# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.\n",
    "#   FORMAT:\n",
    "#     listeners = listener_name://host_name:port\n",
    "#   EXAMPLE:\n",
    "#     listeners = PLAINTEXT://your.host.name:9092\n",
    "listeners=PLAINTEXT://:{listener_port}\n",
    "\n",
    "# Listener name, hostname and port the broker will advertise to clients.\n",
    "# If not set, it uses the value for \"listeners\".\n",
    "#advertised.listeners=PLAINTEXT://your.host.name:9092\n",
    "\n",
    "# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details\n",
    "#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
    "\n",
    "# The number of threads that the server uses for receiving requests from the network and sending responses to the network\n",
    "num.network.threads=3\n",
    "\n",
    "# The number of threads that the server uses for processing requests, which may include disk I/O\n",
    "num.io.threads=8\n",
    "\n",
    "# The send buffer (SO_SNDBUF) used by the socket server\n",
    "socket.send.buffer.bytes=102400\n",
    "\n",
    "# The receive buffer (SO_RCVBUF) used by the socket server\n",
    "socket.receive.buffer.bytes=102400\n",
    "\n",
    "# The maximum size of a request that the socket server will accept (protection against OOM)\n",
    "socket.request.max.bytes=104857600\n",
    "\n",
    "\n",
    "############################# Log Basics #############################\n",
    "\n",
    "# A comma separated list of directories under which to store log files\n",
    "log.dirs={data_dir}/kafka_logs\n",
    "\n",
    "# The default number of log partitions per topic. More partitions allow greater\n",
    "# parallelism for consumption, but this will also result in more files across\n",
    "# the brokers.\n",
    "num.partitions=1\n",
    "\n",
    "# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n",
    "# This value is recommended to be increased for installations with data dirs located in RAID array.\n",
    "num.recovery.threads.per.data.dir=1\n",
    "\n",
    "offsets.topic.replication.factor=1\n",
    "transaction.state.log.replication.factor=1\n",
    "transaction.state.log.min.isr=1\n",
    "\n",
    "# The number of messages to accept before forcing a flush of data to disk\n",
    "log.flush.interval.messages=10000\n",
    "\n",
    "# The maximum amount of time a message can sit in a log before we force a flush\n",
    "log.flush.interval.ms=1000\n",
    "\n",
    "# The minimum age of a log file to be eligible for deletion due to age\n",
    "log.retention.hours=168\n",
    "\n",
    "# A size-based retention policy for logs. Segments are pruned from the log unless the remaining\n",
    "# segments drop below log.retention.bytes. Functions independently of log.retention.hours.\n",
    "log.retention.bytes=1073741824\n",
    "\n",
    "# The maximum size of a log segment file. When this size is reached a new log segment will be created.\n",
    "log.segment.bytes=1073741824\n",
    "\n",
    "# The interval at which log segments are checked to see if they can be deleted according to the retention policies\n",
    "log.retention.check.interval.ms=300000\n",
    "\n",
    "# Zookeeper connection string (see zookeeper docs for details).\n",
    "zookeeper.connect=localhost:{zookeeper_port}\n",
    "\n",
    "# Timeout in ms for connecting to zookeeper\n",
    "zookeeper.connection.timeout.ms=18000\n",
    "\n",
    "# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.\n",
    "group.initial.rebalance.delay.ms=0\n",
    "\"\"\"\n",
    "\n",
    "    return kafka_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = get_kafka_config_string(data_dir=\"..\", listener_port=9999)\n",
    "assert \"log.dirs=../kafka_logs\" in actual\n",
    "assert \"listeners=PLAINTEXT://:9999\" in actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c114df",
   "metadata": {},
   "source": [
    "#### LocalKafkaBroker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf939a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LocalKafkaBroker:\n",
    "    \"\"\"LocalKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.\n",
    "    \n",
    "    Attributes:\n",
    "        lock (ilock.Lock): Lock used for synchronizing the install process between multiple kafka brokers.\n",
    "    \"\"\"\n",
    "    lock = posix_ipc.Semaphore('install_lock:LocalKafkaBroker', posix_ipc.O_CREAT, initial_value=1)\n",
    "    \n",
    "    @delegates(get_kafka_config_string) # type: ignore\n",
    "    @delegates(get_zookeeper_config_string, keep=True) # type: ignore\n",
    "    def __init__(self, **kwargs: Dict[str, Any]):\n",
    "        \"\"\" Initialises the LocalKafkaBroker object\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "            listener_port: Port on which the clients (producers and consumers) can connect\n",
    "        \"\"\"\n",
    "        self.zookeeper_kwargs = filter_using_signature(get_zookeeper_config_string, **kwargs)\n",
    "        self.kafka_kwargs = filter_using_signature(get_kafka_config_string, **kwargs)\n",
    "        self.temporary_directory: Optional[TemporaryDirectory] = None\n",
    "        self.temporary_directory_path: Optional[Path] = None\n",
    "        self.kafka_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.zookeeper_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.started = True\n",
    "        \n",
    "    @classmethod\n",
    "    def _install(cls) -> None:\n",
    "        \"\"\"Prepares the environment for running Kafka brokers.\n",
    "            Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance asynchronously\n",
    "            Returns:\n",
    "               Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "            Returns:\n",
    "               Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "            Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "            Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __enter__(self) -> str:\n",
    "#         LocalKafkaBroker._install()\n",
    "        return self.start()\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        self.stop()\n",
    "        \n",
    "    async def __aenter__(self) -> str:\n",
    "#         LocalKafkaBroker._install()\n",
    "        return await self._start()\n",
    "    \n",
    "    async def __aexit__(self, *args, **kwargs):\n",
    "        await self._stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(LocalKafkaBroker, get_kafka_config_string), get_zookeeper_config_string).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac507521",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "*   retval=0\n",
      "\n",
      "[INFO] [PID=27129]: Entering: 1676973891.4919136\n",
      "[INFO] [PID=27129]:  - 2023-02-21 10:04:51.492002\n",
      "[INFO] [PID=27129]: Exiting: 1676973892.4934962\n",
      "[INFO] [PID=27129]:  - 2023-02-21 10:04:52.493579\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "*   retval=0\n",
      "\n",
      "[INFO] [PID=27130]: Entering: 1676973893.4956\n",
      "[INFO] [PID=27130]:  - 2023-02-21 10:04:53.495723\n",
      "[INFO] [PID=27130]: Exiting: 1676973894.4967983\n",
      "[INFO] [PID=27130]:  - 2023-02-21 10:04:54.496908\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "*   retval=0\n",
      "\n",
      "[INFO] [PID=27131]: Entering: 1676973892.4936507\n",
      "[INFO] [PID=27131]:  - 2023-02-21 10:04:52.493757\n",
      "[INFO] [PID=27131]: Exiting: 1676973893.4947634\n",
      "[INFO] [PID=27131]:  - 2023-02-21 10:04:53.494857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "script = \"\"\"\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from fastcore.foundation import patch\n",
    "import posix_ipc\n",
    "\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka.testing import LocalKafkaBroker\n",
    "\n",
    "pid = os.getpid()\n",
    "\n",
    "logger = get_logger(f\"[PID={pid}]\")\n",
    "\n",
    "@patch(cls_method=True) # type: ignore\n",
    "def check_cls_lock(cls: LocalKafkaBroker) -> None:\n",
    "    with cls.lock:\n",
    "       logger.info(f\"Entering: {time.time()}\")\n",
    "       logger.info(f\" - {datetime.now()}\")\n",
    "       time.sleep(1)\n",
    "       logger.info(f\"Exiting: {time.time()}\")\n",
    "       logger.info(f\" - {datetime.now()}\")\n",
    "\n",
    "broker = LocalKafkaBroker()\n",
    "broker.check_cls_lock()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_times(stdout: str) -> Tuple[float, float]:\n",
    "    stdout_lines = stdout.split(\"\\n\")\n",
    "    enter_time = float([line.split(\" \")[-1] for line in stdout_lines if \"Entering\" in line][0])\n",
    "    exit_time = float([line.split(\" \")[-1] for line in stdout_lines if \"Exiting\" in line][0])\n",
    "    return (enter_time, exit_time)\n",
    "    \n",
    "\n",
    "def check_overlap(intervals: List[Tuple[float]]) -> bool:\n",
    "    for i, (test_start, test_stop) in enumerate(intervals):\n",
    "        for start, stop in intervals[i+1:]:\n",
    "            if (test_start < start < test_stop or start < test_start < stop):\n",
    "                return True\n",
    "    return False\n",
    "    \n",
    "async with asyncer.create_task_group() as tg:\n",
    "    tx = [tg.soonify(run_script_and_cancel)(script, cancel_after=30) for _ in range(3)]\n",
    "retvals, stdouts = zip(*[t.value for t in tx])\n",
    "for retval, stdout in zip(retvals, stdouts):\n",
    "    print(\"*\"*100)\n",
    "    print(f\"*   {retval=}\")\n",
    "    print()\n",
    "    print(stdout.decode(\"utf-8\"))\n",
    "    print()\n",
    "    \n",
    "times = [get_times(stdout.decode(\"utf-8\")) for stdout in stdouts]\n",
    "assert not check_overlap(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7480229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_java() -> None:\n",
    "    \"\"\"Checks if jdk-11 is installed on the machine and installs it if not\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    potential_jdk_path = list(Path(os.environ[\"HOME\"]+\"/.jdk\").glob(\"jdk-11*\"))\n",
    "    if potential_jdk_path != []:\n",
    "        logger.info(\"Java is already installed.\")\n",
    "        if not shutil.which(\"java\"):\n",
    "            logger.info(\"But not exported to PATH, exporting...\")\n",
    "            os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{potential_jdk_path[0]}/bin\"\n",
    "    else:\n",
    "        logger.info(\"Installing Java...\")\n",
    "        logger.info(\" - installing install-jdk...\")\n",
    "        subprocess.run([\"pip\", \"install\", \"install-jdk\"], check=True) # nosec\n",
    "        import jdk\n",
    "\n",
    "        logger.info(\" - installing jdk...\")\n",
    "        jdk_bin_path = jdk.install(\"11\")\n",
    "        print(jdk_bin_path)\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{jdk_bin_path}/bin\"\n",
    "        logger.info(\"Java installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4499887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "install_java()\n",
    "assert shutil.which(\"java\")\n",
    "install_java()\n",
    "assert shutil.which(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a57f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_kafka() -> None:\n",
    "    \"\"\"Checks if kafka is installed on the machine and installs it if not\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    kafka_version = \"3.3.2\"\n",
    "    kafka_fname = f\"kafka_2.13-{kafka_version}\"\n",
    "    kafka_url = f\"https://dlcdn.apache.org/kafka/{kafka_version}/{kafka_fname}.tgz\"\n",
    "    local_path = Path(os.environ[\"HOME\"]) / \".local\"\n",
    "    local_path.mkdir(exist_ok=True, parents=True)\n",
    "    tgz_path = local_path / f\"{kafka_fname}.tgz\"\n",
    "    kafka_path = local_path / f\"{kafka_fname}\"\n",
    "    \n",
    "    if (kafka_path / \"bin\").exists():\n",
    "        logger.info(\"Kafka is already installed.\")\n",
    "        if not shutil.which(\"kafka-server-start.sh\"):\n",
    "            logger.info(\"But not exported to PATH, exporting...\")\n",
    "            os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "    else:\n",
    "        logger.info(\"Installing Kafka...\")\n",
    "\n",
    "        response = requests.get(kafka_url, stream=True, )\n",
    "        try:\n",
    "            total = response.raw.length_remaining // 128\n",
    "        except Exception:\n",
    "            total = None\n",
    "\n",
    "        with open(tgz_path, \"wb\") as f:\n",
    "            for data in tqdm(response.iter_content(chunk_size=128), total=total):\n",
    "                f.write(data)\n",
    "\n",
    "        with tarfile.open(tgz_path) as tar:\n",
    "            for tarinfo in tar:\n",
    "                tar.extract(tarinfo, local_path)\n",
    "\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "        logger.info(f\"Kafka installed in {kafka_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "@patch(cls_method=True) # type: ignore\n",
    "def _install(cls: LocalKafkaBroker) -> None:\n",
    "    with cls.lock:\n",
    "        install_java()\n",
    "        install_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4600ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = LocalKafkaBroker()\n",
    "broker._install()\n",
    "assert shutil.which(\"java\")\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a076ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _start(self: LocalKafkaBroker) -> str:\n",
    "    self._install()\n",
    "\n",
    "    self.temporary_directory = TemporaryDirectory()\n",
    "    self.temporary_directory_path = Path(self.temporary_directory.__enter__())\n",
    "\n",
    "    async def write_config_and_run(\n",
    "        config: str, config_path: Union[str, Path], run_cmd: str\n",
    "    ) -> asyncio.subprocess.Process:\n",
    "        with open(config_path, \"w\") as f:\n",
    "            f.write(config)\n",
    "\n",
    "        return await asyncio.create_subprocess_exec(\n",
    "            run_cmd,\n",
    "            config_path,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stdin=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "    # start_zookeeper\n",
    "\n",
    "    logger.info(\"Starting zookeeper...\")\n",
    "    zookeeper_config_path = self.temporary_directory_path / \"zookeeper.properties\"\n",
    "    self.zookeeper_task = await write_config_and_run(\n",
    "        get_zookeeper_config_string(\n",
    "            data_dir=self.temporary_directory_path, **self.zookeeper_kwargs\n",
    "        ),\n",
    "        zookeeper_config_path,\n",
    "        \"zookeeper-server-start.sh\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"Zookeeper started, sleeping for 5 seconds...\")\n",
    "    await asyncio.sleep(5)\n",
    "    if self.zookeeper_task.returncode is not None:\n",
    "        raise ValueError(f\"Could not start zookeeper with params: {self.zookeeper_kwargs}\")\n",
    "\n",
    "    # start_kafka\n",
    "\n",
    "    logger.info(\"Starting Kafka broker...\")\n",
    "    kafka_config_path = self.temporary_directory_path / \"kafka.properties\"\n",
    "    self.kafka_task = await write_config_and_run(\n",
    "        get_kafka_config_string(\n",
    "            data_dir=self.temporary_directory_path, **self.kafka_kwargs\n",
    "        ),\n",
    "        kafka_config_path,\n",
    "        \"kafka-server-start.sh\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"Kafka broker started, sleeping for 5 seconds...\")\n",
    "    await asyncio.sleep(5)\n",
    "    if self.kafka_task.returncode is not None:\n",
    "        raise ValueError(f\"Could not start Kafka broker with params: {self.kafka_kwargs}\")\n",
    "\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    retval = f\"127.0.0.1:{listener_port}\"\n",
    "    logger.info(f\"Local Kafka broker up and running on {retval}\")\n",
    "    return retval\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _stop(self: LocalKafkaBroker) -> None:\n",
    "    await terminate_asyncio_process(self.kafka_task)  # type: ignore\n",
    "    await terminate_asyncio_process(self.zookeeper_task)  # type: ignore\n",
    "    self.temporary_directory.__exit__(None, None, None)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = LocalKafkaBroker(zookeeper_port=9798, listener_port=9689)\n",
    "async with broker:\n",
    "    pass\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 9939\n",
    "\n",
    "broker_1 = LocalKafkaBroker(zookeeper_port=port, listener_port=9941)\n",
    "broker_2 = LocalKafkaBroker(zookeeper_port=port, listener_port=9942)\n",
    "async with broker_1:\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        async with broker_2:\n",
    "            pass\n",
    "\n",
    "expected = (\n",
    "    \"Could not start zookeeper with params: {\" + f\"'zookeeper_port': {port}\" + \"}\"\n",
    ")\n",
    "assert e.value.args == (expected, )\n",
    "\n",
    "for broker in [broker_2]:\n",
    "    print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "    zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "    print(zookeeper_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@patch # type: ignore\n",
    "def start(self: LocalKafkaBroker) -> str:\n",
    "    \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.start(): entering...\")\n",
    "    try:\n",
    "        # get or create loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): RuntimeError raised when calling asyncio.get_event_loop(): {e}\")\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): asyncio.new_event_loop()\")\n",
    "            loop = asyncio.new_event_loop()\n",
    "            \n",
    "        # start zookeeper and kafka broker in the loop\n",
    "        try:\n",
    "            retval = loop.run_until_complete(self._start())\n",
    "            logger.info(f\"{self.__class__.__name__}.start(): returning {retval}\")\n",
    "            self.started = True\n",
    "            return retval\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): RuntimeError raised for loop ({loop}): {e}\")\n",
    "            logger.warning(f\"{self.__class__.__name__}.start(): calling nest_asyncio.apply()\")\n",
    "            nest_asyncio.apply(loop)\n",
    "\n",
    "            retval = loop.run_until_complete(self._start())\n",
    "            logger.info(f\"{self.__class__}.start(): returning {retval}\")\n",
    "            self.started = True\n",
    "            return retval\n",
    "        \n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.start(): exited.\")\n",
    "\n",
    "@patch # type: ignore\n",
    "def stop(self: LocalKafkaBroker) -> None:\n",
    "    \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.stop(): entering...\")\n",
    "    try:\n",
    "        if not self.started:\n",
    "            raise RuntimeError(\"LocalKafkaBroker not started yet, please call LocalKafkaBroker.start() before!\")\n",
    "            \n",
    "        loop = asyncio.get_event_loop()\n",
    "        self.started = False\n",
    "        return loop.run_until_complete(self._stop())\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.stop(): exited.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = LocalKafkaBroker(zookeeper_port=9998, listener_port=9789)\n",
    "with broker:\n",
    "    print(\"Hello world!\")\n",
    "\n",
    "print(\"*\"*50 + \"ZOOKEEPER LOGS\" + \"+\"*50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "\n",
    "print(\"*\"*50 + \"KAFKA LOGS\" + \"+\"*50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with LocalKafkaBroker(zookeeper_port=9998, listener_port=9789) as bootstrap_servers:\n",
    "    print(bootstrap_servers)\n",
    "    assert bootstrap_servers == \"127.0.0.1:9789\"\n",
    "    \n",
    "    msgs = [\n",
    "        dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "        for i in trange(100_000, desc=\"generating messages\")\n",
    "    ]\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        await anyio.sleep(2)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with LocalKafkaBroker(zookeeper_port=9998, listener_port=9789) as bootstrap_servers:\n",
    "    print(bootstrap_servers)\n",
    "    assert bootstrap_servers == \"127.0.0.1:9789\"\n",
    "    \n",
    "    msgs = [\n",
    "        dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "        for i in trange(100_000, desc=\"generating messages\")\n",
    "    ]\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        await anyio.sleep(2)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
