{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b22f68",
   "metadata": {},
   "source": [
    "# @consumes basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "import asyncio\n",
    "import platform\n",
    "\n",
    "import asyncer\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "from fastkafka._components._subprocess import terminate_asyncio_process\n",
    "from fastkafka._testing.apache_kafka_broker import run_and_match\n",
    "from fastkafka.testing import ApacheKafkaBroker, run_script_and_cancel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c655e4f",
   "metadata": {},
   "source": [
    "You can use `@consumes` decorator to consume messages from Kafka topics. \n",
    "\n",
    "In this guide we will create a simple FastKafka app that will consume `HelloWorld` messages from hello_world topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18535f2f",
   "metadata": {},
   "source": [
    "## Import `FastKafka`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef969d",
   "metadata": {},
   "source": [
    "To use the `@consumes` decorator, first we need to import the base FastKafka app to create our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a8fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from fastkafka import FastKafka\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "import_fastkafka = \"\"\"from fastkafka import FastKafka\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{import_fastkafka}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9cb9f",
   "metadata": {},
   "source": [
    "In this demo we will log the messages to the output so that we can inspect and verify that our app is consuming properly. For that we need to import the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27e68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from fastkafka._components.logger import get_logger\n",
       "\n",
       "logger = get_logger(__name__)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "import_logger = \"\"\"from fastkafka._components.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\"\"\"\n",
    "\n",
    "md(f\"```python\\n{import_logger}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb37e9",
   "metadata": {},
   "source": [
    "## Define the structure of the messages\n",
    "Next, you need to define the structure of the messages you want to consume from the topic using [pydantic](https://docs.pydantic.dev/). For the guide we'll stick to something basic, but you are free to define any complex message structure you wish in your project, just make sure it can be JSON encoded.\n",
    "\n",
    "Let's import `BaseModel` and `Field` from pydantic and create a simple `HelloWorld` class containing one string parameter `msg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83265a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from pydantic import BaseModel, Field\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "import_pydantic = \"\"\"from pydantic import BaseModel, Field\n",
    "\"\"\"\n",
    "md(f\"```python\\n{import_pydantic}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e6ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "class HelloWorld(BaseModel):\n",
       "    msg: str = Field(\n",
       "        ...,\n",
       "        example=\"Hello\",\n",
       "        description=\"Demo hello world message\",\n",
       "    )\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "define_HelloWorld = \"\"\"\n",
    "class HelloWorld(BaseModel):\n",
    "    msg: str = Field(\n",
    "        ...,\n",
    "        example=\"Hello\",\n",
    "        description=\"Demo hello world message\",\n",
    "    )\n",
    "\"\"\"\n",
    "md(f\"```python\\n{define_HelloWorld}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d810c",
   "metadata": {},
   "source": [
    "## Create a base FastKafka app\n",
    "\n",
    "Now we will create and define a base FastKafka app, replace the `<url_of_your_kafka_bootstrap_server>` and `<port_of_your_kafka_bootstrap_server>` with the actual values of your Kafka bootstrap server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732642f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "\n",
       "kafka_brokers = {\n",
       "    \"demo_broker\": {\n",
       "        \"url\": \"<url_of_your_kafka_bootstrap_server>\",\n",
       "        \"description\": \"local demo kafka broker\",\n",
       "        \"port\": \"<port_of_your_kafka_bootstrap_server>\",\n",
       "    }\n",
       "}\n",
       "\n",
       "app = FastKafka(kafka_brokers=kafka_brokers)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "create_app = \"\"\"\n",
    "\n",
    "kafka_brokers = {\n",
    "    \"demo_broker\": {\n",
    "        \"url\": \"<url_of_your_kafka_bootstrap_server>\",\n",
    "        \"description\": \"local demo kafka broker\",\n",
    "        \"port\": \"<port_of_your_kafka_bootstrap_server>\",\n",
    "    }\n",
    "}\n",
    "\n",
    "app = FastKafka(kafka_brokers=kafka_brokers)\n",
    "\"\"\"\n",
    "md(f\"```python\\n{create_app}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41ebe6",
   "metadata": {},
   "source": [
    "## Create a consumer function and decorate it with `@consumes`\n",
    "\n",
    "Let's create a consumer function that will consume `HelloWorld` messages from *hello_world* topic and log them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd7a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "@app.consumes()\n",
       "async def on_hello_world(msg: HelloWorld):\n",
       "    logger.info(f\"Got msg: {msg}\")\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "decorate_consumes = \"\"\"@app.consumes()\n",
    "async def on_hello_world(msg: HelloWorld):\n",
    "    logger.info(f\"Got msg: {msg}\")\n",
    "\"\"\"\n",
    "md(f\"```python\\n{decorate_consumes}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf5198",
   "metadata": {},
   "source": [
    "The function decorated with the `@consumes` decorator will be called when a message is produced to Kafka.\n",
    "\n",
    "The message will then be injected into the typed *msg* argument of the function and its type will be used to parse the message.\n",
    "\n",
    "In this example case, when the message is sent into a *hello_world* topic, it will be parsed into a HelloWorld class and `on_hello_world` function will be called with the parsed class as *msg* argument value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c95f7",
   "metadata": {},
   "source": [
    "## Final app\n",
    "\n",
    "Your app code should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d88d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "from fastkafka import FastKafka\n",
       "from pydantic import BaseModel, Field\n",
       "from fastkafka._components.logger import get_logger\n",
       "\n",
       "logger = get_logger(__name__)\n",
       "\n",
       "class HelloWorld(BaseModel):\n",
       "    msg: str = Field(\n",
       "        ...,\n",
       "        example=\"Hello\",\n",
       "        description=\"Demo hello world message\",\n",
       "    )\n",
       "\n",
       "\n",
       "kafka_brokers = {\n",
       "    \"demo_broker\": {\n",
       "        \"url\": \"<url_of_your_kafka_bootstrap_server>\",\n",
       "        \"description\": \"local demo kafka broker\",\n",
       "        \"port\": \"<port_of_your_kafka_bootstrap_server>\",\n",
       "    }\n",
       "}\n",
       "\n",
       "app = FastKafka(kafka_brokers=kafka_brokers)\n",
       "@app.consumes()\n",
       "async def on_hello_world(msg: HelloWorld):\n",
       "    logger.info(f\"Got msg: {msg}\")\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "consumes_example = (\n",
    "    import_fastkafka\n",
    "    + import_pydantic\n",
    "    + import_logger\n",
    "    + define_HelloWorld\n",
    "    + create_app\n",
    "    + decorate_consumes\n",
    ")\n",
    "md(f\"```python\\n{consumes_example}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa74215",
   "metadata": {},
   "source": [
    "## Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe529ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Now we can run the app. Copy the code above in consumer_example.py and run it by running\n",
       "```shell\n",
       "fastkafka run --num-workers=1 --kafka-broker=demo_broker consumer_example:app\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "\n",
    "script_file = \"consumer_example.py\"\n",
    "filename = script_file.split(\".py\")[0]\n",
    "cmd = f\"fastkafka run --num-workers=1 --kafka-broker=demo_broker {filename}:app\"\n",
    "md(\n",
    "    f\"Now we can run the app. Copy the code above in {script_file} and run it by running\\n```shell\\n{cmd}\\n```\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66be2b9",
   "metadata": {},
   "source": [
    "After running the command, you should see this output in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ec62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-06-02 12:08:39.473 [INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): entering...\n",
      "23-06-02 12:08:39.481 [WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): (<_WindowsSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "23-06-02 12:08:39.481 [WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "23-06-02 12:08:39.481 [INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "23-06-02 12:08:39.489 [INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "23-06-02 12:08:39.489 [INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "23-06-02 12:08:39.497 [INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "23-06-02 12:08:39.497 [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...\n",
      "\n",
      "23-06-02 12:08:39.497 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper startup failed, generating a new port and retrying...\n",
      "23-06-02 12:08:39.497 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper new port=50568\n",
      "\n",
      "23-06-02 12:08:39.505 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper startup failed, generating a new port and retrying...\n",
      "23-06-02 12:08:39.505 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper new port=50569\n",
      "\n",
      "23-06-02 12:08:39.505 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper startup failed, generating a new port and retrying...\n",
      "23-06-02 12:08:39.505 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper new port=50570\n",
      "\n",
      "23-06-02 12:08:39.514 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper startup failed, generating a new port and retrying...\n",
      "23-06-02 12:08:39.514 [INFO] fastkafka._testing.apache_kafka_broker: zookeeper new port=50571\n",
      "23-06-02 12:08:39.514 [INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): exited.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not start zookeeper with params: [{'zookeeper_port': 2181}, {'zookeeper_port': '50568'}, {'zookeeper_port': '50569'}, {'zookeeper_port': '50570'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# | hide\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mApacheKafkaBroker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello_world\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_nest_asyncio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbootstrap_server\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbootstrap_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbootstrap_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\kumaran rajendhiran\\dev\\fastkafka\\fastkafka\\_testing\\apache_kafka_broker.py:289\u001b[0m, in \u001b[0;36mApacheKafkaBroker.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m#         ApacheKafkaBroker._check_deps()\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\kumaran rajendhiran\\dev\\fastkafka\\fastkafka\\_testing\\apache_kafka_broker.py:621\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m--> 621\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.start(): returning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\dev\\fastkafka\\venv\\Lib\\site-packages\\nest_asyncio.py:90\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\users\\kumaran rajendhiran\\dev\\fastkafka\\fastkafka\\_testing\\apache_kafka_broker.py:561\u001b[0m, in \u001b[0;36m_start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporary_directory \u001b[38;5;241m=\u001b[39m TemporaryDirectory()\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporary_directory_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporary_directory\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m())\n\u001b[1;32m--> 561\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_zookeeper()\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_kafka()\n\u001b[0;32m    564\u001b[0m listener_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkafka_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistener_port\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m9092\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\kumaran rajendhiran\\dev\\fastkafka\\fastkafka\\_testing\\apache_kafka_broker.py:515\u001b[0m, in \u001b[0;36m_start_zookeeper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;129m@patch\u001b[39m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_start_zookeeper\u001b[39m(\u001b[38;5;28mself\u001b[39m: ApacheKafkaBroker) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Starts a local ZooKeeper instance asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_service(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzookeeper\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\kumaran rajendhiran\\dev\\fastkafka\\fastkafka\\_testing\\apache_kafka_broker.py:503\u001b[0m, in \u001b[0;36m_start_service\u001b[1;34m(self, service)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_task\u001b[39m\u001b[38;5;124m\"\u001b[39m, service_task)\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not start \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfigs_tried\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not start zookeeper with params: [{'zookeeper_port': 2181}, {'zookeeper_port': '50568'}, {'zookeeper_port': '50569'}, {'zookeeper_port': '50570'}]"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[\"hello_world\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    server_url = bootstrap_server.split(\":\")[0]\n",
    "    server_port = bootstrap_server.split(\":\")[1]\n",
    "    exit_code, output = await run_script_and_cancel(\n",
    "        script=consumes_example.replace(\n",
    "            \"<url_of_your_kafka_bootstrap_server>\", server_url\n",
    "        ).replace(\"<port_of_your_kafka_bootstrap_server>\", server_port),\n",
    "        script_file=script_file,\n",
    "        cmd=cmd,\n",
    "        cancel_after=5,\n",
    "    )\n",
    "\n",
    "    expected_returncode = 1 if platform.system() == \"Windows\" else 0\n",
    "    assert exit_code == expected_returncode, output.decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "print(output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec6dab",
   "metadata": {},
   "source": [
    "## Send the message to kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e65bc",
   "metadata": {},
   "source": [
    "Lets send a `HelloWorld` message to the *hello_world* topic and check if our consumer kafka application has logged the received message. In your terminal, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef181f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "script_extension = \".bat\" if platform.system() == \"Windows\" else \".sh\"\n",
    "escape_char = \"^\" if platform.system() == \"Windows\" else \".\\\\\"\n",
    "\n",
    "kafka_msg = 'echo {{ {escape_char}\"msg{escape_char}\": {escape_char}\"Hello world{escape_char}\" }}'.format(escape_char=escape_char)\n",
    "\n",
    "producer_cmd = f'{kafka_msg} | kafka-console-producer'+script_extension+' --topic=hello_world --bootstrap-server=<addr_of_your_kafka_bootstrap_server>'\n",
    "md(f\"```shell\\n{producer_cmd}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66904c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "consumes_example = (\n",
    "    import_fastkafka\n",
    "    + import_pydantic\n",
    "    + import_logger\n",
    "    + define_HelloWorld\n",
    "    + create_app\n",
    "    + decorate_consumes\n",
    ")\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[\"hello_world\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    async with asyncer.create_task_group() as task_group:\n",
    "        server_url = bootstrap_server.split(\":\")[0]\n",
    "        server_port = bootstrap_server.split(\":\")[1]\n",
    "        consumer_task = task_group.soonify(run_script_and_cancel)(\n",
    "            script=consumes_example.replace(\n",
    "                \"<url_of_your_kafka_bootstrap_server>\", server_url\n",
    "            ).replace(\"<port_of_your_kafka_bootstrap_server>\", server_port),\n",
    "            script_file=script_file,\n",
    "            cmd=cmd,\n",
    "            cancel_after=20,\n",
    "        )\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "        producer_task = task_group.soonify(asyncio.create_subprocess_shell)(\n",
    "            cmd=producer_cmd.replace(\n",
    "                \"<addr_of_your_kafka_bootstrap_server>\", bootstrap_server\n",
    "            ),\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "assert \"Got msg: msg='Hello world'\" in consumer_task.value[1].decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(consumer_task.value[1].decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292901c",
   "metadata": {},
   "source": [
    "You should see the \"Got msg: msg=\\'Hello world\\'\" being logged by your consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9c46e",
   "metadata": {},
   "source": [
    "## Choosing a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e5f0c",
   "metadata": {},
   "source": [
    "You probably noticed that you didn't define which topic you are receiving the message from, this is because the `@consumes` decorator determines the topic by default from your function name.\n",
    "The decorator will take your function name and strip the default \"on_\" prefix from it and use the rest as the topic name. In this example case, the topic is *hello_world*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e9f8a",
   "metadata": {},
   "source": [
    "You can choose your custom prefix by defining the `prefix` parameter in consumes decorator, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da734ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: False\n",
    "\n",
    "decorate_consumes_prefix = \"\"\"\n",
    "@app.consumes(prefix=\"read_from_\")\n",
    "async def read_from_hello_world(msg: HelloWorld):\n",
    "    logger.info(f\"Got msg: {msg}\")\n",
    "\"\"\"\n",
    "md(f\"```python\\n{decorate_consumes_prefix}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7563b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "consumes_example = (\n",
    "    import_fastkafka\n",
    "    + import_pydantic\n",
    "    + import_logger\n",
    "    + define_HelloWorld\n",
    "    + create_app\n",
    "    + decorate_consumes_prefix\n",
    ")\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[\"hello_world\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    async with asyncer.create_task_group() as task_group:\n",
    "        server_url = bootstrap_server.split(\":\")[0]\n",
    "        server_port = bootstrap_server.split(\":\")[1]\n",
    "        consumer_task = task_group.soonify(run_script_and_cancel)(\n",
    "            script=consumes_example.replace(\n",
    "                \"<url_of_your_kafka_bootstrap_server>\", server_url\n",
    "            ).replace(\"<port_of_your_kafka_bootstrap_server>\", server_port),\n",
    "            script_file=script_file,\n",
    "            cmd=cmd,\n",
    "            cancel_after=20,\n",
    "        )\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "        producer_task = task_group.soonify(asyncio.create_subprocess_shell)(\n",
    "            cmd=producer_cmd.replace(\n",
    "                \"<addr_of_your_kafka_bootstrap_server>\", bootstrap_server\n",
    "            ),\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "assert \"Got msg: msg='Hello world'\" in consumer_task.value[1].decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4225dd",
   "metadata": {},
   "source": [
    "Also, you can define the topic name completely by defining the `topic` in parameter in consumes decorator, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf881b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "decorate_consumes_topic = \"\"\"\n",
    "@app.consumes(topic=\"my_special_topic\")\n",
    "async def on_hello_world(msg: HelloWorld):\n",
    "    logger.info(f\"Got msg: {msg}\")\n",
    "\"\"\"\n",
    "md(f\"```python\\n{decorate_consumes_topic}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612baa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "consumes_example = (\n",
    "    import_fastkafka\n",
    "    + import_pydantic\n",
    "    + import_logger\n",
    "    + define_HelloWorld\n",
    "    + create_app\n",
    "    + decorate_consumes_topic\n",
    ")\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[\"my_special_topic\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    async with asyncer.create_task_group() as task_group:\n",
    "        server_url = bootstrap_server.split(\":\")[0]\n",
    "        server_port = bootstrap_server.split(\":\")[1]\n",
    "        consumer_task = task_group.soonify(run_script_and_cancel)(\n",
    "            script=consumes_example.replace(\n",
    "                \"<url_of_your_kafka_bootstrap_server>\", server_url\n",
    "            ).replace(\"<port_of_your_kafka_bootstrap_server>\", server_port),\n",
    "            script_file=script_file,\n",
    "            cmd=cmd,\n",
    "            cancel_after=20,\n",
    "        )\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "        producer_task = task_group.soonify(asyncio.create_subprocess_shell)(\n",
    "            cmd=producer_cmd.replace(\n",
    "                \"<addr_of_your_kafka_bootstrap_server>\", bootstrap_server\n",
    "            ).replace(\"topic=hello_world\", \"topic=my_special_topic\"),\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "assert \"Got msg: msg='Hello world'\" in consumer_task.value[1].decode(\n",
    "    \"UTF-8\"\n",
    "), consumer_task.value[1].decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50972c",
   "metadata": {},
   "source": [
    "## Message data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e8052",
   "metadata": {},
   "source": [
    "The message received from kafka is translated from binary JSON representation int the class defined by typing of *msg* parameter in the function decorated by the `@consumes` decorator.\n",
    "\n",
    "In this example case, the message will be parsed into a `HelloWorld` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a137",
   "metadata": {},
   "source": [
    "## Message metadata\n",
    "\n",
    "If you need any of Kafka message metadata such as timestamp, partition or headers you can access the metadata by adding a EventMetadata typed argument to your consumes function and the metadata from the incoming message will be automatically injected when calling the consumes function.\n",
    "\n",
    "Let's demonstrate that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde9875",
   "metadata": {},
   "source": [
    "### Create a consumer function with metadata\n",
    "\n",
    "The only difference from the original basic consume function is that we are now passing the `meta: EventMetadata` argument to the function. The `@consumes` decorator will register that and, when a message is consumed, it will also pass the metadata to your function. Now you can use the metadata in your consume function. \n",
    "Lets log it to see what it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a4c79",
   "metadata": {},
   "source": [
    "First, we need to import the EventMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc607d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "import_event_metadata = \"\"\"from fastkafka import EventMetadata\n",
    "\"\"\"\n",
    "md(f\"```python\\n{import_event_metadata}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006804ea",
   "metadata": {},
   "source": [
    "Now we can add the `meta` argument to our consuming function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d029e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "decorate_consumes_meta = \"\"\"@app.consumes()\n",
    "async def on_hello_world(msg: HelloWorld, meta: EventMetadata):\n",
    "    logger.info(f\"Got metadata: {meta}\")\n",
    "\"\"\"\n",
    "md(f\"```python\\n{decorate_consumes_meta}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df2e13",
   "metadata": {},
   "source": [
    "Your final app should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b842f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "consumes_example = (\n",
    "    import_fastkafka\n",
    "    + import_pydantic\n",
    "    + import_event_metadata\n",
    "    + import_logger\n",
    "    + define_HelloWorld\n",
    "    + create_app\n",
    "    + decorate_consumes_meta\n",
    ")\n",
    "\n",
    "md(f\"```python\\n{consumes_example}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143036a",
   "metadata": {},
   "source": [
    "Now lets run the app and send a message to the broker to see the logged message metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d637ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[\"hello_world\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    async with asyncer.create_task_group() as task_group:\n",
    "        server_url = bootstrap_server.split(\":\")[0]\n",
    "        server_port = bootstrap_server.split(\":\")[1]\n",
    "        consumer_task = task_group.soonify(run_script_and_cancel)(\n",
    "            script=consumes_example.replace(\n",
    "                \"<url_of_your_kafka_bootstrap_server>\", server_url\n",
    "            ).replace(\"<port_of_your_kafka_bootstrap_server>\", server_port),\n",
    "            script_file=script_file,\n",
    "            cmd=cmd,\n",
    "            cancel_after=20,\n",
    "        )\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "        producer_task = task_group.soonify(asyncio.create_subprocess_shell)(\n",
    "            cmd=producer_cmd.replace(\n",
    "                \"<addr_of_your_kafka_bootstrap_server>\", bootstrap_server\n",
    "            ),\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "assert \"Got metadata: \" in consumer_task.value[1].decode(\n",
    "    \"UTF-8\"\n",
    "), consumer_task.value[1].decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74082867",
   "metadata": {},
   "source": [
    "You should see a similar log as the one below and the metadata being logged in your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "print(consumer_task.value[1].decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252b37a",
   "metadata": {},
   "source": [
    "As you can see in the log, from the metadata you now have the information about the partition, offset, timestamp, key and headers. :tada:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb2a38",
   "metadata": {},
   "source": [
    "## Dealing with high latency consuming functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441facc",
   "metadata": {},
   "source": [
    "If your functions have high latency due to, for example, lengthy database calls you will notice a big decrease in performance. This is due to the issue of how the consumes decorator executes your consume functions when consumeing events. By default, the consume function will run the consuming funtions for one topic sequentially, this is the most straightforward approach and results with the least amount of overhead.\n",
    "\n",
    "But, to handle those high latency tasks and run them in parallel, FastKafka has a `DynamicTaskExecutor` prepared for your consumers. This executor comes with additional overhead, so use it only when you need to handle high latency functions.\n",
    "\n",
    "Lets demonstrate how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decorate_consumes_executor = \"\"\"@app.consumes(executor=\"DynamicTaskExecutor\")\n",
    "async def on_hello_world(msg: HelloWorld):\n",
    "    logger.info(f\"Got msg: {msg}\")\n",
    "\"\"\"\n",
    "md(f\"```python\\n{decorate_consumes}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293d59e",
   "metadata": {},
   "source": [
    "Lets send a `HelloWorld` message to the *hello_world* topic and check if our consumer kafka application has logged the received message. In your terminal, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc1e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "script_extension = \".bat\" if platform.system() == \"Windows\" else \".sh\"\n",
    "escape_char = \"^\" if platform.system() == \"Windows\" else \".\\\\\"\n",
    "\n",
    "kafka_msg = 'echo {{ {escape_char}\"msg{escape_char}\": {escape_char}\"Hello world{escape_char}\" }}'.format(escape_char=escape_char)\n",
    "\n",
    "\n",
    "producer_cmd = f'{kafka_msg} | kafka-console-producer'+script_extension+' --topic=hello_world --bootstrap-server=<addr_of_your_kafka_bootstrap_server>'\n",
    "md(f\"```shell\\n{producer_cmd}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c47569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "\n",
    "consumes_example = (\n",
    "    import_fastkafka\n",
    "    + import_pydantic\n",
    "    + import_logger\n",
    "    + define_HelloWorld\n",
    "    + create_app\n",
    "    + decorate_consumes_executor\n",
    ")\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[\"hello_world\"], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    async with asyncer.create_task_group() as task_group:\n",
    "        server_url = bootstrap_server.split(\":\")[0]\n",
    "        server_port = bootstrap_server.split(\":\")[1]\n",
    "        consumer_task = task_group.soonify(run_script_and_cancel)(\n",
    "            script=consumes_example.replace(\n",
    "                \"<url_of_your_kafka_bootstrap_server>\", server_url\n",
    "            ).replace(\"<port_of_your_kafka_bootstrap_server>\", server_port),\n",
    "            script_file=script_file,\n",
    "            cmd=cmd,\n",
    "            cancel_after=20,\n",
    "        )\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "        producer_task = task_group.soonify(asyncio.create_subprocess_shell)(\n",
    "            cmd=producer_cmd.replace(\n",
    "                \"<addr_of_your_kafka_bootstrap_server>\", bootstrap_server\n",
    "            ),\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "assert \"Got msg: msg='Hello world'\" in consumer_task.value[1].decode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572ebf3",
   "metadata": {},
   "source": [
    "You should see the \"Got msg: msg=\\'Hello world\\'\" being logged by your consumer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
