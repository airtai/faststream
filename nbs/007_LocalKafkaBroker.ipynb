{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c520c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _testing.local_broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d47b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import shutil\n",
    "import socket\n",
    "import subprocess # nosec - Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "\n",
    "import asyncer\n",
    "import nest_asyncio\n",
    "import posix_ipc\n",
    "import requests\n",
    "from fastcore.basics import patch\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "from fastkafka._components._subprocess import terminate_asyncio_process\n",
    "from fastkafka._components.helpers import filter_using_signature\n",
    "from fastkafka._components.logger import get_logger, supress_timestamps\n",
    "from fastkafka.helpers import in_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ed82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "from fastkafka.helpers import consumes_messages, produce_messages\n",
    "from fastkafka.testing import run_script_and_cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81062e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "else:\n",
    "    from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f95ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ef020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fded319",
   "metadata": {},
   "source": [
    "### Local Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552eb74",
   "metadata": {},
   "source": [
    "#### Kafka and zookeeper config helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_zookeeper_config_string(\n",
    "    data_dir: Union[str, Path],  # the directory where the snapshot is stored.\n",
    "    zookeeper_port: int = 2181,  # the port at which the clients will connect\n",
    ") -> str:\n",
    "    \"\"\"Generates a zookeeeper configuration string that can be exported to file\n",
    "    and used to start a zookeeper instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "        zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "    Returns:\n",
    "        Zookeeper configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    zookeeper_config = f\"\"\"dataDir={data_dir}/zookeeper\n",
    "clientPort={zookeeper_port}\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "    return zookeeper_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    get_zookeeper_config_string(data_dir=\"..\")\n",
    "    == \"\"\"dataDir=../zookeeper\n",
    "clientPort=2181\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "assert (\n",
    "    get_zookeeper_config_string(data_dir=\"..\", zookeeper_port=100)\n",
    "    == \"\"\"dataDir=../zookeeper\n",
    "clientPort=100\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_kafka_config_string(\n",
    "    data_dir: Union[str, Path], zookeeper_port: int = 2181, listener_port: int = 9092\n",
    ") -> str:\n",
    "    \"\"\"Generates a kafka broker configuration string that can be exported to file\n",
    "    and used to start a kafka broker instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the kafka broker instance will save data\n",
    "        zookeeper_port: Port on which the zookeeper instance is running\n",
    "        listener_port: Port on which the clients (producers and consumers) can connect\n",
    "    Returns:\n",
    "        Kafka broker configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    kafka_config = f\"\"\"broker.id=0\n",
    "\n",
    "############################# Socket Server Settings #############################\n",
    "\n",
    "# The address the socket server listens on. If not configured, the host name will be equal to the value of\n",
    "# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.\n",
    "#   FORMAT:\n",
    "#     listeners = listener_name://host_name:port\n",
    "#   EXAMPLE:\n",
    "#     listeners = PLAINTEXT://your.host.name:9092\n",
    "listeners=PLAINTEXT://:{listener_port}\n",
    "\n",
    "# Listener name, hostname and port the broker will advertise to clients.\n",
    "# If not set, it uses the value for \"listeners\".\n",
    "#advertised.listeners=PLAINTEXT://your.host.name:9092\n",
    "\n",
    "# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details\n",
    "#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
    "\n",
    "# The number of threads that the server uses for receiving requests from the network and sending responses to the network\n",
    "num.network.threads=3\n",
    "\n",
    "# The number of threads that the server uses for processing requests, which may include disk I/O\n",
    "num.io.threads=8\n",
    "\n",
    "# The send buffer (SO_SNDBUF) used by the socket server\n",
    "socket.send.buffer.bytes=102400\n",
    "\n",
    "# The receive buffer (SO_RCVBUF) used by the socket server\n",
    "socket.receive.buffer.bytes=102400\n",
    "\n",
    "# The maximum size of a request that the socket server will accept (protection against OOM)\n",
    "socket.request.max.bytes=104857600\n",
    "\n",
    "\n",
    "############################# Log Basics #############################\n",
    "\n",
    "# A comma separated list of directories under which to store log files\n",
    "log.dirs={data_dir}/kafka_logs\n",
    "\n",
    "# The default number of log partitions per topic. More partitions allow greater\n",
    "# parallelism for consumption, but this will also result in more files across\n",
    "# the brokers.\n",
    "num.partitions=1\n",
    "\n",
    "# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n",
    "# This value is recommended to be increased for installations with data dirs located in RAID array.\n",
    "num.recovery.threads.per.data.dir=1\n",
    "\n",
    "offsets.topic.replication.factor=1\n",
    "transaction.state.log.replication.factor=1\n",
    "transaction.state.log.min.isr=1\n",
    "\n",
    "# The number of messages to accept before forcing a flush of data to disk\n",
    "log.flush.interval.messages=10000\n",
    "\n",
    "# The maximum amount of time a message can sit in a log before we force a flush\n",
    "log.flush.interval.ms=1000\n",
    "\n",
    "# The minimum age of a log file to be eligible for deletion due to age\n",
    "log.retention.hours=168\n",
    "\n",
    "# A size-based retention policy for logs. Segments are pruned from the log unless the remaining\n",
    "# segments drop below log.retention.bytes. Functions independently of log.retention.hours.\n",
    "log.retention.bytes=1073741824\n",
    "\n",
    "# The maximum size of a log segment file. When this size is reached a new log segment will be created.\n",
    "log.segment.bytes=1073741824\n",
    "\n",
    "# The interval at which log segments are checked to see if they can be deleted according to the retention policies\n",
    "log.retention.check.interval.ms=300000\n",
    "\n",
    "# Zookeeper connection string (see zookeeper docs for details).\n",
    "zookeeper.connect=localhost:{zookeeper_port}\n",
    "\n",
    "# Timeout in ms for connecting to zookeeper\n",
    "zookeeper.connection.timeout.ms=18000\n",
    "\n",
    "# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.\n",
    "group.initial.rebalance.delay.ms=0\n",
    "\"\"\"\n",
    "\n",
    "    return kafka_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = get_kafka_config_string(data_dir=\"..\", listener_port=9999)\n",
    "assert \"log.dirs=../kafka_logs\" in actual\n",
    "assert \"listeners=PLAINTEXT://:9999\" in actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf939a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LocalKafkaBroker:\n",
    "    \"\"\"LocalKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.\n",
    "\n",
    "    Attributes:\n",
    "        lock (ilock.Lock): Lock used for synchronizing the install process between multiple kafka brokers.\n",
    "    \"\"\"\n",
    "\n",
    "    lock = posix_ipc.Semaphore(\n",
    "        \"install_lock:LocalKafkaBroker\", posix_ipc.O_CREAT, initial_value=1\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def clear_install_semaphore() -> None:\n",
    "        \"\"\"Clears semaphore used for synchronizing installation of requirements\n",
    "\n",
    "        Use this function only if the semaphore is being locked due to crashing process (rarely)\n",
    "        \"\"\"\n",
    "        LocalKafkaBroker.lock.unlink()\n",
    "        LocalKafkaBroker.lock = posix_ipc.Semaphore(\n",
    "            \"install_lock:LocalKafkaBroker\", posix_ipc.O_CREAT, initial_value=1\n",
    "        )\n",
    "\n",
    "    @delegates(get_kafka_config_string)  # type: ignore\n",
    "    @delegates(get_zookeeper_config_string, keep=True)  # type: ignore\n",
    "    def __init__(\n",
    "        self,\n",
    "        topics: Iterable[str] = [],\n",
    "        *,\n",
    "        retries: int = 2,\n",
    "        apply_nest_asyncio: bool = False,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ):\n",
    "        \"\"\"Initialises the LocalKafkaBroker object\n",
    "\n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "            listener_port: Port on which the clients (producers and consumers) can connect\n",
    "            topics: List of topics to create after sucessfull Kafka broker startup\n",
    "            retries: Number of retries to create kafka and zookeeper services using random\n",
    "            apply_nest_asyncio: set to True if running in notebook\n",
    "            port allocation if the requested port was taken\n",
    "        \"\"\"\n",
    "        self.zookeeper_kwargs = filter_using_signature(\n",
    "            get_zookeeper_config_string, **kwargs\n",
    "        )\n",
    "        self.retries = retries\n",
    "        self.apply_nest_asyncio = apply_nest_asyncio\n",
    "        self.kafka_kwargs = filter_using_signature(get_kafka_config_string, **kwargs)\n",
    "        self.temporary_directory: Optional[TemporaryDirectory] = None\n",
    "        self.temporary_directory_path: Optional[Path] = None\n",
    "        self.kafka_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.zookeeper_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self._is_started = False\n",
    "        self.topics: Iterable[str] = topics\n",
    "            \n",
    "    @property\n",
    "    def is_started(self) -> bool:\n",
    "        return self._is_started\n",
    "\n",
    "    @classmethod\n",
    "    def _install(cls) -> None:\n",
    "        \"\"\"Prepares the environment for running Kafka brokers.\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance asynchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_service_config_string(self, service: str, *, data_dir: Path) -> str:\n",
    "        \"\"\"Generates a configuration for a service\n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            service: \"kafka\" or \"zookeeper\", defines which service to get config string for\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_service(self, service: str = \"kafka\") -> None:\n",
    "        \"\"\"Starts the service according to defined service var\n",
    "        Args:\n",
    "            service: \"kafka\" or \"zookeeper\", defines which service to start\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_zookeeper(self) -> None:\n",
    "        \"\"\"Start a local zookeeper instance\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_kafka(self) -> None:\n",
    "        \"\"\"Start a local kafka broker\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _create_topics(self) -> None:\n",
    "        \"\"\"Create missing topics in local Kafka broker\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __enter__(self) -> str:\n",
    "        #         LocalKafkaBroker._install()\n",
    "        return self.start()\n",
    "\n",
    "    def __exit__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        self.stop()\n",
    "\n",
    "    async def __aenter__(self) -> str:\n",
    "        #         LocalKafkaBroker._install()\n",
    "        return await self._start()\n",
    "\n",
    "    async def __aexit__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        await self._stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(LocalKafkaBroker, get_kafka_config_string), get_zookeeper_config_string).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac507521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "! nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from fastcore.foundation import patch\n",
    "import posix_ipc\n",
    "\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka.testing import LocalKafkaBroker\n",
    "\n",
    "pid = os.getpid()\n",
    "\n",
    "logger = get_logger(f\"[PID={pid}]\")\n",
    "\n",
    "@patch(cls_method=True) # type: ignore\n",
    "def check_cls_lock(cls: LocalKafkaBroker) -> None:\n",
    "    with cls.lock:\n",
    "       logger.info(f\"Entering: {time.time()}\")\n",
    "       logger.info(f\" - {datetime.now()}\")\n",
    "       time.sleep(1)\n",
    "       logger.info(f\"Exiting: {time.time()}\")\n",
    "       logger.info(f\" - {datetime.now()}\")\n",
    "\n",
    "broker = LocalKafkaBroker()\n",
    "broker.check_cls_lock()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_times(stdout: str) -> Tuple[float, float]:\n",
    "    stdout_lines = stdout.split(\"\\n\")\n",
    "    enter_time = float(\n",
    "        [line.split(\" \")[-1] for line in stdout_lines if \"Entering\" in line][0]\n",
    "    )\n",
    "    exit_time = float(\n",
    "        [line.split(\" \")[-1] for line in stdout_lines if \"Exiting\" in line][0]\n",
    "    )\n",
    "    return (enter_time, exit_time)\n",
    "\n",
    "\n",
    "def check_overlap(intervals: List[Tuple[float]]) -> bool:\n",
    "    for i, (test_start, test_stop) in enumerate(intervals):\n",
    "        for start, stop in intervals[i + 1 :]:\n",
    "            if test_start < start < test_stop or start < test_start < stop:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "async with asyncer.create_task_group() as tg:\n",
    "    tx = [tg.soonify(run_script_and_cancel)(script, cancel_after=30) for _ in range(3)]\n",
    "retvals, stdouts = zip(*[t.value for t in tx])\n",
    "for retval, stdout in zip(retvals, stdouts):\n",
    "    print(\"*\" * 100)\n",
    "    print(f\"*   {retval=}\")\n",
    "    print()\n",
    "    print(stdout.decode(\"utf-8\"))\n",
    "    print()\n",
    "\n",
    "times = [get_times(stdout.decode(\"utf-8\")) for stdout in stdouts]\n",
    "assert not check_overlap(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7480229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_java() -> None:\n",
    "    \"\"\"Checks if jdk-11 is installed on the machine and installs it if not\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    potential_jdk_path = list(Path(os.environ[\"HOME\"] + \"/.jdk\").glob(\"jdk-11*\"))\n",
    "    if potential_jdk_path != []:\n",
    "        logger.info(\"Java is already installed.\")\n",
    "        if not shutil.which(\"java\"):\n",
    "            logger.info(\"But not exported to PATH, exporting...\")\n",
    "            os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{potential_jdk_path[0]}/bin\"\n",
    "    else:\n",
    "        logger.info(\"Installing Java...\")\n",
    "        logger.info(\" - installing install-jdk...\")\n",
    "        subprocess.run([\"pip\", \"install\", \"install-jdk\"], check=True)  # nosec\n",
    "        import jdk\n",
    "\n",
    "        logger.info(\" - installing jdk...\")\n",
    "        jdk_bin_path = jdk.install(\"11\")\n",
    "        print(jdk_bin_path)\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{jdk_bin_path}/bin\"\n",
    "        logger.info(\"Java installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4499887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: But not exported to PATH, exporting...\n",
      "[INFO] __main__: Java is already installed.\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "install_java()\n",
    "assert shutil.which(\"java\")\n",
    "install_java()\n",
    "assert shutil.which(\"java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a57f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def install_kafka() -> None:\n",
    "    \"\"\"Checks if kafka is installed on the machine and installs it if not\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    kafka_version = \"3.3.2\"\n",
    "    kafka_fname = f\"kafka_2.13-{kafka_version}\"\n",
    "    kafka_url = f\"https://dlcdn.apache.org/kafka/{kafka_version}/{kafka_fname}.tgz\"\n",
    "    local_path = Path(os.environ[\"HOME\"]) / \".local\"\n",
    "    local_path.mkdir(exist_ok=True, parents=True)\n",
    "    tgz_path = local_path / f\"{kafka_fname}.tgz\"\n",
    "    kafka_path = local_path / f\"{kafka_fname}\"\n",
    "\n",
    "    if (kafka_path / \"bin\").exists():\n",
    "        logger.info(\"Kafka is already installed.\")\n",
    "        if not shutil.which(\"kafka-server-start.sh\"):\n",
    "            logger.info(\"But not exported to PATH, exporting...\")\n",
    "            os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "    else:\n",
    "        logger.info(\"Installing Kafka...\")\n",
    "\n",
    "        response = requests.get(\n",
    "            kafka_url,\n",
    "            stream=True,\n",
    "        )\n",
    "        try:\n",
    "            total = response.raw.length_remaining // 128\n",
    "        except Exception:\n",
    "            total = None\n",
    "\n",
    "        with open(tgz_path, \"wb\") as f:\n",
    "            for data in tqdm(response.iter_content(chunk_size=128), total=total):\n",
    "                f.write(data)\n",
    "\n",
    "        with tarfile.open(tgz_path) as tar:\n",
    "            for tarinfo in tar:\n",
    "                tar.extract(tarinfo, local_path)\n",
    "\n",
    "        os.environ[\"PATH\"] = os.environ[\"PATH\"] + f\":{kafka_path}/bin\"\n",
    "        logger.info(f\"Kafka installed in {kafka_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: But not exported to PATH, exporting...\n",
      "[INFO] __main__: Kafka is already installed.\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")\n",
    "install_kafka()\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch(cls_method=True)  # type: ignore\n",
    "def _install(cls: LocalKafkaBroker) -> None:\n",
    "    with cls.lock:\n",
    "        install_java()\n",
    "        install_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4600ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker()\n",
    "broker._install()\n",
    "assert shutil.which(\"java\")\n",
    "assert shutil.which(\"kafka-server-start.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a076ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_free_port() -> str:\n",
    "    s = socket.socket()\n",
    "    s.bind((\"127.0.0.1\", 0))\n",
    "    port = str(s.getsockname()[1])\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "\n",
    "async def write_config_and_run(\n",
    "    config: str, config_path: Union[str, Path], run_cmd: str\n",
    ") -> asyncio.subprocess.Process:\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config)\n",
    "\n",
    "    return await asyncio.create_subprocess_exec(\n",
    "        run_cmd,\n",
    "        config_path,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stdin=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def get_service_config_string(\n",
    "    self: LocalKafkaBroker, service: str, *, data_dir: Path\n",
    ") -> str:\n",
    "    service_kwargs = getattr(self, f\"{service}_kwargs\")\n",
    "    if service == \"kafka\":\n",
    "        return get_kafka_config_string(data_dir=data_dir, **service_kwargs)\n",
    "    else:\n",
    "        return get_zookeeper_config_string(data_dir=data_dir, **service_kwargs)\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _start_service(self: LocalKafkaBroker, service: str = \"kafka\") -> None:\n",
    "    logger.info(f\"Starting {service}...\")\n",
    "\n",
    "    if self.temporary_directory_path is None:\n",
    "        raise ValueError(\n",
    "            \"LocalKafkaBroker._start_service(): self.temporary_directory_path is None, did you initialise it?\"\n",
    "        )\n",
    "\n",
    "    for i in range(self.retries + 1):\n",
    "        service_config_path = self.temporary_directory_path / f\"{service}.properties\"\n",
    "        service_task = await write_config_and_run(\n",
    "            self.get_service_config_string(\n",
    "                service, data_dir=self.temporary_directory_path\n",
    "            ),\n",
    "            service_config_path,\n",
    "            f\"{service}-server-start.sh\",\n",
    "        )\n",
    "        setattr(self, f\"{service}_task\", service_task)\n",
    "\n",
    "        logger.info(f\"{service} started, sleeping for 5 seconds...\")\n",
    "        await asyncio.sleep(5)\n",
    "\n",
    "        if service_task.returncode is None:\n",
    "            break\n",
    "        elif i < self.retries:\n",
    "            logger.info(\n",
    "                f\"{service} startup falied, generating a new port and retrying...\"\n",
    "            )\n",
    "            port = get_free_port()\n",
    "\n",
    "            portname = service if service != \"kafka\" else \"listener\"\n",
    "            for d in [self.zookeeper_kwargs, self.kafka_kwargs]:\n",
    "                if f\"{portname}_port\" in d:\n",
    "                    d[f\"{portname}_port\"] = port\n",
    "            logger.info(f\"port={port}\")\n",
    "\n",
    "    if service_task.returncode is not None:\n",
    "        raise ValueError(\n",
    "            f\"Could not start {service} with params: {getattr(self, f'{service}_kwargs')}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _start_kafka(self: LocalKafkaBroker) -> None:\n",
    "    return await self._start_service(\"kafka\")\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _start_zookeeper(self: LocalKafkaBroker) -> None:\n",
    "    return await self._start_service(\"zookeeper\")\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _create_topics(self: LocalKafkaBroker) -> None:\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    bootstrap_server = f\"127.0.0.1:{listener_port}\"\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        processes = [\n",
    "            tg.soonify(asyncio.create_subprocess_exec)(\n",
    "                \"kafka-topics.sh\",\n",
    "                \"--create\",\n",
    "                f\"--topic={topic}\",\n",
    "                f\"--bootstrap-server={bootstrap_server}\",\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stdin=asyncio.subprocess.PIPE,\n",
    "            )\n",
    "            for topic in self.topics\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        return_values = [\n",
    "            await asyncio.wait_for(process.value.wait(), 30) for process in processes\n",
    "        ]\n",
    "        if any(return_value != 0 for return_value in return_values):\n",
    "            raise ValueError(\"Could not create missing topics!\")\n",
    "    except asyncio.TimeoutError as _:\n",
    "        raise ValueError(\"Timed out while creating missing topics!\")\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _start(self: LocalKafkaBroker) -> str:\n",
    "    self._install()\n",
    "\n",
    "    self.temporary_directory = TemporaryDirectory()\n",
    "    self.temporary_directory_path = Path(self.temporary_directory.__enter__())\n",
    "\n",
    "    await self._start_zookeeper()\n",
    "    await self._start_kafka()\n",
    "\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    bootstrap_server = f\"127.0.0.1:{listener_port}\"\n",
    "    logger.info(f\"Local Kafka broker up and running on {bootstrap_server}\")\n",
    "\n",
    "    await self._create_topics()\n",
    "\n",
    "    self._is_started = True\n",
    "\n",
    "    return bootstrap_server\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _stop(self: LocalKafkaBroker) -> None:\n",
    "    await terminate_asyncio_process(self.kafka_task)  # type: ignore\n",
    "    await terminate_asyncio_process(self.zookeeper_task)  # type: ignore\n",
    "    self.temporary_directory.__exit__(None, None, None)  # type: ignore\n",
    "    self._is_started = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "[INFO] __main__: zookeeper started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: kafka started, sleeping for 5 seconds...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9689\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 164911...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 164911 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 164546...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 164546 terminated.\n",
      "**************************************************ZOOKEEPER LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-03-03 10:26:29,857] INFO Reading configuration from: /tmp/tmp80atlpl2/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,868] INFO clientPortAddress is 0.0.0.0:9799 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,870] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,870] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,870] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,872] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-03-03 10:26:29,872] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-03-03 10:26:29,872] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)\n",
      "[2023-03-03 10:26:29,872] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)\n",
      "[2023-03-03 10:26:29,873] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)\n",
      "[2023-03-03 10:26:29,874] INFO Reading configuration from: /tmp/tmp80atlpl2/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,874] INFO clientPortAddress is 0.0.0.0:9799 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,875] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,875] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,876] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n",
      "[2023-03-03 10:26:29,876] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)\n",
      "[2023-03-03 10:26:29,890] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@1649b0e6 (org.apache.zookeeper.server.ServerMetrics)\n",
      "[2023-03-03 10:26:29,893] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-03-03 10:26:29,908] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO    / /    / _ \\   / _ \\  | |/ /  / _ \\  / _ \\ | '_ \\   / _ \\ | '__| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO  /_____|  \\___/   \\___/  |_|\\_\\  \\___|  \\___| | .__/   \\___| |_| (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,908] INFO  (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,909] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.version=11.0.18 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,910] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:os.version=5.15.0-58-generic (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:user.name=tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:user.home=/home/tvrtko (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:os.memory.free=490MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,914] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,918] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,918] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,918] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,918] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,918] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,918] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,920] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)\n",
      "[2023-03-03 10:26:29,921] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,921] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,922] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-03-03 10:26:29,922] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)\n",
      "[2023-03-03 10:26:29,923] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-03-03 10:26:29,923] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-03-03 10:26:29,923] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-03-03 10:26:29,923] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-03-03 10:26:29,923] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-03-03 10:26:29,923] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)\n",
      "[2023-03-03 10:26:29,927] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,927] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,928] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /tmp/tmp80atlpl2/zookeeper/version-2 snapdir /tmp/tmp80atlpl2/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:29,939] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-03-03 10:26:29,940] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)\n",
      "[2023-03-03 10:26:29,942] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 8 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-03-03 10:26:29,946] INFO binding to port 0.0.0.0/0.0.0.0:9799 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n",
      "[2023-03-03 10:26:29,977] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[2023-03-03 10:26:29,977] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)\n",
      "[2023-03-03 10:26:29,978] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-03-03 10:26:29,979] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-03-03 10:26:29,990] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)\n",
      "[2023-03-03 10:26:29,991] INFO Snapshotting: 0x0 to /tmp/tmp80atlpl2/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-03-03 10:26:29,994] INFO Snapshot loaded in 15 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)\n",
      "[2023-03-03 10:26:30,000] INFO Snapshotting: 0x0 to /tmp/tmp80atlpl2/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)\n",
      "[2023-03-03 10:26:30,001] INFO Snapshot taken in 7 ms (org.apache.zookeeper.server.ZooKeeperServer)\n",
      "[2023-03-03 10:26:30,019] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)\n",
      "[2023-03-03 10:26:30,019] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)\n",
      "[2023-03-03 10:26:30,057] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)\n",
      "[2023-03-03 10:26:30,058] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[2023-03-03 10:26:35,027] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "\n",
      "**************************************************KAFKA LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-03-03 10:26:34,593] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n",
      "[2023-03-03 10:26:34,875] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n",
      "[2023-03-03 10:26:34,966] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-03-03 10:26:34,967] INFO starting (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:34,968] INFO Connecting to zookeeper on localhost:9799 (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:34,981] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:9799. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-03 10:26:34,986] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:host.name=tvrtko-fastkafka-devel (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.version=11.0.18 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.vendor=Eclipse Adoptium (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.home=/home/tvrtko/.jdk/jdk-11.0.18+10 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.class.path=/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/activation-1.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/aopalliance-repackaged-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/argparse4j-0.7.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/audience-annotations-0.5.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-cli-1.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.12.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/commons-lang3-3.8.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-basic-auth-extension-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-json-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-mirror-client-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-runtime-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/connect-transforms-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-api-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-locator-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/hk2-utils-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-core-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-databind-2.13.4.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-dataformat-csv-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-datatype-jdk8-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-base-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-jaxrs-json-provider-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-jaxb-annotations-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jackson-module-scala_2.13-2.13.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.activation-api-1.2.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.annotation-api-1.3.5.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.inject-2.6.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.validation-api-2.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javassist-3.27.0-GA.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.servlet-api-3.1.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/javax.ws.rs-api-2.1.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jaxb-api-2.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-client-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-common-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-container-servlet-core-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-hk2-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jersey-server-2.34.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-client-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-continuation-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-http-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-io-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-security-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-server-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlet-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-servlets-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jetty-util-ajax-9.4.48.v20220622.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jline-3.21.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jopt-simple-5.0.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/jose4j-0.7.9.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-clients-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-log4j-appender-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-metadata-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-raft-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-server-common-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-shell-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-storage-api-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-examples-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-scala_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-streams-test-utils-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka-tools-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/kafka_2.13-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/lz4-java-1.8.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/maven-artifact-3.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/metrics-core-4.1.12.1.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-buffer-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-codec-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-handler-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-resolver-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-classes-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-epoll-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/netty-transport-native-unix-common-4.1.78.Final.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/osgi-resource-locator-1.0.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/paranamer-2.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/plexus-utils-3.3.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reflections-0.9.12.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/reload4j-1.2.19.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/rocksdbjni-7.1.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-collection-compat_2.13-2.6.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-library-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-logging_2.13-3.9.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/scala-reflect-2.13.8.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-api-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/slf4j-reload4j-1.7.36.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/snappy-java-1.1.8.4.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/swagger-annotations-2.2.0.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/trogdor-3.3.2.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zookeeper-jute-3.6.3.jar:/home/tvrtko/.local/kafka_2.13-3.3.2/bin/../libs/zstd-jni-1.5.2-1.jar (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:os.version=5.15.0-58-generic (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:user.name=tvrtko (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:user.home=/home/tvrtko (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:user.dir=/work/fastkafka/nbs (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:os.memory.free=1009MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,987] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,990] INFO Initiating client connection, connectString=localhost:9799 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7b84fcf8 (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:34,995] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n",
      "[2023-03-03 10:26:35,000] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-03 10:26:35,009] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-03 10:26:35,009] INFO Opening socket connection to server localhost/127.0.0.1:9799. (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-03 10:26:35,013] INFO Socket connection established, initiating session, client: /127.0.0.1:47310, server: localhost/127.0.0.1:9799 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-03 10:26:35,044] INFO Session establishment complete on server localhost/127.0.0.1:9799, session id = 0x1005d7c84d70000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-03 10:26:35,046] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-03 10:26:35,444] INFO Cluster ID = qy-SbqkSQaW77erPOUVOlg (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:35,446] WARN No meta.properties file under dir /tmp/tmp80atlpl2/kafka_logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)\n",
      "[2023-03-03 10:26:35,494] INFO KafkaConfig values: \n",
      "\tadvertised.listeners = null\n",
      "\talter.config.policy.class.name = null\n",
      "\talter.log.dirs.replication.quota.window.num = 11\n",
      "\talter.log.dirs.replication.quota.window.size.seconds = 1\n",
      "\tauthorizer.class.name = \n",
      "\tauto.create.topics.enable = true\n",
      "\tauto.leader.rebalance.enable = true\n",
      "\tbackground.threads = 10\n",
      "\tbroker.heartbeat.interval.ms = 2000\n",
      "\tbroker.id = 0\n",
      "\tbroker.id.generation.enable = true\n",
      "\tbroker.rack = null\n",
      "\tbroker.session.timeout.ms = 9000\n",
      "\tclient.quota.callback.class = null\n",
      "\tcompression.type = producer\n",
      "\tconnection.failed.authentication.delay.ms = 100\n",
      "\tconnections.max.idle.ms = 600000\n",
      "\tconnections.max.reauth.ms = 0\n",
      "\tcontrol.plane.listener.name = null\n",
      "\tcontrolled.shutdown.enable = true\n",
      "\tcontrolled.shutdown.max.retries = 3\n",
      "\tcontrolled.shutdown.retry.backoff.ms = 5000\n",
      "\tcontroller.listener.names = null\n",
      "\tcontroller.quorum.append.linger.ms = 25\n",
      "\tcontroller.quorum.election.backoff.max.ms = 1000\n",
      "\tcontroller.quorum.election.timeout.ms = 1000\n",
      "\tcontroller.quorum.fetch.timeout.ms = 2000\n",
      "\tcontroller.quorum.request.timeout.ms = 2000\n",
      "\tcontroller.quorum.retry.backoff.ms = 20\n",
      "\tcontroller.quorum.voters = []\n",
      "\tcontroller.quota.window.num = 11\n",
      "\tcontroller.quota.window.size.seconds = 1\n",
      "\tcontroller.socket.timeout.ms = 30000\n",
      "\tcreate.topic.policy.class.name = null\n",
      "\tdefault.replication.factor = 1\n",
      "\tdelegation.token.expiry.check.interval.ms = 3600000\n",
      "\tdelegation.token.expiry.time.ms = 86400000\n",
      "\tdelegation.token.master.key = null\n",
      "\tdelegation.token.max.lifetime.ms = 604800000\n",
      "\tdelegation.token.secret.key = null\n",
      "\tdelete.records.purgatory.purge.interval.requests = 1\n",
      "\tdelete.topic.enable = true\n",
      "\tearly.start.listeners = null\n",
      "\tfetch.max.bytes = 57671680\n",
      "\tfetch.purgatory.purge.interval.requests = 1000\n",
      "\tgroup.initial.rebalance.delay.ms = 0\n",
      "\tgroup.max.session.timeout.ms = 1800000\n",
      "\tgroup.max.size = 2147483647\n",
      "\tgroup.min.session.timeout.ms = 6000\n",
      "\tinitial.broker.registration.timeout.ms = 60000\n",
      "\tinter.broker.listener.name = null\n",
      "\tinter.broker.protocol.version = 3.3-IV3\n",
      "\tkafka.metrics.polling.interval.secs = 10\n",
      "\tkafka.metrics.reporters = []\n",
      "\tleader.imbalance.check.interval.seconds = 300\n",
      "\tleader.imbalance.per.broker.percentage = 10\n",
      "\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
      "\tlisteners = PLAINTEXT://:9689\n",
      "\tlog.cleaner.backoff.ms = 15000\n",
      "\tlog.cleaner.dedupe.buffer.size = 134217728\n",
      "\tlog.cleaner.delete.retention.ms = 86400000\n",
      "\tlog.cleaner.enable = true\n",
      "\tlog.cleaner.io.buffer.load.factor = 0.9\n",
      "\tlog.cleaner.io.buffer.size = 524288\n",
      "\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n",
      "\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n",
      "\tlog.cleaner.min.cleanable.ratio = 0.5\n",
      "\tlog.cleaner.min.compaction.lag.ms = 0\n",
      "\tlog.cleaner.threads = 1\n",
      "\tlog.cleanup.policy = [delete]\n",
      "\tlog.dir = /tmp/kafka-logs\n",
      "\tlog.dirs = /tmp/tmp80atlpl2/kafka_logs\n",
      "\tlog.flush.interval.messages = 10000\n",
      "\tlog.flush.interval.ms = 1000\n",
      "\tlog.flush.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.flush.scheduler.interval.ms = 9223372036854775807\n",
      "\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n",
      "\tlog.index.interval.bytes = 4096\n",
      "\tlog.index.size.max.bytes = 10485760\n",
      "\tlog.message.downconversion.enable = true\n",
      "\tlog.message.format.version = 3.0-IV1\n",
      "\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n",
      "\tlog.message.timestamp.type = CreateTime\n",
      "\tlog.preallocate = false\n",
      "\tlog.retention.bytes = 1073741824\n",
      "\tlog.retention.check.interval.ms = 300000\n",
      "\tlog.retention.hours = 168\n",
      "\tlog.retention.minutes = null\n",
      "\tlog.retention.ms = null\n",
      "\tlog.roll.hours = 168\n",
      "\tlog.roll.jitter.hours = 0\n",
      "\tlog.roll.jitter.ms = null\n",
      "\tlog.roll.ms = null\n",
      "\tlog.segment.bytes = 1073741824\n",
      "\tlog.segment.delete.delay.ms = 60000\n",
      "\tmax.connection.creation.rate = 2147483647\n",
      "\tmax.connections = 2147483647\n",
      "\tmax.connections.per.ip = 2147483647\n",
      "\tmax.connections.per.ip.overrides = \n",
      "\tmax.incremental.fetch.session.cache.slots = 1000\n",
      "\tmessage.max.bytes = 1048588\n",
      "\tmetadata.log.dir = null\n",
      "\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n",
      "\tmetadata.log.segment.bytes = 1073741824\n",
      "\tmetadata.log.segment.min.bytes = 8388608\n",
      "\tmetadata.log.segment.ms = 604800000\n",
      "\tmetadata.max.idle.interval.ms = 500\n",
      "\tmetadata.max.retention.bytes = -1\n",
      "\tmetadata.max.retention.ms = 604800000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tmin.insync.replicas = 1\n",
      "\tnode.id = 0\n",
      "\tnum.io.threads = 8\n",
      "\tnum.network.threads = 3\n",
      "\tnum.partitions = 1\n",
      "\tnum.recovery.threads.per.data.dir = 1\n",
      "\tnum.replica.alter.log.dirs.threads = null\n",
      "\tnum.replica.fetchers = 1\n",
      "\toffset.metadata.max.bytes = 4096\n",
      "\toffsets.commit.required.acks = -1\n",
      "\toffsets.commit.timeout.ms = 5000\n",
      "\toffsets.load.buffer.size = 5242880\n",
      "\toffsets.retention.check.interval.ms = 600000\n",
      "\toffsets.retention.minutes = 10080\n",
      "\toffsets.topic.compression.codec = 0\n",
      "\toffsets.topic.num.partitions = 50\n",
      "\toffsets.topic.replication.factor = 1\n",
      "\toffsets.topic.segment.bytes = 104857600\n",
      "\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n",
      "\tpassword.encoder.iterations = 4096\n",
      "\tpassword.encoder.key.length = 128\n",
      "\tpassword.encoder.keyfactory.algorithm = null\n",
      "\tpassword.encoder.old.secret = null\n",
      "\tpassword.encoder.secret = null\n",
      "\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n",
      "\tprocess.roles = []\n",
      "\tproducer.purgatory.purge.interval.requests = 1000\n",
      "\tqueued.max.request.bytes = -1\n",
      "\tqueued.max.requests = 500\n",
      "\tquota.window.num = 11\n",
      "\tquota.window.size.seconds = 1\n",
      "\tremote.log.index.file.cache.total.size.bytes = 1073741824\n",
      "\tremote.log.manager.task.interval.ms = 30000\n",
      "\tremote.log.manager.task.retry.backoff.max.ms = 30000\n",
      "\tremote.log.manager.task.retry.backoff.ms = 500\n",
      "\tremote.log.manager.task.retry.jitter = 0.2\n",
      "\tremote.log.manager.thread.pool.size = 10\n",
      "\tremote.log.metadata.manager.class.name = null\n",
      "\tremote.log.metadata.manager.class.path = null\n",
      "\tremote.log.metadata.manager.impl.prefix = null\n",
      "\tremote.log.metadata.manager.listener.name = null\n",
      "\tremote.log.reader.max.pending.tasks = 100\n",
      "\tremote.log.reader.threads = 10\n",
      "\tremote.log.storage.manager.class.name = null\n",
      "\tremote.log.storage.manager.class.path = null\n",
      "\tremote.log.storage.manager.impl.prefix = null\n",
      "\tremote.log.storage.system.enable = false\n",
      "\treplica.fetch.backoff.ms = 1000\n",
      "\treplica.fetch.max.bytes = 1048576\n",
      "\treplica.fetch.min.bytes = 1\n",
      "\treplica.fetch.response.max.bytes = 10485760\n",
      "\treplica.fetch.wait.max.ms = 500\n",
      "\treplica.high.watermark.checkpoint.interval.ms = 5000\n",
      "\treplica.lag.time.max.ms = 30000\n",
      "\treplica.selector.class = null\n",
      "\treplica.socket.receive.buffer.bytes = 65536\n",
      "\treplica.socket.timeout.ms = 30000\n",
      "\treplication.quota.window.num = 11\n",
      "\treplication.quota.window.size.seconds = 1\n",
      "\trequest.timeout.ms = 30000\n",
      "\treserved.broker.max.id = 1000\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.enabled.mechanisms = [GSSAPI]\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism.controller.protocol = GSSAPI\n",
      "\tsasl.mechanism.inter.broker.protocol = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsasl.server.callback.handler.class = null\n",
      "\tsasl.server.max.receive.size = 524288\n",
      "\tsecurity.inter.broker.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tsocket.listen.backlog.size = 50\n",
      "\tsocket.receive.buffer.bytes = 102400\n",
      "\tsocket.request.max.bytes = 104857600\n",
      "\tsocket.send.buffer.bytes = 102400\n",
      "\tssl.cipher.suites = []\n",
      "\tssl.client.auth = none\n",
      "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.principal.mapping.rules = DEFAULT\n",
      "\tssl.protocol = TLSv1.3\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n",
      "\ttransaction.max.timeout.ms = 900000\n",
      "\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n",
      "\ttransaction.state.log.load.buffer.size = 5242880\n",
      "\ttransaction.state.log.min.isr = 1\n",
      "\ttransaction.state.log.num.partitions = 50\n",
      "\ttransaction.state.log.replication.factor = 1\n",
      "\ttransaction.state.log.segment.bytes = 104857600\n",
      "\ttransactional.id.expiration.ms = 604800000\n",
      "\tunclean.leader.election.enable = false\n",
      "\tzookeeper.clientCnxnSocket = null\n",
      "\tzookeeper.connect = localhost:9799\n",
      "\tzookeeper.connection.timeout.ms = 18000\n",
      "\tzookeeper.max.in.flight.requests = 10\n",
      "\tzookeeper.session.timeout.ms = 18000\n",
      "\tzookeeper.set.acl = false\n",
      "\tzookeeper.ssl.cipher.suites = null\n",
      "\tzookeeper.ssl.client.enable = false\n",
      "\tzookeeper.ssl.crl.enable = false\n",
      "\tzookeeper.ssl.enabled.protocols = null\n",
      "\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n",
      "\tzookeeper.ssl.keystore.location = null\n",
      "\tzookeeper.ssl.keystore.password = null\n",
      "\tzookeeper.ssl.keystore.type = null\n",
      "\tzookeeper.ssl.ocsp.enable = false\n",
      "\tzookeeper.ssl.protocol = TLSv1.2\n",
      "\tzookeeper.ssl.truststore.location = null\n",
      "\tzookeeper.ssl.truststore.password = null\n",
      "\tzookeeper.ssl.truststore.type = null\n",
      " (kafka.server.KafkaConfig)\n",
      "[2023-03-03 10:26:35,531] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:35,532] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:35,538] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:35,541] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:35,557] INFO Log directory /tmp/tmp80atlpl2/kafka_logs not found, creating it. (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:35,579] INFO Loading logs from log dirs ArraySeq(/tmp/tmp80atlpl2/kafka_logs) (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:35,581] INFO Attempting recovery for all logs in /tmp/tmp80atlpl2/kafka_logs since no clean shutdown file was found (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:35,592] INFO Loaded 0 logs in 12ms. (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:35,592] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:35,594] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:35,699] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-03 10:26:35,712] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n",
      "[2023-03-03 10:26:35,749] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:36,206] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n",
      "[2023-03-03 10:26:36,210] INFO Awaiting socket connections on 0.0.0.0:9689. (kafka.network.DataPlaneAcceptor)\n",
      "[2023-03-03 10:26:36,253] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n",
      "[2023-03-03 10:26:36,278] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:36,302] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,302] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,303] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,304] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,320] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-03 10:26:36,360] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n",
      "[2023-03-03 10:26:36,416] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1677839196395,1677839196395,1,0,0,72160383423938560,228,0,25\n",
      " (kafka.zk.KafkaZkClient)\n",
      "[2023-03-03 10:26:36,422] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://tvrtko-fastkafka-devel:9689, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n",
      "[2023-03-03 10:26:36,544] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,551] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,555] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,582] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-03 10:26:36,588] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-03 10:26:36,588] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n",
      "[2023-03-03 10:26:36,612] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-03 10:26:36,626] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-03 10:26:36,644] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n",
      "[2023-03-03 10:26:36,672] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-03 10:26:36,696] INFO [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)\n",
      "[2023-03-03 10:26:36,756] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:36,801] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-03 10:26:36,839] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n",
      "[2023-03-03 10:26:36,876] INFO Kafka version: 3.3.2 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-03-03 10:26:36,876] INFO Kafka commitId: b66af662e61082cb (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-03-03 10:26:36,876] INFO Kafka startTimeMs: 1677839196864 (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-03-03 10:26:36,877] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:37,087] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9689 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:37,107] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9689 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,732] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-03-03 10:26:39,737] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:39,743] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:39,780] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 27ms (kafka.server.KafkaServer)\n",
      "[2023-03-03 10:26:39,783] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-03 10:26:39,784] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-03 10:26:39,784] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-03 10:26:39,786] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer)\n",
      "[2023-03-03 10:26:39,796] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer)\n",
      "[2023-03-03 10:26:39,796] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-03-03 10:26:39,803] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-03-03 10:26:39,805] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,807] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,807] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,808] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)\n",
      "[2023-03-03 10:26:39,808] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,814] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,814] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,815] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-03 10:26:39,816] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)\n",
      "[2023-03-03 10:26:39,816] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-03 10:26:39,817] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-03 10:26:39,817] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-03 10:26:39,818] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-03 10:26:39,818] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-03 10:26:39,819] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,819] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,819] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,819] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,820] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,820] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,821] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-03 10:26:39,821] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)\n",
      "[2023-03-03 10:26:39,822] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-03 10:26:39,823] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-03 10:26:39,823] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-03 10:26:39,825] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)\n",
      "[2023-03-03 10:26:39,826] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)\n",
      "[2023-03-03 10:26:39,826] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-03-03 10:26:39,826] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-03-03 10:26:39,827] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,827] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,827] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,827] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,833] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,833] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,834] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,834] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,834] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,835] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,835] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,835] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-03 10:26:39,840] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)\n",
      "[2023-03-03 10:26:39,841] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,841] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,841] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,843] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-03-03 10:26:39,843] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,844] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,844] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-03 10:26:39,846] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-03-03 10:26:39,847] INFO Shutting down. (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:39,865] INFO Shutdown complete. (kafka.log.LogManager)\n",
      "[2023-03-03 10:26:39,871] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-03 10:26:39,871] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-03 10:26:39,871] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-03 10:26:39,872] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-03 10:26:39,979] INFO Session: 0x1005d7c84d70000 closed (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-03 10:26:39,979] INFO EventThread shut down for session: 0x1005d7c84d70000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-03 10:26:39,982] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-03 10:26:39,983] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,986] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,986] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,987] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,988] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,988] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,989] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,990] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,990] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,990] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,991] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,991] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-03 10:26:39,994] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer)\n",
      "[2023-03-03 10:26:40,056] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer)\n",
      "[2023-03-03 10:26:40,057] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-03 10:26:40,058] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-03 10:26:40,058] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-03 10:26:40,061] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)\n",
      "[2023-03-03 10:26:40,062] INFO App info kafka.server for 0 unregistered (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-03-03 10:26:40,062] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker(zookeeper_port=9799, listener_port=9689)\n",
    "async with broker:\n",
    "    pass\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 9939\n",
    "\n",
    "broker_1 = LocalKafkaBroker(zookeeper_port=port, listener_port=9941)\n",
    "broker_2 = LocalKafkaBroker(zookeeper_port=port, listener_port=9942)\n",
    "async with broker_1:\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        async with broker_2:\n",
    "            pass\n",
    "\n",
    "expected = (\n",
    "    \"Could not start zookeeper with params: {\" + f\"'zookeeper_port': {port}\" + \"}\"\n",
    ")\n",
    "assert e.value.args == (expected,)\n",
    "\n",
    "for broker in [broker_2]:\n",
    "    print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "    zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "    print(zookeeper_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 9939\n",
    "\n",
    "broker_1 = LocalKafkaBroker(zookeeper_port=port, listener_port=9941)\n",
    "broker_2 = LocalKafkaBroker(zookeeper_port=port, listener_port=9941, retries=1)\n",
    "async with broker_1:\n",
    "    async with broker_2:\n",
    "        pass\n",
    "\n",
    "for broker in [broker_2]:\n",
    "    print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "    zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "    print(zookeeper_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def start(self: LocalKafkaBroker) -> str:\n",
    "    \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "    Returns:\n",
    "       Kafka broker bootstrap server address in string format: add:port\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.start(): entering...\")\n",
    "    try:\n",
    "        # get or create loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): RuntimeError raised when calling asyncio.get_event_loop(): {e}\"\n",
    "            )\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): asyncio.new_event_loop()\"\n",
    "            )\n",
    "            loop = asyncio.new_event_loop()\n",
    "\n",
    "        # start zookeeper and kafka broker in the loop\n",
    "\n",
    "        if loop.is_running():\n",
    "            if self.apply_nest_asyncio:\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__}.start(): ({loop}) is already running!\"\n",
    "                )\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__}.start(): calling nest_asyncio.apply()\"\n",
    "                )\n",
    "                nest_asyncio.apply(loop)\n",
    "            else:\n",
    "                msg = f\"{self.__class__.__name__}.start(): ({loop}) is already running! Use 'apply_nest_asyncio=True' when creating 'LocalKafkaBroker' to prevent this.\"\n",
    "                logger.error(msg)\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "        try:\n",
    "            retval = loop.run_until_complete(self._start())\n",
    "            logger.info(f\"{self.__class__}.start(): returning {retval}\")\n",
    "            return retval\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): RuntimeError raised for loop ({loop}): {e}\"\n",
    "            )\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): calling nest_asyncio.apply()\"\n",
    "            )\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.start(): exited.\")\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def stop(self: LocalKafkaBroker) -> None:\n",
    "    \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.stop(): entering...\")\n",
    "    try:\n",
    "        if not self._is_started:\n",
    "            raise RuntimeError(\n",
    "                \"LocalKafkaBroker not started yet, please call LocalKafkaBroker.start() before!\"\n",
    "            )\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self._stop())\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.stop(): exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = LocalKafkaBroker(\n",
    "    zookeeper_port=9998, listener_port=9789, apply_nest_asyncio=True\n",
    ")\n",
    "with broker:\n",
    "    print(\"Hello world!\")\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with LocalKafkaBroker(\n",
    "    zookeeper_port=9998, listener_port=9789, apply_nest_asyncio=True\n",
    ") as bootstrap_servers:\n",
    "    print(bootstrap_servers)\n",
    "    assert bootstrap_servers == \"127.0.0.1:9789\"\n",
    "\n",
    "    msgs = [\n",
    "        dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "        for i in trange(100_000, desc=\"generating messages\")\n",
    "    ]\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with LocalKafkaBroker(\n",
    "    zookeeper_port=9998, listener_port=9789\n",
    ") as bootstrap_servers:\n",
    "    print(bootstrap_servers)\n",
    "    assert bootstrap_servers == \"127.0.0.1:9789\"\n",
    "\n",
    "    msgs = [\n",
    "        dict(user_id=i, feature_1=[(i / 1_000) ** 2], feature_2=[i % 177])\n",
    "        for i in trange(100_000, desc=\"generating messages\")\n",
    "    ]\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        tg.soonify(consumes_messages)(\n",
    "            msgs_count=len(msgs), topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )\n",
    "\n",
    "        await asyncio.sleep(2)\n",
    "\n",
    "        tg.soonify(produce_messages)(\n",
    "            msgs=msgs, topic=\"test_data\", bootstrap_servers=bootstrap_servers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9020fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"topic_1\", \"topic_2\"]\n",
    "async with LocalKafkaBroker(\n",
    "    zookeeper_port=9788, listener_port=9579, topics=topics\n",
    ") as bootstrap_server:\n",
    "    task = await asyncio.create_subprocess_exec(\n",
    "        \"kafka-topics.sh\",\n",
    "        \"--list\",\n",
    "        f\"--bootstrap-server={bootstrap_server}\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stdin=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "    output, _ = await asyncio.wait_for(task.communicate(), 5)\n",
    "    listed_topics = output.decode(\"UTF-8\").split(\"\\n\")[:-1]\n",
    "    assert set(listed_topics) == set(topics)\n",
    "print(\"ok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
