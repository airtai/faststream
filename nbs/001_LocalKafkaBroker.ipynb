{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c520c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _testing.local_broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d47b16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastkafka._components.fastcore_deps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch, delegates, filter_using_signature\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_subprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m terminate_asyncio_process\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_notebook\n",
      "File \u001b[0;32m/work/fastkafka/fastkafka/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdummy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# %% ../nbs/010_Application_export.ipynb 1\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_application\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastKafka\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproducer_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaEvent\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastKafka\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "File \u001b[0;32m/work/fastkafka/fastkafka/_application/app.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m fastkafka\u001b[38;5;241m.\u001b[39m_components\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mshould_supress_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastkafka\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfastcore_deps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     delegates,\n\u001b[1;32m     31\u001b[0m     filter_using_signature,\n\u001b[1;32m     32\u001b[0m     export,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiokafka_consumer_loop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     aiokafka_consumer_loop,\n\u001b[1;32m     36\u001b[0m     sanitize_kafka_config,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiokafka_producer_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIOKafkaProducerManager\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastkafka._components.fastcore_deps'"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "\n",
    "import asyncer\n",
    "import nest_asyncio\n",
    "\n",
    "from fastkafka._components.meta import patch, delegates, filter_using_signature\n",
    "from fastkafka._components._subprocess import terminate_asyncio_process\n",
    "from fastkafka._components.helpers import in_notebook\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka._components.test_dependencies import check_java, check_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ed82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pytest\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "\n",
    "from fastkafka._components.helpers import change_dir\n",
    "from fastkafka._components.logger import supress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81062e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f95ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ef020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fded319",
   "metadata": {},
   "source": [
    "### Local Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552eb74",
   "metadata": {},
   "source": [
    "#### Kafka and zookeeper config helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_zookeeper_config_string(\n",
    "    data_dir: Union[str, Path],  # the directory where the snapshot is stored.\n",
    "    zookeeper_port: int = 2181,  # the port at which the clients will connect\n",
    ") -> str:\n",
    "    \"\"\"Generates a zookeeeper configuration string that can be exported to file\n",
    "    and used to start a zookeeper instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "        zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "    Returns:\n",
    "        Zookeeper configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    zookeeper_config = f\"\"\"dataDir={data_dir}/zookeeper\n",
    "clientPort={zookeeper_port}\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "    return zookeeper_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    get_zookeeper_config_string(data_dir=\"..\")\n",
    "    == \"\"\"dataDir=../zookeeper\n",
    "clientPort=2181\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "assert (\n",
    "    get_zookeeper_config_string(data_dir=\"..\", zookeeper_port=100)\n",
    "    == \"\"\"dataDir=../zookeeper\n",
    "clientPort=100\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_kafka_config_string(\n",
    "    data_dir: Union[str, Path], zookeeper_port: int = 2181, listener_port: int = 9092\n",
    ") -> str:\n",
    "    \"\"\"Generates a kafka broker configuration string that can be exported to file\n",
    "    and used to start a kafka broker instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the kafka broker instance will save data\n",
    "        zookeeper_port: Port on which the zookeeper instance is running\n",
    "        listener_port: Port on which the clients (producers and consumers) can connect\n",
    "    Returns:\n",
    "        Kafka broker configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    kafka_config = f\"\"\"broker.id=0\n",
    "\n",
    "############################# Socket Server Settings #############################\n",
    "\n",
    "# The address the socket server listens on. If not configured, the host name will be equal to the value of\n",
    "# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.\n",
    "#   FORMAT:\n",
    "#     listeners = listener_name://host_name:port\n",
    "#   EXAMPLE:\n",
    "#     listeners = PLAINTEXT://your.host.name:9092\n",
    "listeners=PLAINTEXT://:{listener_port}\n",
    "\n",
    "# Listener name, hostname and port the broker will advertise to clients.\n",
    "# If not set, it uses the value for \"listeners\".\n",
    "# advertised.listeners=PLAINTEXT://localhost:{listener_port}\n",
    "\n",
    "# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details\n",
    "#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
    "\n",
    "# The number of threads that the server uses for receiving requests from the network and sending responses to the network\n",
    "num.network.threads=3\n",
    "\n",
    "# The number of threads that the server uses for processing requests, which may include disk I/O\n",
    "num.io.threads=8\n",
    "\n",
    "# The send buffer (SO_SNDBUF) used by the socket server\n",
    "socket.send.buffer.bytes=102400\n",
    "\n",
    "# The receive buffer (SO_RCVBUF) used by the socket server\n",
    "socket.receive.buffer.bytes=102400\n",
    "\n",
    "# The maximum size of a request that the socket server will accept (protection against OOM)\n",
    "socket.request.max.bytes=104857600\n",
    "\n",
    "\n",
    "############################# Log Basics #############################\n",
    "\n",
    "# A comma separated list of directories under which to store log files\n",
    "log.dirs={data_dir}/kafka_logs\n",
    "\n",
    "# The default number of log partitions per topic. More partitions allow greater\n",
    "# parallelism for consumption, but this will also result in more files across\n",
    "# the brokers.\n",
    "num.partitions=1\n",
    "\n",
    "# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n",
    "# This value is recommended to be increased for installations with data dirs located in RAID array.\n",
    "num.recovery.threads.per.data.dir=1\n",
    "\n",
    "offsets.topic.replication.factor=1\n",
    "transaction.state.log.replication.factor=1\n",
    "transaction.state.log.min.isr=1\n",
    "\n",
    "# The number of messages to accept before forcing a flush of data to disk\n",
    "log.flush.interval.messages=10000\n",
    "\n",
    "# The maximum amount of time a message can sit in a log before we force a flush\n",
    "log.flush.interval.ms=1000\n",
    "\n",
    "# The minimum age of a log file to be eligible for deletion due to age\n",
    "log.retention.hours=168\n",
    "\n",
    "# A size-based retention policy for logs. Segments are pruned from the log unless the remaining\n",
    "# segments drop below log.retention.bytes. Functions independently of log.retention.hours.\n",
    "log.retention.bytes=1073741824\n",
    "\n",
    "# The maximum size of a log segment file. When this size is reached a new log segment will be created.\n",
    "log.segment.bytes=1073741824\n",
    "\n",
    "# The interval at which log segments are checked to see if they can be deleted according to the retention policies\n",
    "log.retention.check.interval.ms=300000\n",
    "\n",
    "# Zookeeper connection string (see zookeeper docs for details).\n",
    "zookeeper.connect=localhost:{zookeeper_port}\n",
    "\n",
    "# Timeout in ms for connecting to zookeeper\n",
    "zookeeper.connection.timeout.ms=18000\n",
    "\n",
    "# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.\n",
    "group.initial.rebalance.delay.ms=0\n",
    "\"\"\"\n",
    "\n",
    "    return kafka_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = get_kafka_config_string(data_dir=\"..\", listener_port=9999)\n",
    "assert \"log.dirs=../kafka_logs\" in actual\n",
    "assert \"listeners=PLAINTEXT://:9999\" in actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class LocalKafkaBroker:\n",
    "    \"\"\"LocalKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.\"\"\"\n",
    "\n",
    "    @delegates(get_kafka_config_string)\n",
    "    @delegates(get_zookeeper_config_string, keep=True)\n",
    "    def __init__(\n",
    "        self,\n",
    "        topics: Iterable[str] = [],\n",
    "        *,\n",
    "        retries: int = 3,\n",
    "        apply_nest_asyncio: bool = False,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ):\n",
    "        \"\"\"Initialises the LocalKafkaBroker object\n",
    "\n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "            listener_port: Port on which the clients (producers and consumers) can connect\n",
    "            topics: List of topics to create after sucessfull Kafka broker startup\n",
    "            retries: Number of retries to create kafka and zookeeper services using random\n",
    "            apply_nest_asyncio: set to True if running in notebook\n",
    "            port allocation if the requested port was taken\n",
    "        \"\"\"\n",
    "        self.zookeeper_kwargs = filter_using_signature(\n",
    "            get_zookeeper_config_string, **kwargs\n",
    "        )\n",
    "        self.kafka_kwargs = filter_using_signature(get_kafka_config_string, **kwargs)\n",
    "\n",
    "        if \"zookeeper_port\" not in self.zookeeper_kwargs:\n",
    "            self.zookeeper_kwargs[\"zookeeper_port\"] = 2181\n",
    "            self.kafka_kwargs[\"zookeeper_port\"] = 2181\n",
    "\n",
    "        if \"listener_port\" not in self.kafka_kwargs:\n",
    "            self.kafka_kwargs[\"listener_port\"] = 9092\n",
    "\n",
    "        self.retries = retries\n",
    "        self.apply_nest_asyncio = apply_nest_asyncio\n",
    "        self.temporary_directory: Optional[TemporaryDirectory] = None\n",
    "        self.temporary_directory_path: Optional[Path] = None\n",
    "        self.kafka_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.zookeeper_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self._is_started = False\n",
    "        self.topics: Iterable[str] = topics\n",
    "\n",
    "    @property\n",
    "    def is_started(self) -> bool:\n",
    "        return self._is_started\n",
    "\n",
    "    @classmethod\n",
    "    def _check_deps(cls) -> None:\n",
    "        \"\"\"Prepares the environment for running Kafka brokers.\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance asynchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_service_config_string(self, service: str, *, data_dir: Path) -> str:\n",
    "        \"\"\"Generates a configuration for a service\n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            service: \"kafka\" or \"zookeeper\", defines which service to get config string for\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_service(self, service: str = \"kafka\") -> None:\n",
    "        \"\"\"Starts the service according to defined service var\n",
    "        Args:\n",
    "            service: \"kafka\" or \"zookeeper\", defines which service to start\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_zookeeper(self) -> None:\n",
    "        \"\"\"Start a local zookeeper instance\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_kafka(self) -> None:\n",
    "        \"\"\"Start a local kafka broker\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _create_topics(self) -> None:\n",
    "        \"\"\"Create missing topics in local Kafka broker\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __enter__(self) -> str:\n",
    "        #         LocalKafkaBroker._check_deps()\n",
    "        return self.start()\n",
    "\n",
    "    def __exit__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        self.stop()\n",
    "\n",
    "    async def __aenter__(self) -> str:\n",
    "        #         LocalKafkaBroker._check_deps()\n",
    "        return await self._start()\n",
    "\n",
    "    async def __aexit__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        await self._stop()\n",
    "\n",
    "\n",
    "LocalKafkaBroker.__module__ = \"fastkafka.testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(LocalKafkaBroker, get_kafka_config_string), get_zookeeper_config_string).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def _check_deps(cls: LocalKafkaBroker) -> None:\n",
    "    if not check_java():\n",
    "        raise RuntimeError(\n",
    "            \"JDK installation not found! Please install JDK manually or run 'fastkafka testing install_deps'.\"\n",
    "        )\n",
    "    if not check_kafka():\n",
    "        raise RuntimeError(\n",
    "            \"Kafka installation not found! Please install Kafka tools manually or run 'fastkafka testing install_deps'.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4600ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n"
     ]
    }
   ],
   "source": [
    "# TODO: test\n",
    "\n",
    "broker = LocalKafkaBroker()\n",
    "broker._check_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d74671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def run_and_match(\n",
    "    *args: str, capture: str = \"stdout\", timeout: int = 5, pattern: str\n",
    ") -> asyncio.subprocess.Process:\n",
    "    # Create the subprocess; redirect the standard output\n",
    "    # into a pipe.\n",
    "\n",
    "    proc = await asyncio.create_subprocess_exec(\n",
    "        *args,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "    # Read one line of output.\n",
    "    t = datetime.now()\n",
    "    while datetime.now() - t < timedelta(seconds=timeout):\n",
    "        try:\n",
    "            if capture == \"stdout\":\n",
    "                data = await asyncio.wait_for(proc.stdout.readline(), timeout=1.0)  # type: ignore\n",
    "            elif capture == \"stderr\":\n",
    "                data = await asyncio.wait_for(proc.stderr.readline(), timeout=1.0)  # type: ignore\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown capture param value {capture}, supported values are 'stdout', 'stderr'\"\n",
    "                )\n",
    "            ddata = data.decode(\"utf-8\")\n",
    "\n",
    "            if len(re.findall(pattern, ddata)) > 0:\n",
    "                # print(f\"Matched: {ddata}\")\n",
    "                return proc\n",
    "        except asyncio.exceptions.TimeoutError as e:\n",
    "            pass\n",
    "\n",
    "        if proc.returncode is not None:\n",
    "            stdout, stderr = await proc.communicate()\n",
    "            dstdout = stdout.decode(\"utf-8\")\n",
    "            dstderr = stderr.decode(\"utf-8\")\n",
    "            raise RuntimeError(\n",
    "                f\"stdout={dstdout}, stderr={dstderr}, returncode={proc.returncode}\"\n",
    "            )\n",
    "\n",
    "    await terminate_asyncio_process(proc)\n",
    "\n",
    "    raise TimeoutError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 773460...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 773460 terminated.\n"
     ]
    }
   ],
   "source": [
    "cmd = \"import datetime; from time import sleep; sleep(3); print('time is:' + str(datetime.datetime.now()))\"\n",
    "# print('\"' + cmd + '\"')\n",
    "\n",
    "with pytest.raises(TimeoutError):\n",
    "    proc = await run_and_match(\"python3\", \"-c\", cmd, pattern=\"time is\", timeout=1)\n",
    "\n",
    "with pytest.raises(RuntimeError):\n",
    "    proc = await run_and_match(\n",
    "        \"python3\", \"-c\", \"should break on this\", pattern=\"time is\", timeout=5\n",
    "    )\n",
    "\n",
    "proc = await run_and_match(\"python3\", \"-c\", cmd, pattern=\"time is\", timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a076ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_free_port() -> str:\n",
    "    s = socket.socket()\n",
    "    s.bind((\"127.0.0.1\", 0))\n",
    "    port = str(s.getsockname()[1])\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "\n",
    "async def write_config_and_run(\n",
    "    config: str, config_path: Union[str, Path], run_cmd: str\n",
    ") -> asyncio.subprocess.Process:\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config)\n",
    "\n",
    "    return await asyncio.create_subprocess_exec(\n",
    "        run_cmd,\n",
    "        config_path,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stdin=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "\n",
    "@patch\n",
    "def get_service_config_string(\n",
    "    self: LocalKafkaBroker, service: str, *, data_dir: Path\n",
    ") -> str:\n",
    "    service_kwargs = getattr(self, f\"{service}_kwargs\")\n",
    "    if service == \"kafka\":\n",
    "        return get_kafka_config_string(data_dir=data_dir, **service_kwargs)\n",
    "    else:\n",
    "        return get_zookeeper_config_string(data_dir=data_dir, **service_kwargs)\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start_service(self: LocalKafkaBroker, service: str = \"kafka\") -> None:\n",
    "    logger.info(f\"Starting {service}...\")\n",
    "\n",
    "    if self.temporary_directory_path is None:\n",
    "        raise ValueError(\n",
    "            \"LocalKafkaBroker._start_service(): self.temporary_directory_path is None, did you initialise it?\"\n",
    "        )\n",
    "\n",
    "    configs_tried: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(self.retries + 1):\n",
    "        configs_tried = configs_tried + [getattr(self, f\"{service}_kwargs\").copy()]\n",
    "\n",
    "        service_config_path = self.temporary_directory_path / f\"{service}.properties\"\n",
    "\n",
    "        with open(service_config_path, \"w\") as f:\n",
    "            f.write(\n",
    "                self.get_service_config_string(\n",
    "                    service, data_dir=self.temporary_directory_path\n",
    "                )\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            service_task = await run_and_match(\n",
    "                f\"{service}-server-start.sh\",\n",
    "                str(service_config_path),\n",
    "                pattern=\"INFO \\[KafkaServer id=0\\] started\"\n",
    "                if service == \"kafka\"\n",
    "                else \"INFO Snapshot taken\",\n",
    "                timeout=30,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logger.info(\n",
    "                f\"{service} startup falied, generating a new port and retrying...\"\n",
    "            )\n",
    "            port = get_free_port()\n",
    "            if service == \"zookeeper\":\n",
    "                self.zookeeper_kwargs[\"zookeeper_port\"] = port\n",
    "                self.kafka_kwargs[\"zookeeper_port\"] = port\n",
    "            else:\n",
    "                self.kafka_kwargs[\"listener_port\"] = port\n",
    "\n",
    "            logger.info(f\"port={port}\")\n",
    "        else:\n",
    "            setattr(self, f\"{service}_task\", service_task)\n",
    "            return\n",
    "\n",
    "    raise ValueError(f\"Could not start {service} with params: {configs_tried}\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start_kafka(self: LocalKafkaBroker) -> None:\n",
    "    return await self._start_service(\"kafka\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start_zookeeper(self: LocalKafkaBroker) -> None:\n",
    "    return await self._start_service(\"zookeeper\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _create_topics(self: LocalKafkaBroker) -> None:\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    bootstrap_server = f\"127.0.0.1:{listener_port}\"\n",
    "\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        processes = [\n",
    "            tg.soonify(asyncio.create_subprocess_exec)(\n",
    "                \"kafka-topics.sh\",\n",
    "                \"--create\",\n",
    "                f\"--topic={topic}\",\n",
    "                f\"--bootstrap-server={bootstrap_server}\",\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stdin=asyncio.subprocess.PIPE,\n",
    "            )\n",
    "            for topic in self.topics\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        return_values = [\n",
    "            await asyncio.wait_for(process.value.wait(), 30) for process in processes\n",
    "        ]\n",
    "        if any(return_value != 0 for return_value in return_values):\n",
    "            raise ValueError(\"Could not create missing topics!\")\n",
    "    except asyncio.TimeoutError as _:\n",
    "        raise ValueError(\"Timed out while creating missing topics!\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start(self: LocalKafkaBroker) -> str:\n",
    "    self._check_deps()\n",
    "\n",
    "    self.temporary_directory = TemporaryDirectory()\n",
    "    self.temporary_directory_path = Path(self.temporary_directory.__enter__())\n",
    "\n",
    "    await self._start_zookeeper()\n",
    "    await self._start_kafka()\n",
    "\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    bootstrap_server = f\"127.0.0.1:{listener_port}\"\n",
    "    logger.info(f\"Local Kafka broker up and running on {bootstrap_server}\")\n",
    "\n",
    "    await self._create_topics()\n",
    "\n",
    "    self._is_started = True\n",
    "\n",
    "    return bootstrap_server\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _stop(self: LocalKafkaBroker) -> None:\n",
    "    await terminate_asyncio_process(self.kafka_task)  # type: ignore\n",
    "    await terminate_asyncio_process(self.zookeeper_task)  # type: ignore\n",
    "    self.temporary_directory.__exit__(None, None, None)  # type: ignore\n",
    "    self._is_started = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe12c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=34263\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 719446...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 719446 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 719086...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 719086 terminated.\n",
      "**************************************************ZOOKEEPER LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-03-08 13:34:48,027] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)\n",
      "[2023-03-08 13:34:48,036] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)\n",
      "[2023-03-08 13:34:48,048] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)\n",
      "[2023-03-08 13:34:48,049] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[2023-03-08 13:34:49,241] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "\n",
      "**************************************************KAFKA LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-03-08 13:34:50,640] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:50,682] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,493] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-03-08 13:34:51,495] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)\n",
      "[2023-03-08 13:34:51,496] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)\n",
      "[2023-03-08 13:34:51,517] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 16ms (kafka.server.KafkaServer)\n",
      "[2023-03-08 13:34:51,520] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-08 13:34:51,526] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-08 13:34:51,527] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-08 13:34:51,528] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer)\n",
      "[2023-03-08 13:34:51,534] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer)\n",
      "[2023-03-08 13:34:51,535] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-03-08 13:34:51,537] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-03-08 13:34:51,547] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,549] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,549] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,549] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)\n",
      "[2023-03-08 13:34:51,550] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,551] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,551] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,552] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-08 13:34:51,553] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)\n",
      "[2023-03-08 13:34:51,553] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-08 13:34:51,559] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-08 13:34:51,559] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-08 13:34:51,560] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-08 13:34:51,561] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-08 13:34:51,561] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,562] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,562] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,563] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,564] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,564] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,565] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-08 13:34:51,566] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)\n",
      "[2023-03-08 13:34:51,566] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-08 13:34:51,566] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-08 13:34:51,566] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-08 13:34:51,567] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)\n",
      "[2023-03-08 13:34:51,567] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)\n",
      "[2023-03-08 13:34:51,568] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-03-08 13:34:51,568] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-03-08 13:34:51,568] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,569] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,569] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,569] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,577] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,577] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,577] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,578] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,578] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,578] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,578] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,578] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:34:51,583] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)\n",
      "[2023-03-08 13:34:51,584] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,584] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,584] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,586] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-03-08 13:34:51,586] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,586] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,586] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:34:51,587] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-03-08 13:34:51,587] INFO Shutting down. (kafka.log.LogManager)\n",
      "[2023-03-08 13:34:51,617] INFO Shutdown complete. (kafka.log.LogManager)\n",
      "[2023-03-08 13:34:51,622] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-08 13:34:51,622] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-08 13:34:51,622] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-08 13:34:51,623] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-08 13:34:51,730] INFO Session: 0x10077e8b5ab0000 closed (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-08 13:34:51,730] INFO EventThread shut down for session: 0x10077e8b5ab0000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-08 13:34:51,733] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-08 13:34:51,734] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,737] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,738] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,738] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,739] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,739] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,739] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,740] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,740] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,740] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,741] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,741] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:34:51,743] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer)\n",
      "[2023-03-08 13:34:51,782] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer)\n",
      "[2023-03-08 13:34:51,783] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-08 13:34:51,783] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-08 13:34:51,784] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-08 13:34:51,785] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)\n",
      "[2023-03-08 13:34:51,785] INFO App info kafka.server for 0 unregistered (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-03-08 13:34:51,786] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker()\n",
    "async with broker:\n",
    "    pass\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10338a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=39569\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=38265\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 722097...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 722097 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 721737...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 721737 terminated.\n"
     ]
    }
   ],
   "source": [
    "broker_1 = LocalKafkaBroker()\n",
    "async with broker_1:\n",
    "    port = broker_1.zookeeper_kwargs[\"zookeeper_port\"]\n",
    "    broker_2 = LocalKafkaBroker(zookeeper_port=port, retries=0)\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        async with broker_2:\n",
    "            pass\n",
    "\n",
    "assert e.value.args[0].startswith(\"Could not start zookeeper with params:\")\n",
    "\n",
    "for broker in [broker_2]:\n",
    "    assert broker.zookeeper_task == None\n",
    "#     print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "#     zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "#     print(zookeeper_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def start(self: LocalKafkaBroker) -> str:\n",
    "    \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "    Returns:\n",
    "       Kafka broker bootstrap server address in string format: add:port\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.start(): entering...\")\n",
    "    try:\n",
    "        # get or create loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): RuntimeError raised when calling asyncio.get_event_loop(): {e}\"\n",
    "            )\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): asyncio.new_event_loop()\"\n",
    "            )\n",
    "            loop = asyncio.new_event_loop()\n",
    "\n",
    "        # start zookeeper and kafka broker in the loop\n",
    "\n",
    "        if loop.is_running():\n",
    "            if self.apply_nest_asyncio:\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__}.start(): ({loop}) is already running!\"\n",
    "                )\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__}.start(): calling nest_asyncio.apply()\"\n",
    "                )\n",
    "                nest_asyncio.apply(loop)\n",
    "            else:\n",
    "                msg = f\"{self.__class__.__name__}.start(): ({loop}) is already running! Use 'apply_nest_asyncio=True' when creating 'LocalKafkaBroker' to prevent this.\"\n",
    "                logger.error(msg)\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "        retval = loop.run_until_complete(self._start())\n",
    "        logger.info(f\"{self.__class__}.start(): returning {retval}\")\n",
    "        return retval\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.start(): exited.\")\n",
    "\n",
    "\n",
    "@patch\n",
    "def stop(self: LocalKafkaBroker) -> None:\n",
    "    \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.stop(): entering...\")\n",
    "    try:\n",
    "        if not self._is_started:\n",
    "            raise RuntimeError(\n",
    "                \"LocalKafkaBroker not started yet, please call LocalKafkaBroker.start() before!\"\n",
    "            )\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self._stop())\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.stop(): exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: LocalKafkaBroker.start(): entering...\n",
      "[WARNING] __main__: LocalKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "[WARNING] __main__: LocalKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=49167\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: <class 'fastkafka.testing.LocalKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] __main__: LocalKafkaBroker.start(): exited.\n",
      "Hello world!\n",
      "[INFO] __main__: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 723599...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 723599 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 723239...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 723239 terminated.\n",
      "[INFO] __main__: LocalKafkaBroker.stop(): exited.\n",
      "**************************************************ZOOKEEPER LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-03-08 13:38:26,294] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)\n",
      "[2023-03-08 13:38:26,294] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)\n",
      "[2023-03-08 13:38:26,310] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)\n",
      "[2023-03-08 13:38:26,311] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)\n",
      "[2023-03-08 13:38:27,463] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)\n",
      "\n",
      "**************************************************KAFKA LOGS++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[2023-03-08 13:38:28,756] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:28,783] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node tvrtko-fastkafka-devel:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,711] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)\n",
      "[2023-03-08 13:38:29,715] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)\n",
      "[2023-03-08 13:38:29,719] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)\n",
      "[2023-03-08 13:38:29,761] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 29ms (kafka.server.KafkaServer)\n",
      "[2023-03-08 13:38:29,767] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-08 13:38:29,769] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-08 13:38:29,769] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n",
      "[2023-03-08 13:38:29,770] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors (kafka.network.SocketServer)\n",
      "[2023-03-08 13:38:29,779] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors (kafka.network.SocketServer)\n",
      "[2023-03-08 13:38:29,780] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-03-08 13:38:29,782] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)\n",
      "[2023-03-08 13:38:29,785] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,786] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,786] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,787] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)\n",
      "[2023-03-08 13:38:29,787] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,788] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,788] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,790] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-08 13:38:29,790] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)\n",
      "[2023-03-08 13:38:29,791] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-08 13:38:29,792] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-08 13:38:29,792] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n",
      "[2023-03-08 13:38:29,793] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)\n",
      "[2023-03-08 13:38:29,793] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-08 13:38:29,794] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,795] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,795] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,795] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,796] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,796] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,796] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)\n",
      "[2023-03-08 13:38:29,797] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)\n",
      "[2023-03-08 13:38:29,797] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-08 13:38:29,798] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-08 13:38:29,798] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)\n",
      "[2023-03-08 13:38:29,798] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)\n",
      "[2023-03-08 13:38:29,799] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)\n",
      "[2023-03-08 13:38:29,799] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-03-08 13:38:29,800] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)\n",
      "[2023-03-08 13:38:29,800] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,800] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,800] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,801] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,801] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,802] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,802] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,803] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,803] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,803] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,804] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,804] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n",
      "[2023-03-08 13:38:29,809] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)\n",
      "[2023-03-08 13:38:29,809] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,810] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,810] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,812] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-03-08 13:38:29,812] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,813] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,813] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)\n",
      "[2023-03-08 13:38:29,813] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)\n",
      "[2023-03-08 13:38:29,814] INFO Shutting down. (kafka.log.LogManager)\n",
      "[2023-03-08 13:38:29,837] INFO Shutdown complete. (kafka.log.LogManager)\n",
      "[2023-03-08 13:38:29,843] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-08 13:38:29,843] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-08 13:38:29,843] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n",
      "[2023-03-08 13:38:29,843] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-08 13:38:29,951] INFO Session: 0x10077ec0a410000 closed (org.apache.zookeeper.ZooKeeper)\n",
      "[2023-03-08 13:38:29,951] INFO EventThread shut down for session: 0x10077ec0a410000 (org.apache.zookeeper.ClientCnxn)\n",
      "[2023-03-08 13:38:29,954] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)\n",
      "[2023-03-08 13:38:29,955] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,959] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,959] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,960] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,960] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,961] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,961] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,962] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,962] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,962] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,963] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,963] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n",
      "[2023-03-08 13:38:29,966] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server (kafka.network.SocketServer)\n",
      "[2023-03-08 13:38:30,010] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed (kafka.network.SocketServer)\n",
      "[2023-03-08 13:38:30,010] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-08 13:38:30,010] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-08 13:38:30,010] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)\n",
      "[2023-03-08 13:38:30,012] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)\n",
      "[2023-03-08 13:38:30,013] INFO App info kafka.server for 0 unregistered (org.apache.kafka.common.utils.AppInfoParser)\n",
      "[2023-03-08 13:38:30,013] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broker = LocalKafkaBroker(apply_nest_asyncio=True)\n",
    "with broker:\n",
    "    print(\"Hello world!\")\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5fa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=34731\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 724751...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 724751 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 724390...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 724390 terminated.\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "topics = [\"topic_1\", \"topic_2\"]\n",
    "\n",
    "async with LocalKafkaBroker(topics=topics) as bootstrap_server:\n",
    "    task = await asyncio.create_subprocess_exec(\n",
    "        \"kafka-topics.sh\",\n",
    "        \"--list\",\n",
    "        f\"--bootstrap-server={bootstrap_server}\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stdin=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "    output, _ = await asyncio.wait_for(task.communicate(), 30)\n",
    "    listed_topics = output.decode(\"UTF-8\").split(\"\\n\")[:-1]\n",
    "    assert set(listed_topics) == set(topics)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: LocalKafkaBroker.start(): entering...\n",
      "[WARNING] __main__: LocalKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!\n",
      "[WARNING] __main__: LocalKafkaBroker.start(): calling nest_asyncio.apply()\n",
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=51025\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] __main__: <class 'fastkafka.testing.LocalKafkaBroker'>.start(): returning 127.0.0.1:9092\n",
      "[INFO] __main__: LocalKafkaBroker.start(): exited.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'test-topic'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'test-topic': 1}. \n",
      "[INFO] __main__: LocalKafkaBroker.stop(): entering...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 726924...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 726924 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 726562...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 726562 terminated.\n",
      "[INFO] __main__: LocalKafkaBroker.stop(): exited.\n"
     ]
    }
   ],
   "source": [
    "test_topic = \"test-topic\"\n",
    "test_msg = b\"test-msg\"\n",
    "\n",
    "with LocalKafkaBroker(topics=[test_topic], apply_nest_asyncio=True) as bootstrap_server:\n",
    "    consumer = AIOKafkaConsumer(test_topic, bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    await consumer.start()\n",
    "    await producer.start()\n",
    "\n",
    "    try:\n",
    "        await producer.send_and_wait(test_topic, test_msg)\n",
    "        msg = await consumer.getone()\n",
    "        assert msg, value == test_msg\n",
    "    finally:\n",
    "        await consumer.stop()\n",
    "        await producer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Java is already installed.\n",
      "[INFO] __main__: Kafka is already installed.\n",
      "[INFO] __main__: Starting zookeeper...\n",
      "stdout=b'', stderr=b'', returncode=1\n",
      "[INFO] __main__: zookeeper startup falied, generating a new port and retrying...\n",
      "[INFO] __main__: port=37997\n",
      "[INFO] __main__: Starting kafka...\n",
      "[INFO] __main__: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'test-topic'})\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'test-topic': 1}. \n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 728416...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 728416 terminated.\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 728056...\n",
      "[INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 728056 terminated.\n"
     ]
    }
   ],
   "source": [
    "test_topic = \"test-topic\"\n",
    "test_msg = b\"test-msg\"\n",
    "\n",
    "async with LocalKafkaBroker(topics=[test_topic]) as bootstrap_server:\n",
    "    consumer = AIOKafkaConsumer(test_topic, bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    await consumer.start()\n",
    "    await producer.start()\n",
    "\n",
    "    try:\n",
    "        await producer.send_and_wait(test_topic, test_msg)\n",
    "        msg = await consumer.getone()\n",
    "        assert msg, value == test_msg\n",
    "    finally:\n",
    "        await consumer.stop()\n",
    "        await producer.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
