{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff734a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import functools\n",
    "import json\n",
    "import tempfile\n",
    "import time\n",
    "from asyncio import iscoroutinefunction  # do not use the version from inspect\n",
    "from contextlib import asynccontextmanager, contextmanager\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from inspect import signature\n",
    "from os import environ, getpid\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from typing import get_type_hints\n",
    "import threading\n",
    "import signal\n",
    "\n",
    "import anyio\n",
    "import asyncer\n",
    "import confluent_kafka\n",
    "import httpx\n",
    "import yaml\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from confluent_kafka import KafkaError, Message, Producer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from fastcore.foundation import patch\n",
    "from fastcore.meta import delegates\n",
    "from pydantic import BaseModel, EmailStr, Field, HttpUrl, PositiveInt\n",
    "from pydantic.json import timedelta_isoformat\n",
    "from pydantic.schema import schema\n",
    "\n",
    "import fastkafka._components.logger\n",
    "\n",
    "fastkafka._components.logger.should_supress_timestamps = True\n",
    "\n",
    "import fastkafka\n",
    "from fastkafka._components.aiokafka_consumer_loop import (\n",
    "    aiokafka_consumer_loop,\n",
    "    sanitize_kafka_config,\n",
    ")\n",
    "from fastkafka._components.aiokafka_producer_manager import AIOKafkaProducerManager\n",
    "from fastkafka._components.asyncapi import (\n",
    "    ConsumeCallable,\n",
    "    ContactInfo,\n",
    "    KafkaBroker,\n",
    "    KafkaBrokers,\n",
    "    KafkaMessage,\n",
    "    KafkaServiceInfo,\n",
    "    ProduceCallable,\n",
    "    export_async_spec,\n",
    ")\n",
    "from fastkafka._components.helpers import combine_params\n",
    "from fastkafka._components.logger import get_logger, supress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf16b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import unittest.mock\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import nest_asyncio\n",
    "import pytest\n",
    "import uvicorn\n",
    "import yaml\n",
    "from rich.pretty import pprint\n",
    "from starlette.datastructures import Headers\n",
    "\n",
    "from fastkafka.testing import mock_AIOKafkaProducer_send, true_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9177ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "# allows async calls in notebooks\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3411e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n",
    "kafka_server_port = environ[\"KAFKA_PORT\"]\n",
    "\n",
    "kafka_config = {\n",
    "    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n",
    "    \"group_id\": f\"{kafka_server_url}:{kafka_server_port}_group\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211534a4",
   "metadata": {},
   "source": [
    "### Constructor utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@delegates(AIOKafkaConsumer) # type: ignore\n",
    "@delegates(AIOKafkaProducer, keep=True) # type: ignore\n",
    "def _get_kafka_config(\n",
    "    **kwargs,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Get kafka config\n",
    "    Args:\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "        group_id (str or None): name of the consumer group to join for dynamic\n",
    "            partition assignment (if enabled), and to use for fetching and\n",
    "            committing offsets. If None, auto-partition assignment (via\n",
    "            group coordinator) and offset commits are disabled.\n",
    "            Default: None\n",
    "        key_deserializer (Callable): Any callable that takes a\n",
    "            raw message key and returns a deserialized key.\n",
    "        value_deserializer (Callable, Optional): Any callable that takes a\n",
    "            raw message value and returns a deserialized value.\n",
    "        fetch_min_bytes (int): Minimum amount of data the server should\n",
    "            return for a fetch request, otherwise wait up to\n",
    "            `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
    "        fetch_max_bytes (int): The maximum amount of data the server should\n",
    "            return for a fetch request. This is not an absolute maximum, if\n",
    "            the first message in the first non-empty partition of the fetch\n",
    "            is larger than this value, the message will still be returned\n",
    "            to ensure that the consumer can make progress. NOTE: consumer\n",
    "            performs fetches to multiple brokers in parallel so memory\n",
    "            usage will depend on the number of brokers containing\n",
    "            partitions for the topic.\n",
    "            Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
    "        fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
    "            the server will block before answering the fetch request if\n",
    "            there isn't sufficient data to immediately satisfy the\n",
    "            requirement given by fetch_min_bytes. Default: 500.\n",
    "        max_partition_fetch_bytes (int): The maximum amount of data\n",
    "            per-partition the server will return. The maximum total memory\n",
    "            used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
    "            This size must be at least as large as the maximum message size\n",
    "            the server allows or else it is possible for the producer to\n",
    "            send messages larger than the consumer can fetch. If that\n",
    "            happens, the consumer can get stuck trying to fetch a large\n",
    "            message on a certain partition. Default: 1048576.\n",
    "        max_poll_records (int): The maximum number of records returned in a\n",
    "            single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
    "        auto_offset_reset (str): A policy for resetting offsets on\n",
    "            :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
    "            available message, ``latest`` will move to the most recent, and\n",
    "            ``none`` will raise an exception so you can handle this case.\n",
    "            Default: ``latest``.\n",
    "        enable_auto_commit (bool): If true the consumer's offset will be\n",
    "            periodically committed in the background. Default: True.\n",
    "        auto_commit_interval_ms (int): milliseconds between automatic\n",
    "            offset commits, if enable_auto_commit is True. Default: 5000.\n",
    "        check_crcs (bool): Automatically check the CRC32 of the records\n",
    "            consumed. This ensures no on-the-wire or on-disk corruption to\n",
    "            the messages occurred. This check adds some overhead, so it may\n",
    "            be disabled in cases seeking extreme performance. Default: True\n",
    "        partition_assignment_strategy (list): List of objects to use to\n",
    "            distribute partition ownership amongst consumer instances when\n",
    "            group management is used. This preference is implicit in the order\n",
    "            of the strategies in the list. When assignment strategy changes:\n",
    "            to support a change to the assignment strategy, new versions must\n",
    "            enable support both for the old assignment strategy and the new\n",
    "            one. The coordinator will choose the old assignment strategy until\n",
    "            all members have been updated. Then it will choose the new\n",
    "            strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
    "        max_poll_interval_ms (int): Maximum allowed time between calls to\n",
    "            consume messages (e.g., :meth:`.getmany`). If this interval\n",
    "            is exceeded the consumer is considered failed and the group will\n",
    "            rebalance in order to reassign the partitions to another consumer\n",
    "            group member. If API methods block waiting for messages, that time\n",
    "            does not count against this timeout. See `KIP-62`_ for more\n",
    "            information. Default 300000\n",
    "        rebalance_timeout_ms (int): The maximum time server will wait for this\n",
    "            consumer to rejoin the group in a case of rebalance. In Java client\n",
    "            this behaviour is bound to `max.poll.interval.ms` configuration,\n",
    "            but as ``aiokafka`` will rejoin the group in the background, we\n",
    "            decouple this setting to allow finer tuning by users that use\n",
    "            :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
    "            to ``session_timeout_ms``\n",
    "        session_timeout_ms (int): Client group session and failure detection\n",
    "            timeout. The consumer sends periodic heartbeats\n",
    "            (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
    "            If no hearts are received by the broker for a group member within\n",
    "            the session timeout, the broker will remove the consumer from the\n",
    "            group and trigger a rebalance. The allowed range is configured with\n",
    "            the **broker** configuration properties\n",
    "            `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
    "            Default: 10000\n",
    "        heartbeat_interval_ms (int): The expected time in milliseconds\n",
    "            between heartbeats to the consumer coordinator when using\n",
    "            Kafka's group management feature. Heartbeats are used to ensure\n",
    "            that the consumer's session stays active and to facilitate\n",
    "            rebalancing when new consumers join or leave the group. The\n",
    "            value must be set lower than `session_timeout_ms`, but typically\n",
    "            should be set no higher than 1/3 of that value. It can be\n",
    "            adjusted even lower to control the expected time for normal\n",
    "            rebalances. Default: 3000\n",
    "        consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
    "            routine. Mostly defines how fast the system will see rebalance and\n",
    "            request new data for new partitions. Default: 200\n",
    "        exclude_internal_topics (bool): Whether records from internal topics\n",
    "            (such as offsets) should be exposed to the consumer. If set to True\n",
    "            the only way to receive records from an internal topic is\n",
    "            subscribing to it. Requires 0.10+ Default: True\n",
    "        isolation_level (str): Controls how to read messages written\n",
    "            transactionally.\n",
    "\n",
    "            If set to ``read_committed``, :meth:`.getmany` will only return\n",
    "            transactional messages which have been committed.\n",
    "            If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
    "            return all messages, even transactional messages which have been\n",
    "            aborted.\n",
    "\n",
    "            Non-transactional messages will be returned unconditionally in\n",
    "            either mode.\n",
    "\n",
    "            Messages will always be returned in offset order. Hence, in\n",
    "            `read_committed` mode, :meth:`.getmany` will only return\n",
    "            messages up to the last stable offset (LSO), which is the one less\n",
    "            than the offset of the first open transaction. In particular any\n",
    "            messages appearing after messages belonging to ongoing transactions\n",
    "            will be withheld until the relevant transaction has been completed.\n",
    "            As a result, `read_committed` consumers will not be able to read up\n",
    "            to the high watermark when there are in flight transactions.\n",
    "            Further, when in `read_committed` the seek_to_end method will\n",
    "            return the LSO. See method docs below. Default: ``read_uncommitted``\n",
    "        sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
    "            Default: None\n",
    "    \"\"\"\n",
    "    allowed_keys = set(signature(_get_kafka_config).parameters.keys())\n",
    "    if not set(kwargs.keys()) <= allowed_keys:\n",
    "        unallowed_keys = \", \".join(\n",
    "            sorted([f\"'{x}'\" for x in set(kwargs.keys()).difference(allowed_keys)])\n",
    "        )\n",
    "        raise ValueError(f\"Unallowed key arguments passed: {unallowed_keys}\")\n",
    "    retval = kwargs.copy()\n",
    "\n",
    "    # todo: check this values\n",
    "    config_defaults = {\n",
    "        \"bootstrap_servers\": \"localhost:9092\",\n",
    "        \"auto_offset_reset\": \"earliest\",\n",
    "        \"max_poll_records\": 100,\n",
    "        #         \"max_buffer_size\": 10_000,\n",
    "    }\n",
    "    for key, value in config_defaults.items():\n",
    "        if key not in retval:\n",
    "            retval[key] = value\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(_get_kafka_config, AIOKafkaConsumer).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature(_get_kafka_config).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _get_kafka_config() == {\n",
    "    \"bootstrap_servers\": \"localhost:9092\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "    \"max_poll_records\": 100,\n",
    "}\n",
    "\n",
    "assert _get_kafka_config(max_poll_records=1_000) == {\n",
    "    \"bootstrap_servers\": \"localhost:9092\",\n",
    "    \"auto_offset_reset\": \"earliest\",\n",
    "    \"max_poll_records\": 1_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b17b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as e:\n",
    "    _get_kafka_config(random_key=1_000, whatever=\"whocares\")\n",
    "assert e.value.args == (\"Unallowed key arguments passed: 'random_key', 'whatever'\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3477cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_kafka_brokers(kafka_brokers: Optional[Dict[str, Any]] = None) -> KafkaBrokers:\n",
    "    \"\"\"Get Kafka brokers\n",
    "\n",
    "    Args:\n",
    "        kafka_brokers: Kafka brokers\n",
    "\n",
    "    \"\"\"\n",
    "    if kafka_brokers is None:\n",
    "        retval: KafkaBrokers = KafkaBrokers(\n",
    "            brokers={\n",
    "                \"localhost\": KafkaBroker(\n",
    "                    url=\"https://localhost\",\n",
    "                    description=\"Local (dev) Kafka broker\",\n",
    "                    port=\"9092\",\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        retval = KafkaBrokers(\n",
    "            brokers={\n",
    "                k: KafkaBroker.parse_raw(\n",
    "                    v.json() if hasattr(v, \"json\") else json.dumps(v)\n",
    "                )\n",
    "                for k, v in kafka_brokers.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    _get_kafka_brokers(None).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"https://localhost\", \"description\": \"Local (dev) Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")\n",
    "\n",
    "assert (\n",
    "    _get_kafka_brokers(dict(localhost=dict(url=\"localhost\"))).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")\n",
    "\n",
    "assert (\n",
    "    _get_kafka_brokers(\n",
    "        dict(localhost=dict(url=\"localhost\"), staging=dict(url=\"staging.airt.ai\"))\n",
    "    ).json()\n",
    "    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"staging.airt.ai\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_topic_name(\n",
    "    topic_callable: Union[ConsumeCallable, ProduceCallable], prefix: str = \"on_\"\n",
    ") -> str:\n",
    "    \"\"\"Get topic name\n",
    "    Args:\n",
    "        topic_callable: a function\n",
    "        prefix: prefix of the name of the function followed by the topic name\n",
    "\n",
    "    Returns:\n",
    "        The name of the topic\n",
    "    \"\"\"\n",
    "    topic = topic_callable.__name__\n",
    "    if not topic.startswith(prefix) or len(topic) <= len(prefix):\n",
    "        raise ValueError(f\"Function name '{topic}' must start with {prefix}\")\n",
    "    topic = topic[len(prefix) :]\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_topic_name_1():\n",
    "    pass\n",
    "\n",
    "\n",
    "assert _get_topic_name(on_topic_name_1) == \"topic_name_1\"\n",
    "\n",
    "assert _get_topic_name(on_topic_name_1, prefix=\"on_topic_\") == \"name_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_contact_info(\n",
    "    name: str = \"Author\",\n",
    "    url: str = \"https://www.google.com\",\n",
    "    email: str = \"noreply@gmail.com\",\n",
    ") -> ContactInfo:\n",
    "    return ContactInfo(name=name, url=url, email=email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e311f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _get_contact_info() == ContactInfo(\n",
    "    name=\"Author\",\n",
    "    url=HttpUrl(url=\"https://www.google.com\", scheme=\"http\"),\n",
    "    email=\"noreply@gmail.com\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c37353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class FastKafka:\n",
    "    @delegates(_get_kafka_config)  # type: ignore\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        title: Optional[str] = None,\n",
    "        description: Optional[str] = None,\n",
    "        version: Optional[str] = None,\n",
    "        contact: Optional[Dict[str, str]] = None,\n",
    "        kafka_brokers: Optional[Dict[str, Any]] = None,\n",
    "        root_path: Optional[Union[Path, str]] = None,\n",
    "        skip_docs: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Creates FastKafka application\n",
    "\n",
    "        Args:\n",
    "            title: optional title for the documentation. If None,\n",
    "                the title will be set to empty string\n",
    "            description: optional description for the documentation. If\n",
    "                None, the description will be set to empty string\n",
    "            version: optional version for the documentation. If None,\n",
    "                the version will be set to empty string\n",
    "            contact: optional contact for the documentation. If None, the\n",
    "                contact will be set to placeholder values:\n",
    "                name='Author' url=HttpUrl('https://www.google.com', ) email='noreply@gmail.com'\n",
    "            kafka_brokers: dictionary describing kafka brokers used for\n",
    "                generating documentation\n",
    "            root_path: path to where documentation will be created\n",
    "            skip_docs: Boolean indicating whether to skip the generation of html docs, defaults to true\n",
    "            bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "                ``host[:port]`` strings that the producer should contact to\n",
    "                bootstrap initial cluster metadata. This does not have to be the\n",
    "                full node list.  It just needs to have at least one broker that will\n",
    "                respond to a Metadata API Request. Default port is 9092. If no\n",
    "                servers are specified, will default to ``localhost:9092``.\n",
    "            client_id (str): a name for this client. This string is passed in\n",
    "                each request to servers and can be used to identify specific\n",
    "                server-side log entries that correspond to this client.\n",
    "                Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "                per instance)\n",
    "            key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "                If not :data:`None`, called as ``f(key),`` should return\n",
    "                :class:`bytes`.\n",
    "                Default: :data:`None`.\n",
    "            value_serializer (Callable): used to convert user-supplied message\n",
    "                values to :class:`bytes`. If not :data:`None`, called as\n",
    "                ``f(value)``, should return :class:`bytes`.\n",
    "                Default: :data:`None`.\n",
    "            acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "                the producer requires the leader to have received before considering a\n",
    "                request complete. This controls the durability of records that are\n",
    "                sent. The following settings are common:\n",
    "\n",
    "                * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "                  at all. The message will immediately be added to the socket\n",
    "                  buffer and considered sent. No guarantee can be made that the\n",
    "                  server has received the record in this case, and the retries\n",
    "                  configuration will not take effect (as the client won't\n",
    "                  generally know of any failures). The offset given back for each\n",
    "                  record will always be set to -1.\n",
    "                * ``1``: The broker leader will write the record to its local log but\n",
    "                  will respond without awaiting full acknowledgement from all\n",
    "                  followers. In this case should the leader fail immediately\n",
    "                  after acknowledging the record but before the followers have\n",
    "                  replicated it then the record will be lost.\n",
    "                * ``all``: The broker leader will wait for the full set of in-sync\n",
    "                  replicas to acknowledge the record. This guarantees that the\n",
    "                  record will not be lost as long as at least one in-sync replica\n",
    "                  remains alive. This is the strongest available guarantee.\n",
    "\n",
    "                If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "                :data:`True` defaults to ``acks=all``\n",
    "            compression_type (str): The compression type for all data generated by\n",
    "                the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "                or :data:`None`.\n",
    "                Compression is of full batches of data, so the efficacy of batching\n",
    "                will also impact the compression ratio (more batching means better\n",
    "                compression). Default: :data:`None`.\n",
    "            max_batch_size (int): Maximum size of buffered data per partition.\n",
    "                After this amount :meth:`send` coroutine will block until batch is\n",
    "                drained.\n",
    "                Default: 16384\n",
    "            linger_ms (int): The producer groups together any records that arrive\n",
    "                in between request transmissions into a single batched request.\n",
    "                Normally this occurs only under load when records arrive faster\n",
    "                than they can be sent out. However in some circumstances the client\n",
    "                may want to reduce the number of requests even under moderate load.\n",
    "                This setting accomplishes this by adding a small amount of\n",
    "                artificial delay; that is, if first request is processed faster,\n",
    "                than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "                Default: 0 (i.e. no delay).\n",
    "            partitioner (Callable): Callable used to determine which partition\n",
    "                each message is assigned to. Called (after key serialization):\n",
    "                ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "                The default partitioner implementation hashes each non-None key\n",
    "                using the same murmur2 algorithm as the Java client so that\n",
    "                messages with the same key are assigned to the same partition.\n",
    "                When a key is :data:`None`, the message is delivered to a random partition\n",
    "                (filtered to partitions with available leaders only, if possible).\n",
    "            max_request_size (int): The maximum size of a request. This is also\n",
    "                effectively a cap on the maximum record size. Note that the server\n",
    "                has its own cap on record size which may be different from this.\n",
    "                This setting will limit the number of record batches the producer\n",
    "                will send in a single request to avoid sending huge requests.\n",
    "                Default: 1048576.\n",
    "            metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "                which we force a refresh of metadata even if we haven't seen any\n",
    "                partition leadership changes to proactively discover any new\n",
    "                brokers or partitions. Default: 300000\n",
    "            request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "                As it's sent as part of\n",
    "                :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "                call), maximum waiting time can be up to ``2 *\n",
    "                request_timeout_ms``.\n",
    "                Default: 40000.\n",
    "            retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "                errors. Default: 100.\n",
    "            api_version (str): specify which kafka API version to use.\n",
    "                If set to ``auto``, will attempt to infer the broker version by\n",
    "                probing various APIs. Default: ``auto``\n",
    "            security_protocol (str): Protocol used to communicate with brokers.\n",
    "                Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "                Default: ``PLAINTEXT``.\n",
    "            ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "                for wrapping socket connections. Directly passed into asyncio's\n",
    "                :meth:`~asyncio.loop.create_connection`. For more\n",
    "                information see :ref:`ssl_auth`.\n",
    "                Default: :data:`None`\n",
    "            connections_max_idle_ms (int): Close idle connections after the number\n",
    "                of milliseconds specified by this config. Specifying :data:`None` will\n",
    "                disable idle checks. Default: 540000 (9 minutes).\n",
    "            enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "                ensure that exactly one copy of each message is written in the\n",
    "                stream. If :data:`False`, producer retries due to broker failures,\n",
    "                etc., may write duplicates of the retried message in the stream.\n",
    "                Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "                explicitly set by the user it will be chosen. If incompatible\n",
    "                values are set, a :exc:`ValueError` will be thrown.\n",
    "                New in version 0.5.0.\n",
    "            sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "                is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "                are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "                ``OAUTHBEARER``.\n",
    "                Default: ``PLAIN``\n",
    "            sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "                Default: :data:`None`\n",
    "            sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "                Default: :data:`None`\n",
    "            sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "                OAuthBearer token provider instance. (See\n",
    "                :mod:`kafka.oauth.abstract`).\n",
    "                Default: :data:`None`\n",
    "            group_id (str or None): name of the consumer group to join for dynamic\n",
    "                partition assignment (if enabled), and to use for fetching and\n",
    "                committing offsets. If None, auto-partition assignment (via\n",
    "                group coordinator) and offset commits are disabled.\n",
    "                Default: None\n",
    "            key_deserializer (Callable): Any callable that takes a\n",
    "                raw message key and returns a deserialized key.\n",
    "            value_deserializer (Callable, Optional): Any callable that takes a\n",
    "                raw message value and returns a deserialized value.\n",
    "            fetch_min_bytes (int): Minimum amount of data the server should\n",
    "                return for a fetch request, otherwise wait up to\n",
    "                `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
    "            fetch_max_bytes (int): The maximum amount of data the server should\n",
    "                return for a fetch request. This is not an absolute maximum, if\n",
    "                the first message in the first non-empty partition of the fetch\n",
    "                is larger than this value, the message will still be returned\n",
    "                to ensure that the consumer can make progress. NOTE: consumer\n",
    "                performs fetches to multiple brokers in parallel so memory\n",
    "                usage will depend on the number of brokers containing\n",
    "                partitions for the topic.\n",
    "                Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
    "            fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
    "                the server will block before answering the fetch request if\n",
    "                there isn't sufficient data to immediately satisfy the\n",
    "                requirement given by fetch_min_bytes. Default: 500.\n",
    "            max_partition_fetch_bytes (int): The maximum amount of data\n",
    "                per-partition the server will return. The maximum total memory\n",
    "                used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
    "                This size must be at least as large as the maximum message size\n",
    "                the server allows or else it is possible for the producer to\n",
    "                send messages larger than the consumer can fetch. If that\n",
    "                happens, the consumer can get stuck trying to fetch a large\n",
    "                message on a certain partition. Default: 1048576.\n",
    "            max_poll_records (int): The maximum number of records returned in a\n",
    "                single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
    "            auto_offset_reset (str): A policy for resetting offsets on\n",
    "                :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
    "                available message, ``latest`` will move to the most recent, and\n",
    "                ``none`` will raise an exception so you can handle this case.\n",
    "                Default: ``latest``.\n",
    "            enable_auto_commit (bool): If true the consumer's offset will be\n",
    "                periodically committed in the background. Default: True.\n",
    "            auto_commit_interval_ms (int): milliseconds between automatic\n",
    "                offset commits, if enable_auto_commit is True. Default: 5000.\n",
    "            check_crcs (bool): Automatically check the CRC32 of the records\n",
    "                consumed. This ensures no on-the-wire or on-disk corruption to\n",
    "                the messages occurred. This check adds some overhead, so it may\n",
    "                be disabled in cases seeking extreme performance. Default: True\n",
    "            partition_assignment_strategy (list): List of objects to use to\n",
    "                distribute partition ownership amongst consumer instances when\n",
    "                group management is used. This preference is implicit in the order\n",
    "                of the strategies in the list. When assignment strategy changes:\n",
    "                to support a change to the assignment strategy, new versions must\n",
    "                enable support both for the old assignment strategy and the new\n",
    "                one. The coordinator will choose the old assignment strategy until\n",
    "                all members have been updated. Then it will choose the new\n",
    "                strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
    "            max_poll_interval_ms (int): Maximum allowed time between calls to\n",
    "                consume messages (e.g., :meth:`.getmany`). If this interval\n",
    "                is exceeded the consumer is considered failed and the group will\n",
    "                rebalance in order to reassign the partitions to another consumer\n",
    "                group member. If API methods block waiting for messages, that time\n",
    "                does not count against this timeout. See `KIP-62`_ for more\n",
    "                information. Default 300000\n",
    "            rebalance_timeout_ms (int): The maximum time server will wait for this\n",
    "                consumer to rejoin the group in a case of rebalance. In Java client\n",
    "                this behaviour is bound to `max.poll.interval.ms` configuration,\n",
    "                but as ``aiokafka`` will rejoin the group in the background, we\n",
    "                decouple this setting to allow finer tuning by users that use\n",
    "                :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
    "                to ``session_timeout_ms``\n",
    "            session_timeout_ms (int): Client group session and failure detection\n",
    "                timeout. The consumer sends periodic heartbeats\n",
    "                (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
    "                If no hearts are received by the broker for a group member within\n",
    "                the session timeout, the broker will remove the consumer from the\n",
    "                group and trigger a rebalance. The allowed range is configured with\n",
    "                the **broker** configuration properties\n",
    "                `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
    "                Default: 10000\n",
    "            heartbeat_interval_ms (int): The expected time in milliseconds\n",
    "                between heartbeats to the consumer coordinator when using\n",
    "                Kafka's group management feature. Heartbeats are used to ensure\n",
    "                that the consumer's session stays active and to facilitate\n",
    "                rebalancing when new consumers join or leave the group. The\n",
    "                value must be set lower than `session_timeout_ms`, but typically\n",
    "                should be set no higher than 1/3 of that value. It can be\n",
    "                adjusted even lower to control the expected time for normal\n",
    "                rebalances. Default: 3000\n",
    "            consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
    "                routine. Mostly defines how fast the system will see rebalance and\n",
    "                request new data for new partitions. Default: 200\n",
    "            exclude_internal_topics (bool): Whether records from internal topics\n",
    "                (such as offsets) should be exposed to the consumer. If set to True\n",
    "                the only way to receive records from an internal topic is\n",
    "                subscribing to it. Requires 0.10+ Default: True\n",
    "            isolation_level (str): Controls how to read messages written\n",
    "                transactionally.\n",
    "\n",
    "                If set to ``read_committed``, :meth:`.getmany` will only return\n",
    "                transactional messages which have been committed.\n",
    "                If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
    "                return all messages, even transactional messages which have been\n",
    "                aborted.\n",
    "\n",
    "                Non-transactional messages will be returned unconditionally in\n",
    "                either mode.\n",
    "\n",
    "                Messages will always be returned in offset order. Hence, in\n",
    "                `read_committed` mode, :meth:`.getmany` will only return\n",
    "                messages up to the last stable offset (LSO), which is the one less\n",
    "                than the offset of the first open transaction. In particular any\n",
    "                messages appearing after messages belonging to ongoing transactions\n",
    "                will be withheld until the relevant transaction has been completed.\n",
    "                As a result, `read_committed` consumers will not be able to read up\n",
    "                to the high watermark when there are in flight transactions.\n",
    "                Further, when in `read_committed` the seek_to_end method will\n",
    "                return the LSO. See method docs below. Default: ``read_uncommitted``\n",
    "            sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "\n",
    "        # this is neede for documentation generation\n",
    "        self._title = title if title is not None else \"\"\n",
    "        self._description = description if description is not None else \"\"\n",
    "        self._version = version if version is not None else \"\"\n",
    "        if contact is not None:\n",
    "            self._contact_info = _get_contact_info(**contact)\n",
    "        else:\n",
    "            self._contact_info = _get_contact_info()\n",
    "\n",
    "        self._kafka_service_info = KafkaServiceInfo(\n",
    "            title=self._title,\n",
    "            version=self._version,\n",
    "            description=self._description,\n",
    "            contact=self._contact_info,\n",
    "        )\n",
    "        self._kafka_brokers = _get_kafka_brokers(kafka_brokers)\n",
    "\n",
    "        self._root_path = Path(\".\") if root_path is None else Path(root_path)\n",
    "\n",
    "        self._asyncapi_path = self._root_path / \"asyncapi\"\n",
    "        (self._asyncapi_path / \"docs\").mkdir(exist_ok=True, parents=True)\n",
    "        (self._asyncapi_path / \"spec\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # this is used as default parameters for creating AIOProducer and AIOConsumer objects\n",
    "        self._kafka_config = _get_kafka_config(**kwargs)\n",
    "\n",
    "        self.skip_docs = skip_docs\n",
    "\n",
    "        #\n",
    "        self._consumers_store: Dict[str, Tuple[ConsumeCallable, Dict[str, Any]]] = {}\n",
    "\n",
    "        self._producers_store: Dict[  # type: ignore\n",
    "            str, Tuple[ProduceCallable, AIOKafkaProducer, Dict[str, Any]]\n",
    "        ] = {}\n",
    "\n",
    "        self._producers_list: List[  # type: ignore\n",
    "            Union[AIOKafkaProducer, AIOKafkaProducerManager]\n",
    "        ] = []\n",
    "\n",
    "        # background tasks\n",
    "        self._scheduled_bg_tasks: List[Callable[..., Coroutine[Any, Any, Any]]] = []\n",
    "        self._bg_task_group_generator: Optional[anyio.abc.TaskGroup] = None\n",
    "        self._bg_tasks_group: Optional[anyio.abc.TaskGroup] = None\n",
    "\n",
    "        # todo: use this for errrors\n",
    "        self._on_error_topic: Optional[str] = None\n",
    "\n",
    "        self._is_started: bool = False\n",
    "        self._is_shutting_down: bool = False\n",
    "        self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []\n",
    "        self._kafka_producer_tasks: List[asyncio.Task[Any]] = []\n",
    "        self.run = False\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        await self.startup()\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        await self.shutdown()\n",
    "        \n",
    "    async def startup(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def shutdown(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def consumes(\n",
    "        self,\n",
    "        topic: Optional[str] = None,\n",
    "        *,\n",
    "        prefix: str = \"on_\",\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> ConsumeCallable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def produces(  # type: ignore\n",
    "        self,\n",
    "        topic: Optional[str] = None,\n",
    "        *,\n",
    "        prefix: str = \"to_\",\n",
    "        producer: Optional[AIOKafkaProducer] = None,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> ProduceCallable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run_in_background(\n",
    "        self,\n",
    "    ) -> Callable[[], Any]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _populate_consumers(\n",
    "        self,\n",
    "        is_shutting_down_f: Callable[[], bool],\n",
    "    ) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _populate_producers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _populate_bg_tasks(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_docs(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_consumers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_producers(self) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _shutdown_bg_tasks(self) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894af799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_title': '',\n",
       " '_description': '',\n",
       " '_version': '',\n",
       " '_contact_info': ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n",
       " '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n",
       " '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='https://localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}),\n",
       " '_root_path': PosixPath('.'),\n",
       " '_asyncapi_path': PosixPath('asyncapi'),\n",
       " '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n",
       "  'auto_offset_reset': 'earliest',\n",
       "  'max_poll_records': 100},\n",
       " 'skip_docs': True,\n",
       " '_consumers_store': {},\n",
       " '_producers_store': {},\n",
       " '_producers_list': [],\n",
       " '_scheduled_bg_tasks': [],\n",
       " '_bg_task_group_generator': None,\n",
       " '_bg_tasks_group': None,\n",
       " '_on_error_topic': None,\n",
       " '_is_shutting_down': False,\n",
       " '_kafka_consumer_tasks': [],\n",
       " '_kafka_producer_tasks': [],\n",
       " 'run': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_app = FastKafka()\n",
    "kafka_app.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b99de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_title': '',\n",
       " '_description': '',\n",
       " '_version': '',\n",
       " '_contact_info': ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n",
       " '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n",
       " '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='https://localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}),\n",
       " '_root_path': PosixPath('.'),\n",
       " '_asyncapi_path': PosixPath('asyncapi'),\n",
       " '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n",
       "  'auto_offset_reset': 'earliest',\n",
       "  'max_poll_records': 100},\n",
       " 'skip_docs': True,\n",
       " '_consumers_store': {},\n",
       " '_producers_store': {},\n",
       " '_producers_list': [],\n",
       " '_scheduled_bg_tasks': [],\n",
       " '_bg_task_group_generator': None,\n",
       " '_bg_tasks_group': None,\n",
       " '_on_error_topic': None,\n",
       " '_is_shutting_down': False,\n",
       " '_kafka_consumer_tasks': [],\n",
       " '_kafka_producer_tasks': [],\n",
       " 'run': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_app = FastKafka(contact={\"name\":\"Davor\"})\n",
    "kafka_app.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_app():\n",
    "    root_path = \"/tmp/000_FastKafka\"\n",
    "    if Path(root_path).exists():\n",
    "        shutil.rmtree(root_path)\n",
    "\n",
    "    kafka_app = FastKafka(\n",
    "        kafka_brokers={\n",
    "            \"local\": {\n",
    "                \"url\": \"kafka\",\n",
    "                \"name\": \"development\",\n",
    "                \"description\": \"Local (dev) Kafka broker\",\n",
    "                \"port\": 9092,\n",
    "            }\n",
    "        },\n",
    "        root_path=root_path,\n",
    "        **kafka_config,\n",
    "    )\n",
    "\n",
    "    return kafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FastKafka>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = create_testing_app()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "@delegates(AIOKafkaConsumer) # type: ignore\n",
    "def consumes(\n",
    "    self: FastKafka,\n",
    "    topic: Optional[str] = None,\n",
    "    *,\n",
    "    prefix: str = \"on_\",\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Callable[[ConsumeCallable], ConsumeCallable]:\n",
    "    \"\"\"Decorator registering the callback called when a message is received in a topic.\n",
    "\n",
    "    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
    "\n",
    "    Args:\n",
    "        topic: Kafka topic that the consumer will subscribe to and execute the\n",
    "            decorated function when it receives a message from the topic,\n",
    "            default: None. If the topic is not specified, topic name will be\n",
    "            inferred from the decorated function name by stripping the defined prefix\n",
    "        prefix: Prefix stripped from the decorated function to define a topic name\n",
    "            if the topic argument is not passed, default: \"on_\". If the decorated\n",
    "            function name is not prefixed with the defined prefix and topic argument\n",
    "            is not passed, then this method will throw ValueError\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string (or list of\n",
    "            ``host[:port]`` strings) that the consumer should contact to bootstrap\n",
    "            initial cluster metadata.\n",
    "\n",
    "            This does not have to be the full node list.\n",
    "            It just needs to have at least one broker that will respond to a\n",
    "            Metadata API Request. Default port is 9092. If no servers are\n",
    "            specified, will default to ``localhost:9092``.\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client. Also\n",
    "            submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`\n",
    "            for logging with respect to consumer group administration. Default:\n",
    "            ``aiokafka-{version}``\n",
    "        group_id (str or None): name of the consumer group to join for dynamic\n",
    "            partition assignment (if enabled), and to use for fetching and\n",
    "            committing offsets. If None, auto-partition assignment (via\n",
    "            group coordinator) and offset commits are disabled.\n",
    "            Default: None\n",
    "        key_deserializer (Callable): Any callable that takes a\n",
    "            raw message key and returns a deserialized key.\n",
    "        value_deserializer (Callable, Optional): Any callable that takes a\n",
    "            raw message value and returns a deserialized value.\n",
    "        fetch_min_bytes (int): Minimum amount of data the server should\n",
    "            return for a fetch request, otherwise wait up to\n",
    "            `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
    "        fetch_max_bytes (int): The maximum amount of data the server should\n",
    "            return for a fetch request. This is not an absolute maximum, if\n",
    "            the first message in the first non-empty partition of the fetch\n",
    "            is larger than this value, the message will still be returned\n",
    "            to ensure that the consumer can make progress. NOTE: consumer\n",
    "            performs fetches to multiple brokers in parallel so memory\n",
    "            usage will depend on the number of brokers containing\n",
    "            partitions for the topic.\n",
    "            Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
    "        fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
    "            the server will block before answering the fetch request if\n",
    "            there isn't sufficient data to immediately satisfy the\n",
    "            requirement given by fetch_min_bytes. Default: 500.\n",
    "        max_partition_fetch_bytes (int): The maximum amount of data\n",
    "            per-partition the server will return. The maximum total memory\n",
    "            used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
    "            This size must be at least as large as the maximum message size\n",
    "            the server allows or else it is possible for the producer to\n",
    "            send messages larger than the consumer can fetch. If that\n",
    "            happens, the consumer can get stuck trying to fetch a large\n",
    "            message on a certain partition. Default: 1048576.\n",
    "        max_poll_records (int): The maximum number of records returned in a\n",
    "            single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
    "        request_timeout_ms (int): Client request timeout in milliseconds.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        auto_offset_reset (str): A policy for resetting offsets on\n",
    "            :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
    "            available message, ``latest`` will move to the most recent, and\n",
    "            ``none`` will raise an exception so you can handle this case.\n",
    "            Default: ``latest``.\n",
    "        enable_auto_commit (bool): If true the consumer's offset will be\n",
    "            periodically committed in the background. Default: True.\n",
    "        auto_commit_interval_ms (int): milliseconds between automatic\n",
    "            offset commits, if enable_auto_commit is True. Default: 5000.\n",
    "        check_crcs (bool): Automatically check the CRC32 of the records\n",
    "            consumed. This ensures no on-the-wire or on-disk corruption to\n",
    "            the messages occurred. This check adds some overhead, so it may\n",
    "            be disabled in cases seeking extreme performance. Default: True\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        partition_assignment_strategy (list): List of objects to use to\n",
    "            distribute partition ownership amongst consumer instances when\n",
    "            group management is used. This preference is implicit in the order\n",
    "            of the strategies in the list. When assignment strategy changes:\n",
    "            to support a change to the assignment strategy, new versions must\n",
    "            enable support both for the old assignment strategy and the new\n",
    "            one. The coordinator will choose the old assignment strategy until\n",
    "            all members have been updated. Then it will choose the new\n",
    "            strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
    "        max_poll_interval_ms (int): Maximum allowed time between calls to\n",
    "            consume messages (e.g., :meth:`.getmany`). If this interval\n",
    "            is exceeded the consumer is considered failed and the group will\n",
    "            rebalance in order to reassign the partitions to another consumer\n",
    "            group member. If API methods block waiting for messages, that time\n",
    "            does not count against this timeout. See `KIP-62`_ for more\n",
    "            information. Default 300000\n",
    "        rebalance_timeout_ms (int): The maximum time server will wait for this\n",
    "            consumer to rejoin the group in a case of rebalance. In Java client\n",
    "            this behaviour is bound to `max.poll.interval.ms` configuration,\n",
    "            but as ``aiokafka`` will rejoin the group in the background, we\n",
    "            decouple this setting to allow finer tuning by users that use\n",
    "            :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
    "            to ``session_timeout_ms``\n",
    "        session_timeout_ms (int): Client group session and failure detection\n",
    "            timeout. The consumer sends periodic heartbeats\n",
    "            (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
    "            If no hearts are received by the broker for a group member within\n",
    "            the session timeout, the broker will remove the consumer from the\n",
    "            group and trigger a rebalance. The allowed range is configured with\n",
    "            the **broker** configuration properties\n",
    "            `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
    "            Default: 10000\n",
    "        heartbeat_interval_ms (int): The expected time in milliseconds\n",
    "            between heartbeats to the consumer coordinator when using\n",
    "            Kafka's group management feature. Heartbeats are used to ensure\n",
    "            that the consumer's session stays active and to facilitate\n",
    "            rebalancing when new consumers join or leave the group. The\n",
    "            value must be set lower than `session_timeout_ms`, but typically\n",
    "            should be set no higher than 1/3 of that value. It can be\n",
    "            adjusted even lower to control the expected time for normal\n",
    "            rebalances. Default: 3000\n",
    "        consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
    "            routine. Mostly defines how fast the system will see rebalance and\n",
    "            request new data for new partitions. Default: 200\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            :class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more information see\n",
    "            :ref:`ssl_auth`. Default: None.\n",
    "        exclude_internal_topics (bool): Whether records from internal topics\n",
    "            (such as offsets) should be exposed to the consumer. If set to True\n",
    "            the only way to receive records from an internal topic is\n",
    "            subscribing to it. Requires 0.10+ Default: True\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying `None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        isolation_level (str): Controls how to read messages written\n",
    "            transactionally.\n",
    "\n",
    "            If set to ``read_committed``, :meth:`.getmany` will only return\n",
    "            transactional messages which have been committed.\n",
    "            If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
    "            return all messages, even transactional messages which have been\n",
    "            aborted.\n",
    "\n",
    "            Non-transactional messages will be returned unconditionally in\n",
    "            either mode.\n",
    "\n",
    "            Messages will always be returned in offset order. Hence, in\n",
    "            `read_committed` mode, :meth:`.getmany` will only return\n",
    "            messages up to the last stable offset (LSO), which is the one less\n",
    "            than the offset of the first open transaction. In particular any\n",
    "            messages appearing after messages belonging to ongoing transactions\n",
    "            will be withheld until the relevant transaction has been completed.\n",
    "            As a result, `read_committed` consumers will not be able to read up\n",
    "            to the high watermark when there are in flight transactions.\n",
    "            Further, when in `read_committed` the seek_to_end method will\n",
    "            return the LSO. See method docs below. Default: ``read_uncommitted``\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:\n",
    "            ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: None\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: None\n",
    "        sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
    "            Default: None\n",
    "\n",
    "    Returns:\n",
    "        A function returning the same function\n",
    "\n",
    "    Throws:\n",
    "        ValueError\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        on_topic: ConsumeCallable,\n",
    "        topic: Optional[str] = topic,\n",
    "        kwargs: Dict[str, Any] = kwargs,\n",
    "    ) -> ConsumeCallable:\n",
    "        topic_resolved: str = (\n",
    "            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n",
    "            if topic is None\n",
    "            else topic\n",
    "        )\n",
    "\n",
    "        self._consumers_store[topic_resolved] = (on_topic, kwargs)\n",
    "\n",
    "        return on_topic\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature(FastKafka.consumes).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(FastKafka.consumes, AIOKafkaConsumer).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "# Basic check\n",
    "@app.consumes()\n",
    "def on_my_topic_1(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"my_topic_1\"] == (on_my_topic_1, {}), app._consumers_store\n",
    "\n",
    "# Check topic setting\n",
    "@app.consumes(topic=\"test_topic_2\")\n",
    "def some_func_name(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic_2\"] == (\n",
    "    some_func_name,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "# Check prefix change\n",
    "@app.consumes(prefix=\"for_\")\n",
    "def for_test_topic_3(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic_3\"] == (\n",
    "    for_test_topic_3,\n",
    "    {},\n",
    "), app._consumers_store\n",
    "\n",
    "# Check passing of kwargs\n",
    "kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "\n",
    "@app.consumes(topic=\"test_topic\", **kwargs)\n",
    "def for_test_kwargs(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._consumers_store[\"test_topic\"] == (\n",
    "    for_test_kwargs,\n",
    "    kwargs,\n",
    "), app._consumers_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002589f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _to_json_utf8(o: Any) -> bytes:\n",
    "    \"\"\"Converts to JSON and then encodes with UTF-8\"\"\"\n",
    "    if hasattr(o, \"json\"):\n",
    "        return o.json().encode(\"utf-8\")  # type: ignore\n",
    "    else:\n",
    "        return json.dumps(o).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _to_json_utf8({\"a\": 1, \"b\": [2, 3]}) == b'{\"a\": 1, \"b\": [2, 3]}'\n",
    "\n",
    "\n",
    "class A(BaseModel):\n",
    "    name: str = Field()\n",
    "    age: int\n",
    "\n",
    "\n",
    "assert _to_json_utf8(A(name=\"Davor\", age=12)) == b'{\"name\": \"Davor\", \"age\": 12}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def produce_decorator(\n",
    "    self: FastKafka, func: ProduceCallable, topic: str\n",
    ") -> ProduceCallable:\n",
    "    @functools.wraps(func)\n",
    "    async def _produce_async(*args: List[Any], **kwargs: Dict[str, Any]) -> BaseModel:\n",
    "        f: Callable[..., Awaitable[BaseModel]] = func  # type: ignore\n",
    "        return_val = await f(*args, **kwargs)\n",
    "        _, producer, _ = self._producers_store[topic]\n",
    "        fut = await producer.send(topic, _to_json_utf8(return_val))\n",
    "        msg = await fut\n",
    "        return return_val\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def _produce_sync(*args: List[Any], **kwargs: Dict[str, Any]) -> BaseModel:\n",
    "        f: Callable[..., BaseModel] = func  # type: ignore\n",
    "        return_val = f(*args, **kwargs)\n",
    "        _, producer, _ = self._producers_store[topic]\n",
    "        producer.send(topic, _to_json_utf8(return_val))\n",
    "        return return_val\n",
    "\n",
    "    return _produce_async if iscoroutinefunction(func) else _produce_sync  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de8051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "async def test_me(is_async: bool):\n",
    "    with mock_AIOKafkaProducer_send() as send_mock:\n",
    "        topic = \"test_topic\"\n",
    "\n",
    "        class MockMsg(BaseModel):\n",
    "            name: str = \"Micky Mouse\"\n",
    "            id: int = 123\n",
    "\n",
    "        if is_async:\n",
    "\n",
    "            async def func(mock_msg: MockMsg) -> MockMsg:\n",
    "                return mock_msg\n",
    "\n",
    "        else:\n",
    "\n",
    "            def func(mock_msg: MockMsg) -> MockMsg:\n",
    "                return mock_msg\n",
    "\n",
    "        producer = AIOKafkaProducer(bootstrap_servers=kafka_config[\"bootstrap_servers\"])\n",
    "        if not is_async:\n",
    "            producer = AIOKafkaProducerManager(producer)\n",
    "\n",
    "        await producer.start()\n",
    "        try:\n",
    "            app = unittest.mock.Mock()\n",
    "            app._producers_store = {topic: (func, producer, {})}\n",
    "\n",
    "            test_func = produce_decorator(app, func, topic)\n",
    "            assert iscoroutinefunction(test_func) == is_async\n",
    "\n",
    "            mock_msg = MockMsg()\n",
    "            if not is_async:\n",
    "                value = test_func(mock_msg)\n",
    "                await asyncio.sleep(1)\n",
    "            else:\n",
    "                value = await test_func(mock_msg)\n",
    "\n",
    "            send_mock.assert_called_once_with(topic, mock_msg.json().encode(\"utf-8\"))\n",
    "            assert value == mock_msg\n",
    "\n",
    "        finally:\n",
    "            await producer.stop()\n",
    "\n",
    "\n",
    "for is_async in [True, False]:\n",
    "    await test_me(is_async)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "@delegates(AIOKafkaProducer) # type: ignore\n",
    "def produces(\n",
    "    self: FastKafka,\n",
    "    topic: Optional[str] = None,\n",
    "    *,\n",
    "    prefix: str = \"to_\",\n",
    "    producer: Optional[AIOKafkaProducer] = None,\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> Callable[[ProduceCallable], ProduceCallable]:\n",
    "    \"\"\"Decorator registering the callback called when delivery report for a produced message is received\n",
    "\n",
    "    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n",
    "\n",
    "    Args:\n",
    "        topic: Kafka topic that the producer will send returned values from\n",
    "            the decorated function to, default: None- If the topic is not\n",
    "            specified, topic name will be inferred from the decorated function\n",
    "            name by stripping the defined prefix.\n",
    "        prefix: Prefix stripped from the decorated function to define a topic\n",
    "            name if the topic argument is not passed, default: \"to_\". If the\n",
    "            decorated function name is not prefixed with the defined prefix\n",
    "            and topic argument is not passed, then this method will throw ValueError\n",
    "        producer: optional AIOKafkaProducer object used to produce messages\n",
    "        bootstrap_servers (str, list(str)): a ``host[:port]`` string or list of\n",
    "            ``host[:port]`` strings that the producer should contact to\n",
    "            bootstrap initial cluster metadata. This does not have to be the\n",
    "            full node list.  It just needs to have at least one broker that will\n",
    "            respond to a Metadata API Request. Default port is 9092. If no\n",
    "            servers are specified, will default to ``localhost:9092``.\n",
    "        client_id (str): a name for this client. This string is passed in\n",
    "            each request to servers and can be used to identify specific\n",
    "            server-side log entries that correspond to this client.\n",
    "            Default: ``aiokafka-producer-#`` (appended with a unique number\n",
    "            per instance)\n",
    "        key_serializer (Callable): used to convert user-supplied keys to bytes\n",
    "            If not :data:`None`, called as ``f(key),`` should return\n",
    "            :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        value_serializer (Callable): used to convert user-supplied message\n",
    "            values to :class:`bytes`. If not :data:`None`, called as\n",
    "            ``f(value)``, should return :class:`bytes`.\n",
    "            Default: :data:`None`.\n",
    "        acks (Any): one of ``0``, ``1``, ``all``. The number of acknowledgments\n",
    "            the producer requires the leader to have received before considering a\n",
    "            request complete. This controls the durability of records that are\n",
    "            sent. The following settings are common:\n",
    "\n",
    "            * ``0``: Producer will not wait for any acknowledgment from the server\n",
    "              at all. The message will immediately be added to the socket\n",
    "              buffer and considered sent. No guarantee can be made that the\n",
    "              server has received the record in this case, and the retries\n",
    "              configuration will not take effect (as the client won't\n",
    "              generally know of any failures). The offset given back for each\n",
    "              record will always be set to -1.\n",
    "            * ``1``: The broker leader will write the record to its local log but\n",
    "              will respond without awaiting full acknowledgement from all\n",
    "              followers. In this case should the leader fail immediately\n",
    "              after acknowledging the record but before the followers have\n",
    "              replicated it then the record will be lost.\n",
    "            * ``all``: The broker leader will wait for the full set of in-sync\n",
    "              replicas to acknowledge the record. This guarantees that the\n",
    "              record will not be lost as long as at least one in-sync replica\n",
    "              remains alive. This is the strongest available guarantee.\n",
    "\n",
    "            If unset, defaults to ``acks=1``. If `enable_idempotence` is\n",
    "            :data:`True` defaults to ``acks=all``\n",
    "        compression_type (str): The compression type for all data generated by\n",
    "            the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``\n",
    "            or :data:`None`.\n",
    "            Compression is of full batches of data, so the efficacy of batching\n",
    "            will also impact the compression ratio (more batching means better\n",
    "            compression). Default: :data:`None`.\n",
    "        max_batch_size (int): Maximum size of buffered data per partition.\n",
    "            After this amount :meth:`send` coroutine will block until batch is\n",
    "            drained.\n",
    "            Default: 16384\n",
    "        linger_ms (int): The producer groups together any records that arrive\n",
    "            in between request transmissions into a single batched request.\n",
    "            Normally this occurs only under load when records arrive faster\n",
    "            than they can be sent out. However in some circumstances the client\n",
    "            may want to reduce the number of requests even under moderate load.\n",
    "            This setting accomplishes this by adding a small amount of\n",
    "            artificial delay; that is, if first request is processed faster,\n",
    "            than `linger_ms`, producer will wait ``linger_ms - process_time``.\n",
    "            Default: 0 (i.e. no delay).\n",
    "        partitioner (Callable): Callable used to determine which partition\n",
    "            each message is assigned to. Called (after key serialization):\n",
    "            ``partitioner(key_bytes, all_partitions, available_partitions)``.\n",
    "            The default partitioner implementation hashes each non-None key\n",
    "            using the same murmur2 algorithm as the Java client so that\n",
    "            messages with the same key are assigned to the same partition.\n",
    "            When a key is :data:`None`, the message is delivered to a random partition\n",
    "            (filtered to partitions with available leaders only, if possible).\n",
    "        max_request_size (int): The maximum size of a request. This is also\n",
    "            effectively a cap on the maximum record size. Note that the server\n",
    "            has its own cap on record size which may be different from this.\n",
    "            This setting will limit the number of record batches the producer\n",
    "            will send in a single request to avoid sending huge requests.\n",
    "            Default: 1048576.\n",
    "        metadata_max_age_ms (int): The period of time in milliseconds after\n",
    "            which we force a refresh of metadata even if we haven't seen any\n",
    "            partition leadership changes to proactively discover any new\n",
    "            brokers or partitions. Default: 300000\n",
    "        request_timeout_ms (int): Produce request timeout in milliseconds.\n",
    "            As it's sent as part of\n",
    "            :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking\n",
    "            call), maximum waiting time can be up to ``2 *\n",
    "            request_timeout_ms``.\n",
    "            Default: 40000.\n",
    "        retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
    "            errors. Default: 100.\n",
    "        api_version (str): specify which kafka API version to use.\n",
    "            If set to ``auto``, will attempt to infer the broker version by\n",
    "            probing various APIs. Default: ``auto``\n",
    "        security_protocol (str): Protocol used to communicate with brokers.\n",
    "            Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
    "            Default: ``PLAINTEXT``.\n",
    "        ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
    "            for wrapping socket connections. Directly passed into asyncio's\n",
    "            :meth:`~asyncio.loop.create_connection`. For more\n",
    "            information see :ref:`ssl_auth`.\n",
    "            Default: :data:`None`\n",
    "        connections_max_idle_ms (int): Close idle connections after the number\n",
    "            of milliseconds specified by this config. Specifying :data:`None` will\n",
    "            disable idle checks. Default: 540000 (9 minutes).\n",
    "        enable_idempotence (bool): When set to :data:`True`, the producer will\n",
    "            ensure that exactly one copy of each message is written in the\n",
    "            stream. If :data:`False`, producer retries due to broker failures,\n",
    "            etc., may write duplicates of the retried message in the stream.\n",
    "            Note that enabling idempotence acks to set to ``all``. If it is not\n",
    "            explicitly set by the user it will be chosen. If incompatible\n",
    "            values are set, a :exc:`ValueError` will be thrown.\n",
    "            New in version 0.5.0.\n",
    "        sasl_mechanism (str): Authentication mechanism when security_protocol\n",
    "            is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values\n",
    "            are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
    "            ``OAUTHBEARER``.\n",
    "            Default: ``PLAIN``\n",
    "        sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
    "            Default: :data:`None`\n",
    "        sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):\n",
    "            OAuthBearer token provider instance. (See\n",
    "            :mod:`kafka.oauth.abstract`).\n",
    "            Default: :data:`None`\n",
    "\n",
    "    Returns:\n",
    "        A function returning the same function\n",
    "\n",
    "    Raises:\n",
    "        ValueError: when needed\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        on_topic: ProduceCallable,\n",
    "        topic: Optional[str] = topic,\n",
    "        kwargs: Dict[str, Any] = kwargs,\n",
    "    ) -> ProduceCallable:\n",
    "        topic_resolved: str = (\n",
    "            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n",
    "            if topic is None\n",
    "            else topic\n",
    "        )\n",
    "\n",
    "        self._producers_store[topic_resolved] = (on_topic, producer, kwargs)\n",
    "\n",
    "        return produce_decorator(self, on_topic, topic_resolved)\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28376379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature(FastKafka.produces).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(FastKafka.produces, AIOKafkaProducer).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_testing_app()\n",
    "\n",
    "# Basic check\n",
    "async def to_my_topic_1(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Must be done without sugar to keep the original function reference\n",
    "check_func = to_my_topic_1\n",
    "to_my_topic_1 = app.produces()(to_my_topic_1)\n",
    "\n",
    "assert app._producers_store[\"my_topic_1\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), f\"{app._producers_store}, {to_my_topic_1}\"\n",
    "\n",
    "# Check topic setting\n",
    "def some_func_name(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = some_func_name\n",
    "some_func_name = app.produces(topic=\"test_topic_2\")(some_func_name)\n",
    "\n",
    "assert app._producers_store[\"test_topic_2\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), app._producers_store\n",
    "\n",
    "# Check prefix change\n",
    "@app.produces(prefix=\"for_\")\n",
    "def for_test_topic_3(msg: BaseModel) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = for_test_topic_3\n",
    "some_func_name = app.produces(prefix=\"for_\")(for_test_topic_3)\n",
    "\n",
    "assert app._producers_store[\"test_topic_3\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    {},\n",
    "), app._producers_store\n",
    "\n",
    "# Check passing of kwargs\n",
    "kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "\n",
    "async def for_test_kwargs(msg: BaseModel):\n",
    "    pass\n",
    "\n",
    "\n",
    "check_func = for_test_kwargs\n",
    "for_test_kwargs = app.produces(topic=\"test_topic\", **kwargs)(for_test_kwargs)\n",
    "\n",
    "assert app._producers_store[\"test_topic\"] == (\n",
    "    check_func,\n",
    "    None,\n",
    "    kwargs,\n",
    "), app._producers_store\n",
    "\n",
    "# Check passing of custom Producer\n",
    "async def test_me():\n",
    "    kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n",
    "\n",
    "    async def for_test_producer(msg: BaseModel):\n",
    "        pass\n",
    "\n",
    "    check_func = for_test_producer\n",
    "    producer = AIOKafkaProducer()\n",
    "    for_test_producer = app.produces(\n",
    "        topic=\"test_producer\", producer=producer, **kwargs\n",
    "    )(for_test_producer)\n",
    "\n",
    "    assert app._producers_store[\"test_producer\"] == (\n",
    "        check_func,\n",
    "        producer,\n",
    "        kwargs,\n",
    "    ), app._producers_store\n",
    "\n",
    "\n",
    "await test_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4744bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def run_in_background(\n",
    "    self: FastKafka,\n",
    ") -> Callable[\n",
    "    [Callable[..., Coroutine[Any, Any, Any]]], Callable[..., Coroutine[Any, Any, Any]]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Decorator to schedule a task to be run in the background.\n",
    "\n",
    "    This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.\n",
    "\n",
    "    Returns:\n",
    "        Callable[None, None]: A decorator function that takes a background task as an input and stores it to be run in the backround.\n",
    "    \"\"\"\n",
    "\n",
    "    def _decorator(\n",
    "        bg_task: Callable[..., Coroutine[Any, Any, Any]]\n",
    "    ) -> Callable[..., Coroutine[Any, Any, Any]]:\n",
    "        \"\"\"\n",
    "        Store the background task.\n",
    "\n",
    "        Args:\n",
    "            bg_task (Callable[[], None]): The background task to be run asynchronously.\n",
    "\n",
    "        Returns:\n",
    "            Callable[[], None]: Original background task.\n",
    "        \"\"\"\n",
    "        self._scheduled_bg_tasks.append(bg_task)\n",
    "\n",
    "        return bg_task\n",
    "\n",
    "    return _decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16917ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the background job is getting registered\n",
    "\n",
    "app = create_testing_app()\n",
    "\n",
    "\n",
    "@app.run_in_background()\n",
    "async def async_background_job():\n",
    "    \"\"\"Async background job\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "assert app._scheduled_bg_tasks[0] == async_background_job, app._scheduled_bg_tasks[0]\n",
    "assert app._scheduled_bg_tasks.__len__() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInfo(BaseModel):\n",
    "    mobile: str = Field(..., example=\"+385987654321\")\n",
    "    name: str = Field(..., example=\"James Bond\")\n",
    "\n",
    "\n",
    "class MyMsgUrl(BaseModel):\n",
    "    info: MyInfo = Field(..., example=dict(mobile=\"+385987654321\", name=\"James Bond\"))\n",
    "    url: HttpUrl = Field(..., example=\"https://sis.gov.uk/agents/007\")\n",
    "\n",
    "\n",
    "class MyMsgEmail(BaseModel):\n",
    "    msg_url: MyMsgUrl = Field(\n",
    "        ...,\n",
    "        example=dict(\n",
    "            info=dict(mobile=\"+385987654321\", name=\"James Bond\"),\n",
    "            url=\"https://sis.gov.uk/agents/007\",\n",
    "        ),\n",
    "    )\n",
    "    email: EmailStr = Field(..., example=\"agent-007@sis.gov.uk\")\n",
    "\n",
    "\n",
    "def setup_testing_app():\n",
    "    app = create_testing_app()\n",
    "\n",
    "    @app.consumes(\"my_topic_1\")\n",
    "    def on_my_topic_one(msg: MyMsgUrl) -> None:\n",
    "        logger.debug(f\"on_my_topic_one(msg={msg},)\")\n",
    "\n",
    "    @app.consumes()\n",
    "    async def on_my_topic_2(msg: MyMsgEmail) -> None:\n",
    "        logger.debug(f\"on_my_topic_2(msg={msg},)\")\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "\n",
    "        @app.consumes()\n",
    "        def my_topic_3(msg: MyMsgEmail) -> None:\n",
    "            raise NotImplemented\n",
    "\n",
    "    @app.produces()\n",
    "    def to_my_topic_3(url: str) -> MyMsgUrl:\n",
    "        logger.debug(f\"on_my_topic_3(msg={url}\")\n",
    "        return MyMsgUrl(info=MyInfo(\"+3851987654321\", \"Sean Connery\"), url=url)\n",
    "\n",
    "    @app.produces()\n",
    "    async def to_my_topic_4(msg: MyMsgEmail) -> MyMsgEmail:\n",
    "        logger.debug(f\"on_my_topic_4(msg={msg}\")\n",
    "        return msg\n",
    "\n",
    "    @app.produces()\n",
    "    def to_my_topic_5(url: str) -> MyMsgUrl:\n",
    "        logger.debug(f\"on_my_topic_5(msg={url}\")\n",
    "        return MyMsgUrl(info=MyInfo(\"+3859123456789\", \"John Wayne\"), url=url)\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def long_bg_job():\n",
    "        await asyncio.sleep(100)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a945425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app._kafka_service_info=title='' version='' description='' contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')\n",
      "app._kafka_brokers=brokers={'local': KafkaBroker(url='kafka', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}\n"
     ]
    }
   ],
   "source": [
    "app = setup_testing_app()\n",
    "\n",
    "assert set(app._consumers_store.keys()) == set([\"my_topic_1\", \"my_topic_2\"])\n",
    "assert set(app._producers_store.keys()) == set(\n",
    "    [\"my_topic_3\", \"my_topic_4\", \"my_topic_5\"]\n",
    ")\n",
    "\n",
    "print(f\"app._kafka_service_info={app._kafka_service_info}\")\n",
    "print(f\"app._kafka_brokers={app._kafka_brokers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def filter_using_signature(f: Callable, **kwargs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    param_names = list(signature(f).parameters.keys())\n",
    "    return {k: v for k, v in kwargs.items() if k in param_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a: int, *, b: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert filter_using_signature(f, a=1, c=3) == {\"a\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ea338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def _populate_consumers(\n",
    "    self: FastKafka,\n",
    "    is_shutting_down_f: Callable[[], bool],\n",
    ") -> None:\n",
    "    default_config: Dict[str, Any] = filter_using_signature(\n",
    "        AIOKafkaConsumer, **self._kafka_config\n",
    "    )\n",
    "    self._kafka_consumer_tasks = [\n",
    "        asyncio.create_task(\n",
    "            aiokafka_consumer_loop(\n",
    "                topic=topic,\n",
    "                callback=consumer,\n",
    "                msg_type=signature(consumer).parameters[\"msg\"].annotation,\n",
    "                is_shutting_down_f=is_shutting_down_f,\n",
    "                **{**default_config, **override_config},\n",
    "            )\n",
    "        )\n",
    "        for topic, (consumer, override_config) in self._consumers_store.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _shutdown_consumers(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    if self._kafka_consumer_tasks:\n",
    "        await asyncio.wait(self._kafka_consumer_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092', 'group_id': 'tvrtko-fastkafka-kafka-1:9092_group', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092', 'group_id': 'tvrtko-fastkafka-kafka-1:9092_group', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Joined group 'tvrtko-fastkafka-kafka-1:9092_group' (generation 174) with member_id aiokafka-0.8.0-b6714042-a867-426d-ae4d-4f515b944ee3\n",
      "[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Joined group 'tvrtko-fastkafka-kafka-1:9092_group' (generation 175) with member_id aiokafka-0.8.0-7c566c91-7d10-4cc9-bf6a-8ee0205612f5\n",
      "[INFO] aiokafka.consumer.group_coordinator: Joined group 'tvrtko-fastkafka-kafka-1:9092_group' (generation 175) with member_id aiokafka-0.8.0-b6714042-a867-426d-ae4d-4f515b944ee3\n",
      "[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {'my_topic_1': 1} to {'my_topic_2': 1, 'my_topic_1': 1}. \n",
      "[INFO] aiokafka.consumer.group_coordinator: Successfully synced group tvrtko-fastkafka-kafka-1:9092_group with generation 175\n",
      "[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_2', partition=0)} for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Successfully synced group tvrtko-fastkafka-kafka-1:9092_group with generation 175\n",
      "[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_1', partition=0)} for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n"
     ]
    }
   ],
   "source": [
    "app = setup_testing_app()\n",
    "app._populate_consumers(is_shutting_down_f=true_after(1))\n",
    "assert len(app._kafka_consumer_tasks) == 2\n",
    "\n",
    "await app._shutdown_consumers()\n",
    "\n",
    "assert all([t.done() for t in app._kafka_consumer_tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ee2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# TODO: Add passing of vars\n",
    "async def _create_producer(  # type: ignore\n",
    "    *,\n",
    "    callback: ProduceCallable,\n",
    "    producer: Optional[AIOKafkaProducer],\n",
    "    default_config: Dict[str, Any],\n",
    "    override_config: Dict[str, Any],\n",
    "    producers_list: List[Union[AIOKafkaProducer, AIOKafkaProducerManager]],\n",
    ") -> Union[AIOKafkaProducer, AIOKafkaProducerManager]:\n",
    "    \"\"\"Creates a producer\n",
    "\n",
    "    Args:\n",
    "        callback: A callback function that is called when the producer is ready.\n",
    "        producer: An existing producer to use.\n",
    "        default_config: A dictionary of default configuration values.\n",
    "        override_config: A dictionary of configuration values to override.\n",
    "        producers_list: A list of producers to add the new producer to.\n",
    "\n",
    "    Returns:\n",
    "        A producer.\n",
    "    \"\"\"\n",
    "\n",
    "    if producer is None:\n",
    "        config = {\n",
    "            **filter_using_signature(AIOKafkaProducer, **default_config),\n",
    "            **override_config,\n",
    "        }\n",
    "        producer = AIOKafkaProducer(**config)\n",
    "        logger.info(\n",
    "            f\"_create_producer() : created producer using the config: '{sanitize_kafka_config(**config)}'\"\n",
    "        )\n",
    "\n",
    "    if not iscoroutinefunction(callback):\n",
    "        producer = AIOKafkaProducerManager(producer)\n",
    "\n",
    "    await producer.start()\n",
    "\n",
    "    producers_list.append(producer)\n",
    "\n",
    "    return producer\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _populate_producers(self: FastKafka) -> None:\n",
    "    \"\"\"Populates the producers for the FastKafka instance.\n",
    "\n",
    "    Args:\n",
    "        self: The FastKafka instance.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    default_config: Dict[str, Any] = self._kafka_config\n",
    "    self._producers_list = []\n",
    "    self._producers_store = {\n",
    "        topic: (\n",
    "            callback,\n",
    "            await _create_producer(\n",
    "                callback=callback,\n",
    "                producer=producer,\n",
    "                default_config=default_config,\n",
    "                override_config=override_config,\n",
    "                producers_list=self._producers_list,\n",
    "            ),\n",
    "            override_config,\n",
    "        )\n",
    "        for topic, (\n",
    "            callback,\n",
    "            producer,\n",
    "            override_config,\n",
    "        ) in self._producers_store.items()\n",
    "    }\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _shutdown_producers(self: FastKafka) -> None:\n",
    "    [await producer.stop() for producer in self._producers_list[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager>,\n",
       " <aiokafka.producer.producer.AIOKafkaProducer>,\n",
       " <fastkafka._components.aiokafka_producer_manager.AIOKafkaProducerManager>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = setup_testing_app()\n",
    "await app._populate_producers()\n",
    "await app._shutdown_producers()\n",
    "assert len(app._producers_list) == 3\n",
    "\n",
    "app._producers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _populate_bg_tasks(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    self._bg_task_group_generator = anyio.create_task_group()\n",
    "    self._bg_tasks_group = await self._bg_task_group_generator.__aenter__()\n",
    "    for task in self._scheduled_bg_tasks:\n",
    "        self._bg_tasks_group.start_soon(task)\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def _shutdown_bg_tasks(\n",
    "    self: FastKafka,\n",
    ") -> None:\n",
    "    self._bg_tasks_group.cancel_scope.cancel()  # type: ignore\n",
    "    await self._bg_task_group_generator.__aexit__(None, None, None)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c687d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = setup_testing_app()\n",
    "await app._populate_bg_tasks()\n",
    "assert len(app._scheduled_bg_tasks) == 1\n",
    "assert app._bg_task_group_generator is not None\n",
    "assert app._bg_tasks_group is not None\n",
    "\n",
    "await app._shutdown_bg_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880411a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def create_docs(self: FastKafka) -> None:\n",
    "    export_async_spec(\n",
    "        consumers={\n",
    "            topic: callback for topic, (callback, _) in self._consumers_store.items()\n",
    "        },\n",
    "        producers={\n",
    "            topic: callback for topic, (callback, _, _) in self._producers_store.items()\n",
    "        },\n",
    "        kafka_brokers=self._kafka_brokers,\n",
    "        kafka_service_info=self._kafka_service_info,\n",
    "        asyncapi_path=self._asyncapi_path,\n",
    "        skip_docs=self.skip_docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.asyncapi: Old async specifications at '/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml' does not exist.\n",
      "[INFO] fastkafka._components.asyncapi: New async specifications generated at: '/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml'\n"
     ]
    }
   ],
   "source": [
    "app = setup_testing_app()\n",
    "app.create_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def startup(self: FastKafka) -> None:\n",
    "\n",
    "    self._is_shutting_down = False\n",
    "    \n",
    "    def is_shutting_down_f(self: FastKafka = self) -> bool:\n",
    "        return self._is_shutting_down\n",
    "\n",
    "    self.create_docs()\n",
    "    await self._populate_producers()\n",
    "    self._populate_consumers(is_shutting_down_f)\n",
    "    await self._populate_bg_tasks()\n",
    "\n",
    "    self._is_started = True\n",
    "\n",
    "@patch  # type: ignore\n",
    "async def shutdown(self: FastKafka) -> None:\n",
    "    self._is_shutting_down = True\n",
    "\n",
    "    await self._shutdown_bg_tasks()\n",
    "    await self._shutdown_consumers()\n",
    "    await self._shutdown_producers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@asynccontextmanager\n",
    "async def start_test_app():\n",
    "    app = setup_testing_app()\n",
    "    try:\n",
    "        await app.startup()\n",
    "        yield app\n",
    "    finally:\n",
    "        await app.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec33cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = \"\"\"asyncapi: 2.5.0\n",
    "channels:\n",
    "  my_topic_1:\n",
    "    subscribe:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "  my_topic_2:\n",
    "    subscribe:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgEmail'\n",
    "  my_topic_3:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "  my_topic_4:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgEmail'\n",
    "  my_topic_5:\n",
    "    publish:\n",
    "      message:\n",
    "        $ref: '#/components/messages/MyMsgUrl'\n",
    "components:\n",
    "  messages:\n",
    "    MyMsgEmail:\n",
    "      payload:\n",
    "        example:\n",
    "          email: agent-007@sis.gov.uk\n",
    "          msg_url:\n",
    "            info:\n",
    "              mobile: '+385987654321'\n",
    "              name: James Bond\n",
    "            url: https://sis.gov.uk/agents/007\n",
    "        properties:\n",
    "          email:\n",
    "            example: agent-007@sis.gov.uk\n",
    "            format: email\n",
    "            title: Email\n",
    "            type: string\n",
    "          msg_url:\n",
    "            allOf:\n",
    "            - $ref: '#/components/messages/MyMsgUrl'\n",
    "            example:\n",
    "              info:\n",
    "                mobile: '+385987654321'\n",
    "                name: James Bond\n",
    "              url: https://sis.gov.uk/agents/007\n",
    "            title: Msg Url\n",
    "        required:\n",
    "        - msg_url\n",
    "        - email\n",
    "        title: MyMsgEmail\n",
    "        type: object\n",
    "    MyMsgUrl:\n",
    "      payload:\n",
    "        example:\n",
    "          info:\n",
    "            mobile: '+385987654321'\n",
    "            name: James Bond\n",
    "          url: https://sis.gov.uk/agents/007\n",
    "        properties:\n",
    "          info:\n",
    "            allOf:\n",
    "            - $ref: '#/components/schemas/MyInfo'\n",
    "            example:\n",
    "              mobile: '+385987654321'\n",
    "              name: James Bond\n",
    "            title: Info\n",
    "          url:\n",
    "            example: https://sis.gov.uk/agents/007\n",
    "            format: uri\n",
    "            maxLength: 2083\n",
    "            minLength: 1\n",
    "            title: Url\n",
    "            type: string\n",
    "        required:\n",
    "        - info\n",
    "        - url\n",
    "        title: MyMsgUrl\n",
    "        type: object\n",
    "  schemas:\n",
    "    MyInfo:\n",
    "      payload:\n",
    "        properties:\n",
    "          mobile:\n",
    "            example: '+385987654321'\n",
    "            title: Mobile\n",
    "            type: string\n",
    "          name:\n",
    "            example: James Bond\n",
    "            title: Name\n",
    "            type: string\n",
    "        required:\n",
    "        - mobile\n",
    "        - name\n",
    "        title: MyInfo\n",
    "        type: object\n",
    "  securitySchemes: {}\n",
    "info:\n",
    "  contact:\n",
    "    email: noreply@gmail.com\n",
    "    name: Author\n",
    "    url: https://www.google.com\n",
    "  description: ''\n",
    "  title: ''\n",
    "  version: ''\n",
    "servers:\n",
    "  local:\n",
    "    description: Local (dev) Kafka broker\n",
    "    protocol: kafka\n",
    "    url: kafka\n",
    "    variables:\n",
    "      port:\n",
    "        default: '9092'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.asyncapi: Old async specifications at '/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml' does not exist.\n",
      "[INFO] fastkafka._components.asyncapi: New async specifications generated at: '/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092', 'group_id': 'tvrtko-fastkafka-kafka-1:9092_group', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092', 'group_id': 'tvrtko-fastkafka-kafka-1:9092_group', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})\n",
      "[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Joined group 'tvrtko-fastkafka-kafka-1:9092_group' (generation 177) with member_id aiokafka-0.8.0-0ec78673-c1d6-4001-8f16-129c8babc255\n",
      "[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[INFO] aiokafka.consumer.group_coordinator: Successfully synced group tvrtko-fastkafka-kafka-1:9092_group with generation 177\n",
      "[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_2', partition=0)} for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[INFO] aiokafka.consumer.group_coordinator: Joined group 'tvrtko-fastkafka-kafka-1:9092_group' (generation 179) with member_id aiokafka-0.8.0-68a304fd-e8b2-4101-94e1-e2e5d3deae9e\n",
      "[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] aiokafka.consumer.group_coordinator: Successfully synced group tvrtko-fastkafka-kafka-1:9092_group with generation 179\n",
      "[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_1', partition=0)} for group tvrtko-fastkafka-kafka-1:9092_group\n",
      "[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "[INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "d1, d2 = None, None\n",
    "\n",
    "\n",
    "async def test_me():\n",
    "    global d1\n",
    "    global d2\n",
    "    async with start_test_app() as app:\n",
    "        with open(\"/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml\") as specs:\n",
    "            d1 = yaml.safe_load(specs)\n",
    "            d2 = yaml.safe_load(expected)\n",
    "            assert d1 == d2, f\"{d1} != {d2}\"\n",
    "\n",
    "\n",
    "asyncio.run(test_me())\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cfce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'tvrtko-fastkafka-kafka-1:9092'}'\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n",
      "[INFO] fastkafka._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n"
     ]
    }
   ],
   "source": [
    "# don't wait for specs to be generated (takes 10 sec or so)\n",
    "with unittest.mock.patch(\"__main__.export_async_spec\"):\n",
    "\n",
    "    # mock up send method of AIOKafkaProducer\n",
    "    with mock_AIOKafkaProducer_send() as mock:\n",
    "\n",
    "        app = create_testing_app()\n",
    "\n",
    "        @app.produces()\n",
    "        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        @app.produces()\n",
    "        def to_my_test_topic_2(mobile: str, url: str) -> MyMsgUrl:\n",
    "            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n",
    "            return msg\n",
    "\n",
    "        try:\n",
    "            await app.startup()\n",
    "            await to_my_test_topic(mobile=\"+385912345678\", url=\"https://www.vip.hr\")\n",
    "            to_my_test_topic_2(mobile=\"+385987654321\", url=\"https://www.ht.hr\")\n",
    "        finally:\n",
    "            await app.shutdown()\n",
    "\n",
    "        mock.assert_has_calls(\n",
    "            [\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385912345678\", \"name\": \"James Bond\"}, \"url\": \"https://www.vip.hr\"}',\n",
    "                ),\n",
    "                unittest.mock.call(\n",
    "                    \"my_test_topic_2\",\n",
    "                    b'{\"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"}, \"url\": \"https://www.ht.hr\"}',\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b114fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# don't wait for specs to be generated (takes 10 sec or so)\n",
    "with unittest.mock.patch(\"__main__.export_async_spec\"):\n",
    "\n",
    "    app = create_testing_app()\n",
    "    fast_task = unittest.mock.Mock()\n",
    "    long_task = unittest.mock.Mock()\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def bg_task():\n",
    "        fast_task()\n",
    "        await asyncio.sleep(100)\n",
    "        long_task()\n",
    "\n",
    "    fast_task_second = unittest.mock.Mock()\n",
    "    long_task_second = unittest.mock.Mock()\n",
    "\n",
    "    @app.run_in_background()\n",
    "    async def bg_task_second():\n",
    "        fast_task_second()\n",
    "        await asyncio.sleep(100)\n",
    "        long_task_second()\n",
    "\n",
    "    try:\n",
    "        await app.startup()\n",
    "    finally:\n",
    "        await app.shutdown()\n",
    "\n",
    "    fast_task.assert_called()\n",
    "    long_task.assert_not_called()\n",
    "\n",
    "    fast_task_second.assert_called()\n",
    "    long_task_second.assert_not_called()\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f99375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
