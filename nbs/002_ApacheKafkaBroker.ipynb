{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c520c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _testing.apache_kafka_broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d47b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "import platform\n",
    "import socket\n",
    "from datetime import datetime, timedelta\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import *\n",
    "\n",
    "import asyncer\n",
    "import nest_asyncio\n",
    "\n",
    "from fastkafka._components._subprocess import terminate_asyncio_process\n",
    "from fastkafka._components.helpers import in_notebook\n",
    "from fastkafka._components.logger import get_logger\n",
    "from fastkafka._components.meta import delegates, export, filter_using_signature, patch\n",
    "from fastkafka._components.test_dependencies import check_java, check_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ed82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pytest\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "\n",
    "from fastkafka._components.helpers import change_dir\n",
    "from fastkafka._components.logger import suppress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81062e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "if in_notebook():\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f95ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ef020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fded319",
   "metadata": {},
   "source": [
    "### Local Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552eb74",
   "metadata": {},
   "source": [
    "#### Kafka and zookeeper config helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_zookeeper_config_string(\n",
    "    data_dir: Union[str, Path],  # the directory where the snapshot is stored.\n",
    "    zookeeper_port: int = 2181,  # the port at which the clients will connect\n",
    ") -> str:\n",
    "    \"\"\"Generates a zookeeeper configuration string that can be exported to file\n",
    "    and used to start a zookeeper instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "        zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "    Returns:\n",
    "        Zookeeper configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "    zookeeper_data_dir = str((Path(data_dir) / \"zookeeper\").resolve())\n",
    "    if platform.system() == \"Windows\":\n",
    "        zookeeper_data_dir = zookeeper_data_dir.replace(\"\\\\\", \"/\")\n",
    "    zookeeper_config = f\"\"\"dataDir={zookeeper_data_dir}\n",
    "clientPort={zookeeper_port}\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    "\n",
    "    return zookeeper_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"..\").resolve()\n",
    "data_dir = str(p).replace(\"\\\\\", \"/\") if platform.system() == \"Windows\" else str(p)\n",
    "assert (\n",
    "    get_zookeeper_config_string(data_dir=\"..\")\n",
    "    == f\"\"\"dataDir={data_dir}/zookeeper\n",
    "clientPort=2181\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "assert (\n",
    "    get_zookeeper_config_string(data_dir=\"..\", zookeeper_port=100)\n",
    "    == f\"\"\"dataDir={data_dir}/zookeeper\n",
    "clientPort=100\n",
    "maxClientCnxns=0\n",
    "admin.enableServer=false\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_kafka_config_string(\n",
    "    data_dir: Union[str, Path], zookeeper_port: int = 2181, listener_port: int = 9092\n",
    ") -> str:\n",
    "    \"\"\"Generates a kafka broker configuration string that can be exported to file\n",
    "    and used to start a kafka broker instance.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory where the kafka broker instance will save data\n",
    "        zookeeper_port: Port on which the zookeeper instance is running\n",
    "        listener_port: Port on which the clients (producers and consumers) can connect\n",
    "    Returns:\n",
    "        Kafka broker configuration string.\n",
    "\n",
    "    \"\"\"\n",
    "    kafka_logs_dir = str((Path(data_dir) / \"kafka_logs\").resolve())\n",
    "    if platform.system() == \"Windows\":\n",
    "        kafka_logs_dir = kafka_logs_dir.replace(\"\\\\\", \"/\")\n",
    "    kafka_config = f\"\"\"broker.id=0\n",
    "\n",
    "############################# Socket Server Settings #############################\n",
    "\n",
    "# The address the socket server listens on. If not configured, the host name will be equal to the value of\n",
    "# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.\n",
    "#   FORMAT:\n",
    "#     listeners = listener_name://host_name:port\n",
    "#   EXAMPLE:\n",
    "#     listeners = PLAINTEXT://your.host.name:9092\n",
    "listeners=PLAINTEXT://:{listener_port}\n",
    "\n",
    "# Listener name, hostname and port the broker will advertise to clients.\n",
    "# If not set, it uses the value for \"listeners\".\n",
    "# advertised.listeners=PLAINTEXT://localhost:{listener_port}\n",
    "\n",
    "# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details\n",
    "#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n",
    "\n",
    "# The number of threads that the server uses for receiving requests from the network and sending responses to the network\n",
    "num.network.threads=3\n",
    "\n",
    "# The number of threads that the server uses for processing requests, which may include disk I/O\n",
    "num.io.threads=8\n",
    "\n",
    "# The send buffer (SO_SNDBUF) used by the socket server\n",
    "socket.send.buffer.bytes=102400\n",
    "\n",
    "# The receive buffer (SO_RCVBUF) used by the socket server\n",
    "socket.receive.buffer.bytes=102400\n",
    "\n",
    "# The maximum size of a request that the socket server will accept (protection against OOM)\n",
    "socket.request.max.bytes=104857600\n",
    "\n",
    "\n",
    "############################# Log Basics #############################\n",
    "\n",
    "# A comma separated list of directories under which to store log files\n",
    "log.dirs={kafka_logs_dir}\n",
    "\n",
    "# The default number of log partitions per topic. More partitions allow greater\n",
    "# parallelism for consumption, but this will also result in more files across\n",
    "# the brokers.\n",
    "num.partitions=1\n",
    "\n",
    "# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.\n",
    "# This value is recommended to be increased for installations with data dirs located in RAID array.\n",
    "num.recovery.threads.per.data.dir=1\n",
    "\n",
    "offsets.topic.replication.factor=1\n",
    "transaction.state.log.replication.factor=1\n",
    "transaction.state.log.min.isr=1\n",
    "\n",
    "# The number of messages to accept before forcing a flush of data to disk\n",
    "log.flush.interval.messages=10000\n",
    "\n",
    "# The maximum amount of time a message can sit in a log before we force a flush\n",
    "log.flush.interval.ms=1000\n",
    "\n",
    "# The minimum age of a log file to be eligible for deletion due to age\n",
    "log.retention.hours=168\n",
    "\n",
    "# A size-based retention policy for logs. Segments are pruned from the log unless the remaining\n",
    "# segments drop below log.retention.bytes. Functions independently of log.retention.hours.\n",
    "log.retention.bytes=1073741824\n",
    "\n",
    "# The maximum size of a log segment file. When this size is reached a new log segment will be created.\n",
    "log.segment.bytes=1073741824\n",
    "\n",
    "# The interval at which log segments are checked to see if they can be deleted according to the retention policies\n",
    "log.retention.check.interval.ms=300000\n",
    "\n",
    "# Zookeeper connection string (see zookeeper docs for details).\n",
    "zookeeper.connect=localhost:{zookeeper_port}\n",
    "\n",
    "# Timeout in ms for connecting to zookeeper\n",
    "zookeeper.connection.timeout.ms=18000\n",
    "\n",
    "# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.\n",
    "group.initial.rebalance.delay.ms=0\n",
    "\"\"\"\n",
    "\n",
    "    return kafka_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"..\").resolve()\n",
    "data_dir = str(p).replace(\"\\\\\", \"/\") if platform.system() == \"Windows\" else str(p)\n",
    "actual = get_kafka_config_string(data_dir=\"..\", listener_port=9999)\n",
    "assert f\"log.dirs={data_dir}/kafka_logs\" in actual\n",
    "assert \"listeners=PLAINTEXT://:9999\" in actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@export(\"fastkafka.testing\")\n",
    "class ApacheKafkaBroker:\n",
    "    \"\"\"ApacheKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.\"\"\"\n",
    "\n",
    "    @delegates(get_kafka_config_string)\n",
    "    @delegates(get_zookeeper_config_string, keep=True)\n",
    "    def __init__(\n",
    "        self,\n",
    "        topics: Iterable[str] = [],\n",
    "        *,\n",
    "        retries: int = 3,\n",
    "        apply_nest_asyncio: bool = False,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ):\n",
    "        \"\"\"Initialises the ApacheKafkaBroker object\n",
    "\n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            zookeeper_port: Port for clients (Kafka brokes) to connect\n",
    "            listener_port: Port on which the clients (producers and consumers) can connect\n",
    "            topics: List of topics to create after sucessfull Kafka broker startup\n",
    "            retries: Number of retries to create kafka and zookeeper services using random\n",
    "            apply_nest_asyncio: set to True if running in notebook\n",
    "            port allocation if the requested port was taken\n",
    "        \"\"\"\n",
    "        self.zookeeper_kwargs = filter_using_signature(\n",
    "            get_zookeeper_config_string, **kwargs\n",
    "        )\n",
    "        self.kafka_kwargs = filter_using_signature(get_kafka_config_string, **kwargs)\n",
    "\n",
    "        if \"zookeeper_port\" not in self.zookeeper_kwargs:\n",
    "            self.zookeeper_kwargs[\"zookeeper_port\"] = 2181\n",
    "            self.kafka_kwargs[\"zookeeper_port\"] = 2181\n",
    "\n",
    "        if \"listener_port\" not in self.kafka_kwargs:\n",
    "            self.kafka_kwargs[\"listener_port\"] = 9092\n",
    "\n",
    "        self.retries = retries\n",
    "        self.apply_nest_asyncio = apply_nest_asyncio\n",
    "        self.temporary_directory: Optional[TemporaryDirectory] = None\n",
    "        self.temporary_directory_path: Optional[Path] = None\n",
    "        self.kafka_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self.zookeeper_task: Optional[asyncio.subprocess.Process] = None\n",
    "        self._is_started = False\n",
    "        self.topics: Iterable[str] = topics\n",
    "\n",
    "    @property\n",
    "    def is_started(self) -> bool:\n",
    "        \"\"\"Property indicating whether the ApacheKafkaBroker object is started.\n",
    "\n",
    "        The is_started property indicates if the ApacheKafkaBroker object is currently \n",
    "        in a started state. This implies that Zookeeper and Kafka broker processes have\n",
    "        sucesfully started and are ready for handling events.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the object is started, False otherwise.\n",
    "        \"\"\"\n",
    "        return self._is_started\n",
    "\n",
    "    @classmethod\n",
    "    def _check_deps(cls) -> None:\n",
    "        \"\"\"Prepares the environment for running Kafka brokers.\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance asynchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def start(self) -> str:\n",
    "        \"\"\"Starts a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           Kafka broker bootstrap server address in string format: add:port\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _stop(self) -> None:\n",
    "        \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_service_config_string(self, service: str, *, data_dir: Path) -> str:\n",
    "        \"\"\"Generates a configuration for a service\n",
    "        Args:\n",
    "            data_dir: Path to the directory where the zookeepeer instance will save data\n",
    "            service: \"kafka\" or \"zookeeper\", defines which service to get config string for\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_service(self, service: str = \"kafka\") -> None:\n",
    "        \"\"\"Starts the service according to defined service var\n",
    "        Args:\n",
    "            service: \"kafka\" or \"zookeeper\", defines which service to start\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_zookeeper(self) -> None:\n",
    "        \"\"\"Start a local zookeeper instance\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _start_kafka(self) -> None:\n",
    "        \"\"\"Start a local kafka broker\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    async def _create_topics(self) -> None:\n",
    "        \"\"\"Create missing topics in local Kafka broker\n",
    "        Returns:\n",
    "           None\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __enter__(self) -> str:\n",
    "        #         ApacheKafkaBroker._check_deps()\n",
    "        return self.start()\n",
    "\n",
    "    def __exit__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        self.stop()\n",
    "\n",
    "    async def __aenter__(self) -> str:\n",
    "        #         ApacheKafkaBroker._check_deps()\n",
    "        return await self._start()\n",
    "\n",
    "    async def __aexit__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        await self._stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combine_params(combine_params(ApacheKafkaBroker, get_kafka_config_string), get_zookeeper_config_string).__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch(cls_method=True)  # type: ignore\n",
    "def _check_deps(cls: ApacheKafkaBroker) -> None:\n",
    "    \"\"\"Checks the dependencies required to run Apache KafkaBroker.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If JDK installation or Kafka installation is not found.\n",
    "    \"\"\"\n",
    "    if not check_java():\n",
    "        raise RuntimeError(\n",
    "            \"JDK installation not found! Please install JDK manually or run 'fastkafka testing install_deps'.\"\n",
    "        )\n",
    "    if not check_kafka():\n",
    "        raise RuntimeError(\n",
    "            \"Kafka installation not found! Please install Kafka tools manually or run 'fastkafka testing install_deps'.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c383b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4600ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "[INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "[INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n"
     ]
    }
   ],
   "source": [
    "# TODO: test\n",
    "\n",
    "broker = ApacheKafkaBroker()\n",
    "broker._check_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d74671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "async def run_and_match(\n",
    "    *args: str,\n",
    "    capture: str = \"stdout\",\n",
    "    timeout: int = 5,\n",
    "    pattern: str,\n",
    "    num_to_match: int = 1,\n",
    ") -> asyncio.subprocess.Process:\n",
    "    \"\"\"Runs a command asynchronously and matches the output against a pattern.\n",
    "\n",
    "    Args:\n",
    "        *args: Command-line arguments for the subprocess.\n",
    "        capture: Which output to capture (\"stdout\" or \"stderr\").\n",
    "        timeout: Timeout in seconds for reading the output.\n",
    "        pattern: Regular expression pattern to match in the output.\n",
    "        num_to_match: Number of matches to wait for.\n",
    "\n",
    "    Returns:\n",
    "        The subprocess process object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the capture parameter has an unsupported value.\n",
    "        TimeoutError: If the process times out.\n",
    "        RuntimeError: If the process returns a non-zero return code.\n",
    "    \"\"\"\n",
    "    # Create the subprocess; redirect the standard output\n",
    "    # into a pipe.\n",
    "    matched = 0\n",
    "    proc = await asyncio.create_subprocess_exec(\n",
    "        *args,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "    # Read one line of output.\n",
    "    t = datetime.now()\n",
    "    while datetime.now() - t < timedelta(seconds=timeout):\n",
    "        try:\n",
    "            if capture == \"stdout\":\n",
    "                data = await asyncio.wait_for(proc.stdout.readline(), timeout=1.0)  # type: ignore\n",
    "            elif capture == \"stderr\":\n",
    "                data = await asyncio.wait_for(proc.stderr.readline(), timeout=1.0)  # type: ignore\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown capture param value {capture}, supported values are 'stdout', 'stderr'\"\n",
    "                )\n",
    "            ddata = data.decode(\"utf-8\")\n",
    "\n",
    "            if len(re.findall(pattern, ddata)) > 0:\n",
    "                # print(f\"Matched: {ddata}\")\n",
    "                matched += 1\n",
    "                if matched == num_to_match:\n",
    "                    return proc\n",
    "        except asyncio.exceptions.TimeoutError as e:\n",
    "            pass\n",
    "\n",
    "        if proc.returncode is not None:\n",
    "            stdout, stderr = await proc.communicate()\n",
    "            dstdout = stdout.decode(\"utf-8\")\n",
    "            dstderr = stderr.decode(\"utf-8\")\n",
    "            if proc.returncode == 0:\n",
    "                raise TimeoutError(\n",
    "                    f\"stdout={dstdout}, stderr={dstderr}, returncode={proc.returncode}\"\n",
    "                )\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    f\"stdout={dstdout}, stderr={dstderr}, returncode={proc.returncode}\"\n",
    "                )\n",
    "\n",
    "    await terminate_asyncio_process(proc)\n",
    "\n",
    "    raise TimeoutError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682b7da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print('\"' + cmd + '\"')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pytest\u001b[38;5;241m.\u001b[39mraises(\u001b[38;5;167;01mTimeoutError\u001b[39;00m):\n\u001b[1;32m----> 5\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_and_match(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m\"\u001b[39m, cmd, pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime is\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pytest\u001b[38;5;241m.\u001b[39mraises(\u001b[38;5;167;01mRuntimeError\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_and_match(\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould break on this\u001b[39m\u001b[38;5;124m\"\u001b[39m, pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime is\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     10\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mrun_and_match\u001b[1;34m(capture, timeout, pattern, num_to_match, *args)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_match\u001b[39m(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m, capture: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, pattern: \u001b[38;5;28mstr\u001b[39m, num_to_match: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m asyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mProcess:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Create the subprocess; redirect the standard output\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# into a pipe.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     12\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m     13\u001b[0m         stderr\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Read one line of output.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     t \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\subprocess.py:218\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    215\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m    216\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    217\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 218\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    219\u001b[0m     protocol_factory,\n\u001b[0;32m    220\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    221\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    222\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:1694\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1692\u001b[0m     debug_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1694\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1695\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1696\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1698\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, debug_log, transport)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py:502\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[0;32m    499\u001b[0m                                      stdin, stdout, stderr, bufsize,\n\u001b[0;32m    500\u001b[0m                                      extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cmd = \"import datetime; from time import sleep; sleep(3); print('time is:' + str(datetime.datetime.now()))\"\n",
    "# print('\"' + cmd + '\"')\n",
    "\n",
    "with pytest.raises(TimeoutError):\n",
    "    proc = await run_and_match(\"python\", \"-c\", cmd, pattern=\"time is\", timeout=1)\n",
    "\n",
    "with pytest.raises(RuntimeError):\n",
    "    proc = await run_and_match(\n",
    "        \"python3\", \"-c\", \"should break on this\", pattern=\"time is\", timeout=5\n",
    "    )\n",
    "\n",
    "proc = await run_and_match(\"python\", \"-c\", cmd, pattern=\"time is\", timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"import datetime; from time import sleep; sleep(3); print('time is:' + str(datetime.datetime.now())); print('time is:' + str(datetime.datetime.now()))\"\n",
    "\n",
    "with pytest.raises(TimeoutError):\n",
    "    proc = await run_and_match(\"python\", \"-c\", cmd, pattern=\"time is\", timeout=5, num_to_match=3)\n",
    "\n",
    "\n",
    "proc = await run_and_match(\"python\", \"-c\", cmd, pattern=\"time is\", timeout=5, num_to_match=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a076ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_free_port() -> str:\n",
    "    \"\"\"Gets a port number which is available and free in the system.\n",
    "\n",
    "    Returns:\n",
    "        The free port number as a string.\n",
    "    \"\"\"\n",
    "    s = socket.socket()\n",
    "    s.bind((\"127.0.0.1\", 0))\n",
    "    port = str(s.getsockname()[1])\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "\n",
    "async def write_config_and_run(\n",
    "    config: str, config_path: Union[str, Path], run_cmd: str\n",
    ") -> asyncio.subprocess.Process:\n",
    "    \"\"\"Writes the configuration to a file, and runs a command using the configuration.\n",
    "\n",
    "    Args:\n",
    "        config: The configuration string.\n",
    "        config_path: Path to the configuration file.\n",
    "        run_cmd: The command to run.\n",
    "\n",
    "    Returns:\n",
    "        The subprocess process object.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config)\n",
    "\n",
    "    return await asyncio.create_subprocess_exec(\n",
    "        run_cmd,\n",
    "        config_path,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stdin=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "\n",
    "@patch\n",
    "def get_service_config_string(\n",
    "    self: ApacheKafkaBroker, service: str, *, data_dir: Path\n",
    ") -> str:\n",
    "    \"\"\"Gets the configuration string for a service.\n",
    "\n",
    "    Args:\n",
    "        service: Name of the service (\"kafka\" or \"zookeeper\").\n",
    "        data_dir: Path to the directory where the service will save data.\n",
    "\n",
    "    Returns:\n",
    "        The service configuration string.\n",
    "    \"\"\"\n",
    "    service_kwargs = getattr(self, f\"{service}_kwargs\")\n",
    "    if service == \"kafka\":\n",
    "        return get_kafka_config_string(data_dir=data_dir, **service_kwargs)\n",
    "    else:\n",
    "        return get_zookeeper_config_string(data_dir=data_dir, **service_kwargs)\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start_service(self: ApacheKafkaBroker, service: str = \"kafka\") -> None:\n",
    "    \"\"\"Starts a service (kafka or zookeeper) asynchronously.\n",
    "\n",
    "    Args:\n",
    "        service: Name of the service (\"kafka\" or \"zookeeper\").\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting {service}...\")\n",
    "\n",
    "    if self.temporary_directory_path is None:\n",
    "        raise ValueError(\n",
    "            \"ApacheKafkaBroker._start_service(): self.temporary_directory_path is None, did you initialise it?\"\n",
    "        )\n",
    "\n",
    "    configs_tried: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i in range(self.retries + 1):\n",
    "        configs_tried = configs_tried + [getattr(self, f\"{service}_kwargs\").copy()]\n",
    "\n",
    "        service_config_path = self.temporary_directory_path / f\"{service}.properties\"\n",
    "\n",
    "        with open(service_config_path, \"w\") as f:\n",
    "            f.write(\n",
    "                self.get_service_config_string(\n",
    "                    service, data_dir=self.temporary_directory_path\n",
    "                )\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            script_extension = \"bat\" if platform.system() == \"Windows\" else \"sh\"\n",
    "            service_start_script = f\"{service}-server-start.{script_extension}\"\n",
    "            service_task = await run_and_match(\n",
    "                service_start_script,\n",
    "                str(service_config_path),\n",
    "                pattern=\"INFO \\[KafkaServer id=0\\] started\"\n",
    "                if service == \"kafka\"\n",
    "                else \"INFO Snapshot taken\",\n",
    "                timeout=30,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logger.info(\n",
    "                f\"{service} startup failed, generating a new port and retrying...\"\n",
    "            )\n",
    "            port = get_free_port()\n",
    "            if service == \"zookeeper\":\n",
    "                self.zookeeper_kwargs[\"zookeeper_port\"] = port\n",
    "                self.kafka_kwargs[\"zookeeper_port\"] = port\n",
    "            else:\n",
    "                self.kafka_kwargs[\"listener_port\"] = port\n",
    "\n",
    "            logger.info(f\"{service} new port={port}\")\n",
    "        else:\n",
    "            setattr(self, f\"{service}_task\", service_task)\n",
    "            return\n",
    "\n",
    "    raise ValueError(f\"Could not start {service} with params: {configs_tried}\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start_kafka(self: ApacheKafkaBroker) -> None:\n",
    "    \"\"\"Starts a local Kafka broker asynchronously.\"\"\"\n",
    "    return await self._start_service(\"kafka\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start_zookeeper(self: ApacheKafkaBroker) -> None:\n",
    "    \"\"\"Starts a local ZooKeeper instance asynchronously.\"\"\"\n",
    "    return await self._start_service(\"zookeeper\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _create_topics(self: ApacheKafkaBroker) -> None:\n",
    "    \"\"\"Creates missing topics in a local Kafka broker asynchronously.\"\"\"\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    bootstrap_server = f\"127.0.0.1:{listener_port}\"\n",
    "\n",
    "    script_extension = \"bat\" if platform.system() == \"Windows\" else \"sh\"\n",
    "    topics_script = f\"kafka-topics.{script_extension}\"\n",
    "    async with asyncer.create_task_group() as tg:\n",
    "        processes = [\n",
    "            tg.soonify(asyncio.create_subprocess_exec)(\n",
    "                topics_script,\n",
    "                \"--create\",\n",
    "                f\"--topic={topic}\",\n",
    "                f\"--bootstrap-server={bootstrap_server}\",\n",
    "                stdout=asyncio.subprocess.PIPE,\n",
    "                stdin=asyncio.subprocess.PIPE,\n",
    "            )\n",
    "            for topic in self.topics\n",
    "        ]\n",
    "\n",
    "    try:\n",
    "        return_values = [\n",
    "            await asyncio.wait_for(process.value.wait(), 30) for process in processes\n",
    "        ]\n",
    "        if any(return_value != 0 for return_value in return_values):\n",
    "            raise ValueError(\"Could not create missing topics!\")\n",
    "    except asyncio.TimeoutError as _:\n",
    "        raise ValueError(\"Timed out while creating missing topics!\")\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _start(self: ApacheKafkaBroker) -> str:\n",
    "    \"\"\"Starts a local Kafka broker and ZooKeeper instance asynchronously.\n",
    "\n",
    "    Returns:\n",
    "        The Kafka broker bootstrap server address in string format: host:port.\n",
    "    \"\"\"\n",
    "    self._check_deps()\n",
    "\n",
    "    self.temporary_directory = TemporaryDirectory()\n",
    "    self.temporary_directory_path = Path(self.temporary_directory.__enter__())\n",
    "\n",
    "    await self._start_zookeeper()\n",
    "    await self._start_kafka()\n",
    "\n",
    "    listener_port = self.kafka_kwargs.get(\"listener_port\", 9092)\n",
    "    bootstrap_server = f\"127.0.0.1:{listener_port}\"\n",
    "    logger.info(f\"Local Kafka broker up and running on {bootstrap_server}\")\n",
    "\n",
    "    await self._create_topics()\n",
    "\n",
    "    self._is_started = True\n",
    "\n",
    "    return bootstrap_server\n",
    "\n",
    "\n",
    "@patch\n",
    "async def _stop(self: ApacheKafkaBroker) -> None:\n",
    "    \"\"\"Stops a local Kafka broker and ZooKeeper instance asynchronously.\"\"\"\n",
    "    await terminate_asyncio_process(self.kafka_task)  # type: ignore\n",
    "    await terminate_asyncio_process(self.zookeeper_task)  # type: ignore\n",
    "    self.temporary_directory.__exit__(None, None, None)  # type: ignore\n",
    "    self._is_started = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = ApacheKafkaBroker()\n",
    "async with broker:\n",
    "    pass\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10338a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker_1 = ApacheKafkaBroker()\n",
    "async with broker_1:\n",
    "    port = broker_1.zookeeper_kwargs[\"zookeeper_port\"]\n",
    "    broker_2 = ApacheKafkaBroker(zookeeper_port=port, retries=0)\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        async with broker_2:\n",
    "            pass\n",
    "\n",
    "assert e.value.args[0].startswith(\"Could not start zookeeper with params:\")\n",
    "\n",
    "for broker in [broker_2]:\n",
    "    assert broker.zookeeper_task == None\n",
    "#     print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "#     zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "#     print(zookeeper_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def start(self: ApacheKafkaBroker) -> str:\n",
    "    \"\"\"Starts a local Kafka broker and ZooKeeper instance synchronously.\n",
    "\n",
    "    Returns:\n",
    "        The Kafka broker bootstrap server address in string format: host:port.\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.start(): entering...\")\n",
    "    try:\n",
    "        # get or create loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): RuntimeError raised when calling asyncio.get_event_loop(): {e}\"\n",
    "            )\n",
    "            logger.warning(\n",
    "                f\"{self.__class__.__name__}.start(): asyncio.new_event_loop()\"\n",
    "            )\n",
    "            loop = asyncio.new_event_loop()\n",
    "\n",
    "        # start zookeeper and kafka broker in the loop\n",
    "\n",
    "        if loop.is_running():\n",
    "            if self.apply_nest_asyncio:\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__}.start(): ({loop}) is already running!\"\n",
    "                )\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__}.start(): calling nest_asyncio.apply()\"\n",
    "                )\n",
    "                nest_asyncio.apply(loop)\n",
    "            else:\n",
    "                msg = f\"{self.__class__.__name__}.start(): ({loop}) is already running! Use 'apply_nest_asyncio=True' when creating 'ApacheKafkaBroker' to prevent this.\"\n",
    "                logger.error(msg)\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "        retval = loop.run_until_complete(self._start())\n",
    "        logger.info(f\"{self.__class__}.start(): returning {retval}\")\n",
    "        return retval\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.start(): exited.\")\n",
    "\n",
    "\n",
    "@patch\n",
    "def stop(self: ApacheKafkaBroker) -> None:\n",
    "    \"\"\"Stops a local kafka broker and zookeeper instance synchronously\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{self.__class__.__name__}.stop(): entering...\")\n",
    "    try:\n",
    "        if not self._is_started:\n",
    "            raise RuntimeError(\n",
    "                \"ApacheKafkaBroker not started yet, please call ApacheKafkaBroker.start() before!\"\n",
    "            )\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self._stop())\n",
    "    finally:\n",
    "        logger.info(f\"{self.__class__.__name__}.stop(): exited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker = ApacheKafkaBroker(apply_nest_asyncio=True)\n",
    "with broker:\n",
    "    print(\"Hello world!\")\n",
    "\n",
    "print(\"*\" * 50 + \"ZOOKEEPER LOGS\" + \"+\" * 50)\n",
    "zookeeper_output, _ = await broker.zookeeper_task.communicate()\n",
    "print(zookeeper_output.decode(\"UTF-8\"))\n",
    "\n",
    "\n",
    "print(\"*\" * 50 + \"KAFKA LOGS\" + \"+\" * 50)\n",
    "kafka_output, _ = await broker.kafka_task.communicate()\n",
    "print(kafka_output.decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"topic_1\", \"topic_2\"]\n",
    "script_extension = \"bat\" if platform.system() == \"Windows\" else \"sh\"\n",
    "topics_script = f\"kafka-topics.{script_extension}\"\n",
    "\n",
    "async with ApacheKafkaBroker(topics=topics) as bootstrap_server:\n",
    "    task = await asyncio.create_subprocess_exec(\n",
    "        topics_script,\n",
    "        \"--list\",\n",
    "        f\"--bootstrap-server={bootstrap_server}\",\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stdin=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "    output, _ = await asyncio.wait_for(task.communicate(), 30)\n",
    "    listed_topics = output.decode(\"UTF-8\").split(\"\\n\")[:-1]\n",
    "    for i, topic in enumerate(listed_topics):\n",
    "        listed_topics[i] = topic.strip()\n",
    "    assert set(listed_topics) == set(topics), set(listed_topics)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = \"test-topic\"\n",
    "test_msg = b\"test-msg\"\n",
    "\n",
    "with ApacheKafkaBroker(\n",
    "    topics=[test_topic], apply_nest_asyncio=True\n",
    ") as bootstrap_server:\n",
    "    consumer = AIOKafkaConsumer(test_topic, bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    await consumer.start()\n",
    "    await producer.start()\n",
    "\n",
    "    try:\n",
    "        await producer.send_and_wait(test_topic, test_msg)\n",
    "        msg = await consumer.getone()\n",
    "        assert msg, value == test_msg\n",
    "    finally:\n",
    "        await consumer.stop()\n",
    "        await producer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topic = \"test-topic\"\n",
    "test_msg = b\"test-msg\"\n",
    "\n",
    "async with ApacheKafkaBroker(topics=[test_topic]) as bootstrap_server:\n",
    "    consumer = AIOKafkaConsumer(test_topic, bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    producer = AIOKafkaProducer(bootstrap_servers=bootstrap_server)\n",
    "\n",
    "    await consumer.start()\n",
    "    await producer.start()\n",
    "\n",
    "    try:\n",
    "        await producer.send_and_wait(test_topic, test_msg)\n",
    "        msg = await consumer.getone()\n",
    "        assert msg, value == test_msg\n",
    "    finally:\n",
    "        await consumer.stop()\n",
    "        await producer.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
