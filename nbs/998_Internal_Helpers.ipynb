{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _components.helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7d267",
   "metadata": {},
   "source": [
    "# Internal helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    except AttributeError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930711d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import contextlib\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from functools import wraps\n",
    "from inspect import signature\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import docstring_parser\n",
    "import nbformat\n",
    "import typer\n",
    "from fastcore.meta import delegates\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "from fastkafka._components.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n",
    "from nbdev_mkdocs.docstring import run_examples_from_docstring\n",
    "\n",
    "from fastkafka._application.app import FastKafka\n",
    "from fastkafka._components.logger import supress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f04ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "supress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5072aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "F = TypeVar(\"F\", bound=Callable[..., Any])\n",
    "\n",
    "\n",
    "def _format_args(xs: List[docstring_parser.DocstringParam]) -> str:\n",
    "    return \"\\nArgs:\\n - \" + \"\\n - \".join(\n",
    "        [f\"{x.arg_name} ({x.type_name}): {x.description}\" for x in xs]\n",
    "    )\n",
    "\n",
    "\n",
    "def combine_params(f: F, o: Union[Type, Callable[..., Any]]) -> F:\n",
    "    \"\"\"Combines docstring arguments of a function and another object or function\n",
    "\n",
    "    Args:\n",
    "        f: destination functions where combined arguments will end up\n",
    "        o: source function from which arguments are taken from\n",
    "\n",
    "    Returns:\n",
    "        Function f with augumented docstring including arguments from both functions/objects\n",
    "    \"\"\"\n",
    "    src_params = docstring_parser.parse_from_object(o).params\n",
    "    #     logger.info(f\"combine_params(): source:{_format_args(src_params)}\")\n",
    "    docs = docstring_parser.parse_from_object(f)\n",
    "    #     logger.info(f\"combine_params(): destination:{_format_args(docs.params)}\")\n",
    "    dst_params_names = [p.arg_name for p in docs.params]\n",
    "\n",
    "    combined_params = docs.params + [\n",
    "        x for x in src_params if not x.arg_name in dst_params_names\n",
    "    ]\n",
    "    #     logger.info(f\"combine_params(): combined:{_format_args(combined_params)}\")\n",
    "\n",
    "    docs.meta = [\n",
    "        x for x in docs.meta if not isinstance(x, docstring_parser.DocstringParam)\n",
    "    ] + combined_params  # type: ignore\n",
    "\n",
    "    f.__doc__ = docstring_parser.compose(\n",
    "        docs, style=docstring_parser.DocstringStyle.GOOGLE\n",
    "    )\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530648f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(a: int, b: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a: parameter a\n",
    "        b: parameter bbbb\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def f1(b: str, c: int):\n",
    "    \"\"\"Function f1\n",
    "    Args:\n",
    "        b: parameter b\n",
    "        c: parameter c\n",
    "\n",
    "    Raises:\n",
    "        ValueError: sometimes\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "combine_params(f1, f2).__doc__\n",
    "\n",
    "expected = \"\"\"Function f1\n",
    "Args:\n",
    "    b: parameter b\n",
    "    c: parameter c\n",
    "    a: parameter a\n",
    "\n",
    "Raises:\n",
    "    ValueError: sometimes\"\"\"\n",
    "\n",
    "assert f1.__doc__ == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def delegates_using_docstring(o: Union[Type, Callable[..., Any]]) -> Callable[[F], F]:\n",
    "    def _delegates_using_docstring(f: F) -> F:\n",
    "        def _combine_params(o: Union[Type, Callable[..., Any]]) -> Callable[[F], F]:\n",
    "            def __combine_params(f: F, o: Union[Type, Callable[..., Any]] = o) -> F:\n",
    "                return combine_params(f=f, o=o)\n",
    "\n",
    "            return __combine_params\n",
    "\n",
    "        @_combine_params(o)\n",
    "        @delegates(o)  # type: ignore\n",
    "        @wraps(f)\n",
    "        def _f(*args: Any, **kwargs: Any) -> Any:\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return _f\n",
    "\n",
    "    return _delegates_using_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2166b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(a: str, d: int) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a: parameter a\n",
    "        b: parameter bbbb\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@delegates_using_docstring(f2)\n",
    "def f1(b: str, c: int, **kwargs):\n",
    "    \"\"\"Function f1\n",
    "    Args:\n",
    "        b: parameter b\n",
    "        c: parameter c\n",
    "\n",
    "    Raises:\n",
    "        ValueError: sometimes\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "expected = \"\"\"Function f1\n",
    "Args:\n",
    "    b: parameter b\n",
    "    c: parameter c\n",
    "    a: parameter a\n",
    "\n",
    "Raises:\n",
    "    ValueError: sometimes\"\"\"\n",
    "assert f1.__doc__ == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23721f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mappingproxy({'b': <Parameter \"b: str\">, 'c': <Parameter \"c: int\">}),\n",
       " mappingproxy({'a': <Parameter \"a: str\">, 'd': <Parameter \"d: int\">}))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(f1).parameters, signature(f2).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132fad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'b': <Parameter \"b: str\">, 'c': <Parameter \"c: int\">})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@delegates(f2)\n",
    "def f3(b: str, c: int, **kwargs):\n",
    "    \"\"\"Function f1\n",
    "    Args:\n",
    "        b: parameter b\n",
    "        c: parameter c\n",
    "\n",
    "    Raises:\n",
    "        ValueError: sometimes\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "signature(f3).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c95fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates_using_docstring(AIOKafkaConsumer)\n",
    "def f(a: int, **kwargs) -> str:\n",
    "    \"\"\"function a\n",
    "    Args:\n",
    "        a: parameter a\n",
    "\n",
    "    Returns:\n",
    "        stuff\n",
    "    \"\"\"\n",
    "    print(f\"{a=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f04611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function a\n",
      "Args:\n",
      "    a: parameter a\n",
      "    *topics (list(str)): optional list of topics to subscribe to. If not set,\n",
      "        call :meth:`.subscribe` or :meth:`.assign` before consuming records.\n",
      "        Passing topics directly is same as calling :meth:`.subscribe` API.\n",
      "    bootstrap_servers (str, list(str)): a ``host[:port]`` string (or list of\n",
      "        ``host[:port]`` strings) that the consumer should contact to bootstrap\n",
      "        initial cluster metadata.\n",
      "        \n",
      "        This does not have to be the full node list.\n",
      "        It just needs to have at least one broker that will respond to a\n",
      "        Metadata API Request. Default port is 9092. If no servers are\n",
      "        specified, will default to ``localhost:9092``.\n",
      "    client_id (str): a name for this client. This string is passed in\n",
      "        each request to servers and can be used to identify specific\n",
      "        server-side log entries that correspond to this client. Also\n",
      "        submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`\n",
      "        for logging with respect to consumer group administration. Default:\n",
      "        ``aiokafka-{version}``\n",
      "    group_id (str or None): name of the consumer group to join for dynamic\n",
      "        partition assignment (if enabled), and to use for fetching and\n",
      "        committing offsets. If None, auto-partition assignment (via\n",
      "        group coordinator) and offset commits are disabled.\n",
      "        Default: None\n",
      "    key_deserializer (Callable): Any callable that takes a\n",
      "        raw message key and returns a deserialized key.\n",
      "    value_deserializer (Callable, Optional): Any callable that takes a\n",
      "        raw message value and returns a deserialized value.\n",
      "    fetch_min_bytes (int): Minimum amount of data the server should\n",
      "        return for a fetch request, otherwise wait up to\n",
      "        `fetch_max_wait_ms` for more data to accumulate. Default: 1.\n",
      "    fetch_max_bytes (int): The maximum amount of data the server should\n",
      "        return for a fetch request. This is not an absolute maximum, if\n",
      "        the first message in the first non-empty partition of the fetch\n",
      "        is larger than this value, the message will still be returned\n",
      "        to ensure that the consumer can make progress. NOTE: consumer\n",
      "        performs fetches to multiple brokers in parallel so memory\n",
      "        usage will depend on the number of brokers containing\n",
      "        partitions for the topic.\n",
      "        Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).\n",
      "    fetch_max_wait_ms (int): The maximum amount of time in milliseconds\n",
      "        the server will block before answering the fetch request if\n",
      "        there isn't sufficient data to immediately satisfy the\n",
      "        requirement given by fetch_min_bytes. Default: 500.\n",
      "    max_partition_fetch_bytes (int): The maximum amount of data\n",
      "        per-partition the server will return. The maximum total memory\n",
      "        used for a request ``= #partitions * max_partition_fetch_bytes``.\n",
      "        This size must be at least as large as the maximum message size\n",
      "        the server allows or else it is possible for the producer to\n",
      "        send messages larger than the consumer can fetch. If that\n",
      "        happens, the consumer can get stuck trying to fetch a large\n",
      "        message on a certain partition. Default: 1048576.\n",
      "    max_poll_records (int): The maximum number of records returned in a\n",
      "        single call to :meth:`.getmany`. Defaults ``None``, no limit.\n",
      "    request_timeout_ms (int): Client request timeout in milliseconds.\n",
      "        Default: 40000.\n",
      "    retry_backoff_ms (int): Milliseconds to backoff when retrying on\n",
      "        errors. Default: 100.\n",
      "    auto_offset_reset (str): A policy for resetting offsets on\n",
      "        :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest\n",
      "        available message, ``latest`` will move to the most recent, and\n",
      "        ``none`` will raise an exception so you can handle this case.\n",
      "        Default: ``latest``.\n",
      "    enable_auto_commit (bool): If true the consumer's offset will be\n",
      "        periodically committed in the background. Default: True.\n",
      "    auto_commit_interval_ms (int): milliseconds between automatic\n",
      "        offset commits, if enable_auto_commit is True. Default: 5000.\n",
      "    check_crcs (bool): Automatically check the CRC32 of the records\n",
      "        consumed. This ensures no on-the-wire or on-disk corruption to\n",
      "        the messages occurred. This check adds some overhead, so it may\n",
      "        be disabled in cases seeking extreme performance. Default: True\n",
      "    metadata_max_age_ms (int): The period of time in milliseconds after\n",
      "        which we force a refresh of metadata even if we haven't seen any\n",
      "        partition leadership changes to proactively discover any new\n",
      "        brokers or partitions. Default: 300000\n",
      "    partition_assignment_strategy (list): List of objects to use to\n",
      "        distribute partition ownership amongst consumer instances when\n",
      "        group management is used. This preference is implicit in the order\n",
      "        of the strategies in the list. When assignment strategy changes:\n",
      "        to support a change to the assignment strategy, new versions must\n",
      "        enable support both for the old assignment strategy and the new\n",
      "        one. The coordinator will choose the old assignment strategy until\n",
      "        all members have been updated. Then it will choose the new\n",
      "        strategy. Default: [:class:`.RoundRobinPartitionAssignor`]\n",
      "    max_poll_interval_ms (int): Maximum allowed time between calls to\n",
      "        consume messages (e.g., :meth:`.getmany`). If this interval\n",
      "        is exceeded the consumer is considered failed and the group will\n",
      "        rebalance in order to reassign the partitions to another consumer\n",
      "        group member. If API methods block waiting for messages, that time\n",
      "        does not count against this timeout. See `KIP-62`_ for more\n",
      "        information. Default 300000\n",
      "    rebalance_timeout_ms (int): The maximum time server will wait for this\n",
      "        consumer to rejoin the group in a case of rebalance. In Java client\n",
      "        this behaviour is bound to `max.poll.interval.ms` configuration,\n",
      "        but as ``aiokafka`` will rejoin the group in the background, we\n",
      "        decouple this setting to allow finer tuning by users that use\n",
      "        :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults\n",
      "        to ``session_timeout_ms``\n",
      "    session_timeout_ms (int): Client group session and failure detection\n",
      "        timeout. The consumer sends periodic heartbeats\n",
      "        (`heartbeat.interval.ms`) to indicate its liveness to the broker.\n",
      "        If no hearts are received by the broker for a group member within\n",
      "        the session timeout, the broker will remove the consumer from the\n",
      "        group and trigger a rebalance. The allowed range is configured with\n",
      "        the **broker** configuration properties\n",
      "        `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.\n",
      "        Default: 10000\n",
      "    heartbeat_interval_ms (int): The expected time in milliseconds\n",
      "        between heartbeats to the consumer coordinator when using\n",
      "        Kafka's group management feature. Heartbeats are used to ensure\n",
      "        that the consumer's session stays active and to facilitate\n",
      "        rebalancing when new consumers join or leave the group. The\n",
      "        value must be set lower than `session_timeout_ms`, but typically\n",
      "        should be set no higher than 1/3 of that value. It can be\n",
      "        adjusted even lower to control the expected time for normal\n",
      "        rebalances. Default: 3000\n",
      "    consumer_timeout_ms (int): maximum wait timeout for background fetching\n",
      "        routine. Mostly defines how fast the system will see rebalance and\n",
      "        request new data for new partitions. Default: 200\n",
      "    api_version (str): specify which kafka API version to use.\n",
      "        :class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.\n",
      "        If set to ``auto``, will attempt to infer the broker version by\n",
      "        probing various APIs. Default: ``auto``\n",
      "    security_protocol (str): Protocol used to communicate with brokers.\n",
      "        Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.\n",
      "    ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`\n",
      "        for wrapping socket connections. Directly passed into asyncio's\n",
      "        :meth:`~asyncio.loop.create_connection`. For more information see\n",
      "        :ref:`ssl_auth`. Default: None.\n",
      "    exclude_internal_topics (bool): Whether records from internal topics\n",
      "        (such as offsets) should be exposed to the consumer. If set to True\n",
      "        the only way to receive records from an internal topic is\n",
      "        subscribing to it. Requires 0.10+ Default: True\n",
      "    connections_max_idle_ms (int): Close idle connections after the number\n",
      "        of milliseconds specified by this config. Specifying `None` will\n",
      "        disable idle checks. Default: 540000 (9 minutes).\n",
      "    isolation_level (str): Controls how to read messages written\n",
      "        transactionally.\n",
      "        \n",
      "        If set to ``read_committed``, :meth:`.getmany` will only return\n",
      "        transactional messages which have been committed.\n",
      "        If set to ``read_uncommitted`` (the default), :meth:`.getmany` will\n",
      "        return all messages, even transactional messages which have been\n",
      "        aborted.\n",
      "        \n",
      "        Non-transactional messages will be returned unconditionally in\n",
      "        either mode.\n",
      "        \n",
      "        Messages will always be returned in offset order. Hence, in\n",
      "        `read_committed` mode, :meth:`.getmany` will only return\n",
      "        messages up to the last stable offset (LSO), which is the one less\n",
      "        than the offset of the first open transaction. In particular any\n",
      "        messages appearing after messages belonging to ongoing transactions\n",
      "        will be withheld until the relevant transaction has been completed.\n",
      "        As a result, `read_committed` consumers will not be able to read up\n",
      "        to the high watermark when there are in flight transactions.\n",
      "        Further, when in `read_committed` the seek_to_end method will\n",
      "        return the LSO. See method docs below. Default: ``read_uncommitted``\n",
      "    sasl_mechanism (str): Authentication mechanism when security_protocol\n",
      "        is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:\n",
      "        ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,\n",
      "        ``OAUTHBEARER``.\n",
      "        Default: ``PLAIN``\n",
      "    sasl_plain_username (str): username for SASL ``PLAIN`` authentication.\n",
      "        Default: None\n",
      "    sasl_plain_password (str): password for SASL ``PLAIN`` authentication.\n",
      "        Default: None\n",
      "    sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).\n",
      "        Default: None\n",
      "\n",
      "Returns:\n",
      "    : stuff\n"
     ]
    }
   ],
   "source": [
    "print(f.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3422da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'a': <Parameter \"a: int\">,\n",
       "              'loop': <Parameter \"loop=None\">,\n",
       "              'bootstrap_servers': <Parameter \"bootstrap_servers='localhost'\">,\n",
       "              'client_id': <Parameter \"client_id='aiokafka-0.8.0'\">,\n",
       "              'group_id': <Parameter \"group_id=None\">,\n",
       "              'key_deserializer': <Parameter \"key_deserializer=None\">,\n",
       "              'value_deserializer': <Parameter \"value_deserializer=None\">,\n",
       "              'fetch_max_wait_ms': <Parameter \"fetch_max_wait_ms=500\">,\n",
       "              'fetch_max_bytes': <Parameter \"fetch_max_bytes=52428800\">,\n",
       "              'fetch_min_bytes': <Parameter \"fetch_min_bytes=1\">,\n",
       "              'max_partition_fetch_bytes': <Parameter \"max_partition_fetch_bytes=1048576\">,\n",
       "              'request_timeout_ms': <Parameter \"request_timeout_ms=40000\">,\n",
       "              'retry_backoff_ms': <Parameter \"retry_backoff_ms=100\">,\n",
       "              'auto_offset_reset': <Parameter \"auto_offset_reset='latest'\">,\n",
       "              'enable_auto_commit': <Parameter \"enable_auto_commit=True\">,\n",
       "              'auto_commit_interval_ms': <Parameter \"auto_commit_interval_ms=5000\">,\n",
       "              'check_crcs': <Parameter \"check_crcs=True\">,\n",
       "              'metadata_max_age_ms': <Parameter \"metadata_max_age_ms=300000\">,\n",
       "              'partition_assignment_strategy': <Parameter \"partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,)\">,\n",
       "              'max_poll_interval_ms': <Parameter \"max_poll_interval_ms=300000\">,\n",
       "              'rebalance_timeout_ms': <Parameter \"rebalance_timeout_ms=None\">,\n",
       "              'session_timeout_ms': <Parameter \"session_timeout_ms=10000\">,\n",
       "              'heartbeat_interval_ms': <Parameter \"heartbeat_interval_ms=3000\">,\n",
       "              'consumer_timeout_ms': <Parameter \"consumer_timeout_ms=200\">,\n",
       "              'max_poll_records': <Parameter \"max_poll_records=None\">,\n",
       "              'ssl_context': <Parameter \"ssl_context=None\">,\n",
       "              'security_protocol': <Parameter \"security_protocol='PLAINTEXT'\">,\n",
       "              'api_version': <Parameter \"api_version='auto'\">,\n",
       "              'exclude_internal_topics': <Parameter \"exclude_internal_topics=True\">,\n",
       "              'connections_max_idle_ms': <Parameter \"connections_max_idle_ms=540000\">,\n",
       "              'isolation_level': <Parameter \"isolation_level='read_uncommitted'\">,\n",
       "              'sasl_mechanism': <Parameter \"sasl_mechanism='PLAIN'\">,\n",
       "              'sasl_plain_password': <Parameter \"sasl_plain_password=None\">,\n",
       "              'sasl_plain_username': <Parameter \"sasl_plain_username=None\">,\n",
       "              'sasl_kerberos_service_name': <Parameter \"sasl_kerberos_service_name='kafka'\">,\n",
       "              'sasl_kerberos_domain_name': <Parameter \"sasl_kerberos_domain_name=None\">,\n",
       "              'sasl_oauth_token_provider': <Parameter \"sasl_oauth_token_provider=None\">})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(f).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc25785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def use_parameters_of(\n",
    "    o: Union[Type, Callable[..., Any]], **kwargs: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Restrict parameters passwed as keyword arguments to parameters from the signature of ``o``\n",
    "\n",
    "    Args:\n",
    "        o: object or callable which signature is used for restricting keyword arguments\n",
    "        kwargs: keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        restricted keyword arguments\n",
    "\n",
    "    \"\"\"\n",
    "    allowed_keys = set(signature(o).parameters.keys())\n",
    "    return {k: v for k, v in kwargs.items() if k in allowed_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7db3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert use_parameters_of(AIOKafkaConsumer, api_version=0.1, radnom_param=\"random\") == {\n",
    "    \"api_version\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_app_src(out_path: Union[Path, str]) -> None:\n",
    "    path = Path(\"099_Test_Service.ipynb\")\n",
    "    if not path.exists():\n",
    "        path = Path(\"..\") / \"099_Test_Service.ipynb\"\n",
    "    if not path.exists():\n",
    "        raise ValueError(f\"Path '{path.resolve()}' does not exists.\")\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        notebook = nbformat.reads(f.read(), nbformat.NO_CONVERT)\n",
    "        exporter = PythonExporter()\n",
    "        source, _ = exporter.from_notebook_node(notebook)\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5cd63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwx------ 2 davor davor 4096 Mar 15 09:19 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwxrwxrwt 1 root  root  4096 Mar 15 09:19 \u001b[30;42m..\u001b[0m\n",
      "-rw-rw-r-- 1 davor davor 9888 Mar 15 09:19 main.py\n",
      "    \u001b[01;31m\u001b[K@kafka_app\u001b[m\u001b[K.consumes()  # type: ignore\n",
      "    \u001b[01;31m\u001b[K@kafka_app\u001b[m\u001b[K.consumes()  # type: ignore\n",
      "    \u001b[01;31m\u001b[K@kafka_app\u001b[m\u001b[K.produces()  # type: ignore\n",
      "    \u001b[01;31m\u001b[K@kafka_app\u001b[m\u001b[K.produces()  # type: ignore\n",
      "    \u001b[01;31m\u001b[K@kafka_app\u001b[m\u001b[K.produces()  # type: ignore\n",
      "    \u001b[01;31m\u001b[K@kafka_app\u001b[m\u001b[K.produces()  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    generate_app_src((Path(d) / \"main.py\"))\n",
    "    !ls -al {d}\n",
    "    !cat {d}/main.py | grep @kafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def change_dir(d: str) -> Generator[None, None, None]:\n",
    "    curdir = os.getcwd()\n",
    "    os.chdir(d)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    original_wd = os.getcwd()\n",
    "    assert original_wd != d\n",
    "    with change_dir(d):\n",
    "        assert os.getcwd() == d\n",
    "    assert os.getcwd() == original_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ImportFromStringError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _import_from_string(import_str: str) -> Any:\n",
    "    \"\"\"Imports library from string\n",
    "\n",
    "    Note:\n",
    "        copied from https://github.com/encode/uvicorn/blob/master/uvicorn/importer.py\n",
    "\n",
    "    Args:\n",
    "        import_str: input string in form 'main:app'\n",
    "\n",
    "    \"\"\"\n",
    "    sys.path.append(\".\")\n",
    "\n",
    "    if not isinstance(import_str, str):\n",
    "        return import_str\n",
    "\n",
    "    module_str, _, attrs_str = import_str.partition(\":\")\n",
    "    if not module_str or not attrs_str:\n",
    "        message = (\n",
    "            'Import string \"{import_str}\" must be in format \"<module>:<attribute>\".'\n",
    "        )\n",
    "        typer.secho(f\"{message}\", err=True, fg=typer.colors.RED)\n",
    "        raise ImportFromStringError(message.format(import_str=import_str))\n",
    "\n",
    "    try:\n",
    "        # nosemgrep: python.lang.security.audit.non-literal-import.non-literal-import\n",
    "        module = importlib.import_module(module_str)\n",
    "    except ImportError as exc:\n",
    "        if exc.name != module_str:\n",
    "            raise exc from None\n",
    "        message = 'Could not import module \"{module_str}\".'\n",
    "        raise ImportFromStringError(message.format(module_str=module_str))\n",
    "\n",
    "    instance = module\n",
    "    try:\n",
    "        for attr_str in attrs_str.split(\".\"):\n",
    "            instance = getattr(instance, attr_str)\n",
    "    except AttributeError:\n",
    "        message = 'Attribute \"{attrs_str}\" not found in module \"{module_str}\".'\n",
    "        raise ImportFromStringError(\n",
    "            message.format(attrs_str=attrs_str, module_str=module_str)\n",
    "        )\n",
    "\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e14aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    src_path = Path(d) / \"main.py\"\n",
    "    generate_app_src(src_path)\n",
    "    with change_dir(d):\n",
    "        kafka_app = _import_from_string(f\"{src_path.stem}:kafka_app\")\n",
    "        assert isinstance(kafka_app, FastKafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def filter_using_signature(f: Callable, **kwargs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"todo: write docs\"\"\"\n",
    "    param_names = list(signature(f).parameters.keys())\n",
    "    return {k: v for k, v in kwargs.items() if k in param_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a: int, *, b: str):\n",
    "    pass\n",
    "\n",
    "\n",
    "assert filter_using_signature(f, a=1, c=3) == {\"a\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf42be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
